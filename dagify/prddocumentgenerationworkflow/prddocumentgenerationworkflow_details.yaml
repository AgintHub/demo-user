dag_type: pure
description: Decompose the process for generating a Product Requirements Document
  (PRD) from workflow requirements into granular, type-safe, elemental steps.
name: PRDDocumentGenerationWorkflow
nodes:
  compose_prd_summary:
    agent: false
    code_node_type: pure
    description: Create a PRD executive summary referencing all prior outputs. Include
      a brief note outlining the purpose and main topics for an upcoming meeting with
      senior managers regarding the PRD.
    implementation: "from pydantic import BaseModel, Field\nfrom typing import List\n\
      \n\nclass IdentifyCoreObjectiveOutput(BaseModel):\n    \"\"\"Pydantic model\
      \ for identify_core_objective node outputs.\"\"\"\n    core_objective: str =\
      \ Field(..., description=\"The primary objective statement for the workflow\
      \ as described in the requirements, reflecting the need to include a meeting\
      \ with senior managers prior to determining user roles, and noting that constraints\
      \ should be available for use by dependent nodes, such as those identifying\
      \ success metrics.\")\n\n\nclass ListPrimaryUserRolesOutput(BaseModel):\n  \
      \  \"\"\"Pydantic model for list_primary_user_roles node outputs.\"\"\"\n  \
      \  user_roles: List[str] = Field(..., description=\"List of primary user roles\
      \ or personas interacting with the workflow, explicitly incorporating the node\
      \ for a meeting with senior managers when applicable.\")\n\n\nclass ExtractKeyFeaturesOutput(BaseModel):\n\
      \    \"\"\"Pydantic model for extract_key_features node outputs.\"\"\"\n   \
      \ feature_names: List[str] = Field(..., description=\"Names of fundamental workflow\
      \ features required in the PRD, in sequence starting with a node to schedule\
      \ and conduct a meeting with senior managers prior to any role assignment, followed\
      \ by other required workflow features.\")\n\n\nclass GenerateFeatureDescriptionsOutput(BaseModel):\n\
      \    \"\"\"Pydantic model for generate_feature_descriptions node outputs.\"\"\
      \"\n    feature_descriptions: List[str] = Field(..., description=\"Concise description\
      \ for each feature, including a clear specification of the meeting with senior\
      \ managers as a required step prior to determining user roles, aligned with\
      \ feature_names.\")\n\n\nclass ComposePrdSummaryOutput(BaseModel):\n    \"\"\
      \"Pydantic model for compose_prd_summary node outputs.\"\"\"\n    prd_summary:\
      \ str = Field(..., description=\"Executive summary for the PRD document, synthesizing\
      \ objective, users, and features, and including a note about the meeting with\
      \ senior managers.\")\n\n\ndef compose_prd_summary_fx(identify_core_objective_input:\
      \ IdentifyCoreObjectiveOutput, list_primary_user_roles_input: ListPrimaryUserRolesOutput,\
      \ extract_key_features_input: ExtractKeyFeaturesOutput, generate_feature_descriptions_input:\
      \ GenerateFeatureDescriptionsOutput, **kwargs) -> ComposePrdSummaryOutput:\n\
      \    \"\"\"Create a PRD executive summary referencing all prior outputs. Include\
      \ a brief note outlining the purpose and main topics for an upcoming meeting\
      \ with senior managers regarding the PRD.\n\n    Args:\n        identify_core_objective_input:\
      \ Input from the 'identify_core_objective' node.\n        list_primary_user_roles_input:\
      \ Input from the 'list_primary_user_roles' node.\n        extract_key_features_input:\
      \ Input from the 'extract_key_features' node.\n        generate_feature_descriptions_input:\
      \ Input from the 'generate_feature_descriptions' node.\n        **kwargs: Additional\
      \ keyword arguments.\n\n    Returns:\n        ComposePrdSummaryOutput: Object\
      \ containing outputs for this node.\n    \"\"\"\n    # --- AGGREGATE & VALIDATE\
      \ INPUTS ---\n    inputs_valid: bool = validate_prd_aggregation_inputs(\n  \
      \      core_obj=identify_core_objective_input,\n        user_roles=list_primary_user_roles_input,\n\
      \        features=extract_key_features_input,\n        feature_desc=generate_feature_descriptions_input\n\
      \    )\n\n    # --- LEAD-IN GENERATION ---\n    lead_in: str = compose_executive_leadin(core_objective=identify_core_objective_input.core_objective)\n\
      \n    # --- USER ROLES SECTION ---\n    user_roles_section: str = render_user_roles_section(user_roles=list_primary_user_roles_input.user_roles)\n\
      \n    # --- FEATURES & DESCRIPTIONS SECTION ---\n    features_with_desc: str\
      \ = format_features_with_descriptions(\n        feature_names=extract_key_features_input.feature_names,\n\
      \        feature_descriptions=generate_feature_descriptions_input.feature_descriptions\n\
      \    )\n\n    # --- UPCOMING MEETING NOTE ---\n    meeting_note: str = compose_meeting_note(\n\
      \        topics=[\n            \"Objective\",\n            \"Primary User Roles\"\
      ,\n            \"Key Features\",\n            \"Feature Definitions\"\n    \
      \    ]\n    )\n\n    # --- FINAL SUMMARY ASSEMBLY ---\n    prd_summary_str:\
      \ str = assemble_executive_summary(\n        lead_in=lead_in,\n        user_roles_section=user_roles_section,\n\
      \        feature_section=features_with_desc,\n        meeting_note=meeting_note\n\
      \    )\n\n    return ComposePrdSummaryOutput(prd_summary=prd_summary_str)\n"
    name: compose_prd_summary
    nodes_depended_on:
    - identify_core_objective
    - list_primary_user_roles
    - extract_key_features
    - generate_feature_descriptions
    nodes_dependent_on: []
    output_structure:
    - description: Executive summary for the PRD document, synthesizing objective,
        users, and features, and including a note about the meeting with senior managers.
      key: prd_summary
      type: str
    prd:
      bullets:
      - complexity: "LOW \u2013 Simple dependency extraction and structuring."
        impact: "HIGH \u2013 This foundational aggregation guarantees correctness\
          \ and consistency in the summary, and is critical for later documentation\
          \ and management buy-in."
        method: Accept structured outputs as function parameters or via context injection,
          validating presence and schema conformance at runtime.
        reason: Ensures that the executive summary is comprehensive, drawing only
          on validated upstream information, guaranteeing factual integrity and full
          stakeholder alignment.
        text: Aggregate the outputs from 'identify_core_objective', 'list_primary_user_roles',
          'extract_key_features', and 'generate_feature_descriptions' as structured
          data inputs to this node.
      - complexity: "LOW \u2013 Standard language generation, leveraging the extracted\
          \ core objective."
        impact: "MEDIUM \u2013 Frames the rest of the summary and improves stakeholder\
          \ comprehension and buy-in."
        method: Format a natural language template with slot injection for the core
          objective (e.g., 'This Product Requirements Document is designed to...').
        reason: The lead-in orients the executive audience immediately and foregrounds
          the purpose and alignment of the PRD.
        text: Concatenate the core objective with a brief, context-setting introduction
          to create a clear lead-in for the executive summary tailored to a C-level
          reader.
      - complexity: "LOW \u2013 Straightforward enumeration of user roles."
        impact: "MEDIUM \u2013 Ensures the inclusion of all relevant roles, aiding\
          \ leadership in alignment and decision making."
        method: Generate a comma-separated list or bullet list of roles, using a simple
          sentence or short phrase for each.
        reason: Executives must understand stakeholders and user personas to assess
          scope, impact, and resource planning.
        text: List primary user roles (from 'list_primary_user_roles'), ensuring each
          role is named and briefly contextualized as to their interaction in the
          workflow.
      - complexity: "MEDIUM \u2013 Requires ordered merging of two input lists, maintaining\
          \ alignment and formatting for readability."
        impact: "HIGH \u2013 Ensures that workflow dependencies and process understanding\
          \ are clear at a glance to executives."
        method: Iterate feature_names and for each, append its corresponding feature_description
          using list zip and indexed formatting.
        reason: Feature clarity and sequencing are critical for conveying workflow
          logic, especially the mandated ordering (e.g., meeting with senior managers
          before user role determination).
        text: Present the key features in strict sequence (from 'extract_key_features'),
          pairing each with its respective concise description (from 'generate_feature_descriptions').
      - complexity: "LOW \u2013 Standard text generation with parameterized topic\
          \ list."
        impact: "HIGH \u2013 Directly supports stakeholder communication, approval\
          \ processes, and compliance with workflow constraints."
        method: Utilize a markdown or bulleted note template, injecting summary-level
          metadata and a standardized topic breakdown.
        reason: Sets clear expectations for executive stakeholders and drives the
          agenda for follow-up action, reinforcing the necessary sequencing established
          earlier in the workflow.
        text: 'Add a dedicated ''Upcoming Senior Manager Meeting'' note after the
          summary, outlining the purpose of the meeting and listing the main PRD topics
          for review: objective, user roles, features, and their definitions.'
      - complexity: "LOW \u2013 String concatenation and formatting."
        impact: "HIGH \u2013 Centralizes executive communication and feeds authoritative\
          \ summary content to subsequent nodes or outward communication channels."
        method: Assemble all generated segments in sequence, using markdown, bullet
          points, or section headers for maximum clarity.
        reason: A unified summary string is easier to propagate, display, and archive,
          and supports integration with reporting, versioning, and notification systems.
        text: Format the final executive summary output as a single coherent string,
          ensuring professional structure (lead-in, user roles, features with descriptions,
          meeting note), and provide as 'prd_summary' for downstream use.
    prompt: Write an executive summary for the PRD that succinctly combines the core
      objective, user roles, key features, and their descriptions. The summary should
      present a clear overview to an executive reader. Additionally, include a brief
      note outlining the objective of an upcoming meeting with senior managers, specifying
      the main topics to be discussed from the PRD (such as alignment on objective,
      user roles, and features). Do not include metrics, constraints, or risks here.
    shims:
    - agent: false
      code_node_type: virtual-pure
      description: "This node verifies that the aggregated PRD components\u2014core\
        \ objective, user roles, features, and feature descriptions\u2014are present\
        \ and consistently structured before generating the final PRD summary."
      implementation: "def validate_prd_aggregation_inputs(core_obj: str, user_roles:\
        \ str, features: str, feature_desc: str) -> bool:\n    \"\"\"\n    This node\
        \ verifies that the aggregated PRD components\u2014core objective, user roles,\
        \ features, and feature descriptions\u2014are present and consistently structured\
        \ before generating the final PRD summary.\n\n    Args:\n        core_obj:\
        \ Input parameter of type str\nuser_roles: Input parameter of type str\nfeatures:\
        \ Input parameter of type str\nfeature_desc: Input parameter of type str\n\
        \n    Returns:\n        bool: Output of type bool\n    \"\"\"\n    # --- PURE\
        \ IMPLEMENTATION: Replace all shims ---\n    import json\n\n    # 1. Check\
        \ all required fields are present and non-empty (not None, not empty, not\
        \ just whitespace)\n    def is_nonempty(s):\n        return isinstance(s,\
        \ str) and bool(s.strip())\n\n    if not (is_nonempty(core_obj) and is_nonempty(user_roles)\
        \ and is_nonempty(features) and is_nonempty(feature_desc)):\n        return\
        \ False\n\n    # 2. Parse the features and feature_desc as lists. Assume they\
        \ are either JSON-encoded lists or comma/semicolon separated as fallback.\n\
        \    def parse_list(s):\n        try:\n            val = json.loads(s)\n \
        \           if isinstance(val, list):\n                return val\n      \
        \  except Exception:\n            pass\n        # fallback: comma-separated\
        \ or semicolon-separated\n        if ',' in s:\n            return [i.strip()\
        \ for i in s.split(',') if i.strip()]\n        elif '\\n' in s:\n        \
        \    return [i.strip() for i in s.split('\\n') if i.strip()]\n        elif\
        \ ';' in s:\n            return [i.strip() for i in s.split(';') if i.strip()]\n\
        \        else:\n            return [s.strip()] if s.strip() else []\n\n  \
        \  features_list = parse_list(features)\n    feature_desc_list = parse_list(feature_desc)\n\
        \n    # 3. Validate both lists are non-empty and have same length\n    if\
        \ not features_list or not feature_desc_list:\n        return False\n    if\
        \ len(features_list) != len(feature_desc_list):\n        return False\n\n\
        \    # 4. Extra content check: If 'meeting with senior managers' is a required\
        \ element, check for (case-insensitive) substring in either feature or description\n\
        \    required_phrase = 'meeting with senior managers'\n    found_in_features\
        \ = any(required_phrase in str(f).lower() for f in features_list)\n    found_in_descs\
        \ = any(required_phrase in str(d).lower() for d in feature_desc_list)\n  \
        \  # If required, ensure at least one appearance in either field. As per PRD,\
        \ make this check (if strictly required, set as needed):\n    if not (found_in_features\
        \ or found_in_descs):\n        return False\n\n    return True\n"
      name: validate_prd_aggregation_inputs
      nodes_depended_on: []
      nodes_dependent_on: []
      output_structure:
      - description: Output of type bool
        key: output
        type: bool
      - description: Input parameter of type str
        key: core_obj
        type: str
      - description: Input parameter of type str
        key: user_roles
        type: str
      - description: Input parameter of type str
        key: features
        type: str
      - description: Input parameter of type str
        key: feature_desc
        type: str
      prd:
        bullets:
        - complexity: LOW
          impact: Prevents runtime errors and ensures the PRD summary node operates
            on a complete, minimal dataset.
          method: Use data validation checks such as verifying null, empty, or malformed
            values before proceeding.
          reason: Missing or malformed input fields can cause downstream errors and
            an incomplete or invalid PRD summary.
          text: Implement input validation to ensure all required PRD fields (core
            objective, user roles, features, feature descriptions) are present before
            aggregation.
        - complexity: MEDIUM
          impact: Guarantees each feature is described once with a clear mapping,
            improving the PRD's clarity and usability.
          method: Parse the serialized lists to native types and confirm that feature
            and description lists are of equal length and correspondence.
          reason: Mismatched lists can cause misaligned documentation and confusion
            for stakeholders.
          text: Validate consistency between lists of feature names and their corresponding
            descriptions.
        - complexity: MEDIUM
          impact: Ensures adherence to business process requirements, avoiding missed
            steps in downstream implementation.
          method: Include a content check for required elements (e.g., substring inspection
            or key phrase matching in features and descriptions data).
          reason: A required workflow element omitted from the features set would
            result in an incomplete requirements document.
          text: Confirm that the meeting with senior managers is referenced or included
            in both features and descriptions if required.
      prompt: 'Description: Typed node for shim validate_prd_aggregation_inputs'
      shims: []
    - agent: false
      code_node_type: virtual-pure
      description: Generates a professional, succinct executive lead-in paragraph
        for a PRD summary based on the provided core objective.
      implementation: "def compose_executive_leadin(core_objective: str) -> str:\n\
        \    \"\"\"\n    Generates a professional, succinct executive lead-in paragraph\
        \ for a PRD summary based on the provided core objective.\n\n    Args:\n \
        \       core_objective: Input parameter of type str\n\n    Returns:\n    \
        \    str: Output of type str\n    \"\"\"\n    # PURE IMPLEMENTATION: Creates\
        \ an executive-level introduction based on the objective, utilizing template\
        \ language with slight adaptability.\n    import re\n    \n    # Clean and\
        \ trim the core objective for presentation\n    objective = core_objective.strip().rstrip('.')\n\
        \    \n    # Analyze for adaptability: If the objective contains typical product/feature\
        \ keywords, use more targeted phrasing\n    lower_obj = objective.lower()\n\
        \    \n    # Heuristic targeting for tone adjustment\n    technology_terms\
        \ = [\n        'platform', 'feature', 'system', 'solution', 'product', 'service',\n\
        \        'capability', 'tool', 'suite', 'framework', 'module', 'application',\
        \ 'process', 'workflow', 'initiative'\n    ]\n    context_aware = any(term\
        \ in lower_obj for term in technology_terms)\n    \n    # Select a template\
        \ dynamically, but maintain a professional, concise, executive style\n   \
        \ if context_aware:\n        templates = [\n            \"This PRD outlines\
        \ the core objective to {obj}, serving as the foundation for strategic alignment\
        \ and product vision.\",\n            \"At the heart of this Product Requirements\
        \ Document is the objective to {obj}, which will guide subsequent planning\
        \ and execution.\",\n            \"The following executive summary frames\
        \ our intent to {obj}, ensuring clarity and direction for all stakeholders.\"\
        \n        ]\n    else:\n        templates = [\n            \"This executive\
        \ summary presents the primary objective: {obj}.\",\n            \"The purpose\
        \ of this document is to succinctly articulate the goal to {obj} for all relevant\
        \ stakeholders.\",\n            \"Framing the PRD, the key objective is to\
        \ {obj}, establishing the basis for this initiative.\"\n        ]\n    \n\
        \    # To ensure slight variation, pick a template based on the length or\
        \ hash of input\n    idx = len(objective) % len(templates)\n    selected_template\
        \ = templates[idx]\n    \n    leadin = selected_template.format(obj=objective)\n\
        \    return leadin\n"
      name: compose_executive_leadin
      nodes_depended_on: []
      nodes_dependent_on: []
      output_structure:
      - description: Output of type str
        key: output
        type: str
      - description: Input parameter of type str
        key: core_objective
        type: str
      prd:
        bullets:
        - complexity: LOW
          impact: "Ensures consistent tone and immediate clarity for readers on the\
            \ document\u2019s purpose, setting expectations for subsequent PRD sections."
          method: Implement a function or template that takes the core objective and
            wraps it in concise, formal language appropriate for executive communication.
          reason: The executive summary must begin with a clear, tailored lead-in
            that frames the underlying objective for high-level stakeholders.
          text: Accept a core objective string and transform it into an executive-level
            introduction for the PRD summary.
        - complexity: MEDIUM
          impact: Provides robust, context-sensitive introductory statements to improve
            engagement and comprehension of PRD objectives.
          method: Leverage templated language with conditional formatting or dynamic
            wording based on analyzed content of the core objective input.
          reason: PRDs may vary in scope or terminology, so the lead-in needs to remain
            relevant and authoritative regardless of input.
          text: Ensure adaptability in phrasing for different PRD contexts while maintaining
            professionalism and clarity.
      prompt: Given the core objective of a product requirements document (PRD), compose
        a concise, professional executive lead-in paragraph that clearly describes
        the workflow's main goal and context, suitable for the opening of an executive
        summary.
      shims: []
    - agent: false
      code_node_type: virtual-pure
      description: Generates a well-formatted, human-readable section that describes
        the primary user roles in the workflow based on the provided user roles input.
      implementation: "def render_user_roles_section(user_roles: str) -> str:\n  \
        \  \"\"\"\n    Generates a well-formatted, human-readable section that describes\
        \ the primary user roles in the workflow based on the provided user roles\
        \ input.\n\n    Args:\n        user_roles: Input parameter of type str\n\n\
        \    Returns:\n        str: Output of type str\n    \"\"\"\n    # --- PURE\
        \ IMPLEMENTATION ---\n    import re\n    \n    # Step 1: Accept either a comma-separated\
        \ string, semicolon separated, or basic list-resembling string\n    # Normalize\
        \ input to a list of role strings\n    if isinstance(user_roles, str):\n \
        \       # Remove any brackets or array-like characters, split by comma, semicolon,\
        \ or whitespace if necessary\n        input_clean = user_roles.strip().strip('[](){}')\n\
        \        # Attempt to split on commas or semicolons\n        if ',' in input_clean:\n\
        \            role_candidates = [r.strip() for r in input_clean.split(',')]\n\
        \        elif ';' in input_clean:\n            role_candidates = [r.strip()\
        \ for r in input_clean.split(';')]\n        else:\n            # If it's a\
        \ single word or phrase\n            role_candidates = [input_clean.strip()]\n\
        \    elif isinstance(user_roles, list):\n        role_candidates = [str(r).strip()\
        \ for r in user_roles]\n    else:\n        return \"No user roles specified.\"\
        \n\n    # Remove any empty strings\n    roles = [r for r in role_candidates\
        \ if r]\n\n    if not roles:\n        return \"No user roles specified.\"\n\
        \n    # Step 2: Format into a clear human-readable, prose-based section\n\
        \    # Adapt output based on the number of roles, using proper conjunctions\
        \ and Oxford comma\n    if len(roles) == 1:\n        return f\"Primary user\
        \ role: {roles[0]}.\"\n    elif len(roles) == 2:\n        return f\"Primary\
        \ user roles: {roles[0]} and {roles[1]}.\"\n    else:\n        # Oxford comma\
        \ for more than 2 items\n        prose = ', '.join(roles[:-1]) + f\", and\
        \ {roles[-1]}\"\n        return f\"Primary user roles: {prose}.\"\n"
      name: render_user_roles_section
      nodes_depended_on: []
      nodes_dependent_on: []
      output_structure:
      - description: Output of type str
        key: output
        type: str
      - description: Input parameter of type str
        key: user_roles
        type: str
      prd:
        bullets:
        - complexity: LOW
          impact: Enhances the PRD's readability and helps stakeholders quickly understand
            who the principal users are.
          method: Iterate over the user_roles input (accepting both comma-separated
            strings or lists), convert to bullet points or a short descriptive paragraph,
            and return as a string.
          reason: Raw role lists lack clarity and contextualization, so a formatted
            explanation improves document professionalism and usability.
          text: Transform the raw user_roles input into a clearly structured, prose-based
            section for human readers.
        - complexity: LOW
          impact: Reduces the need for post-processing and improves end-user trust
            in document quality.
          method: Implement conditional logic to check the number of roles and adjust
            list separators and language (e.g., Oxford comma, 'and', singular/plural
            verbs).
          reason: Grammatical accuracy and adaptable structure ensures the section
            remains professional regardless of input length.
          text: Ensure adaptability in output formatting for different numbers of
            user roles (e.g., singular vs. plural, appropriate conjunctions).
      prompt: 'Description: Typed node for shim render_user_roles_section'
      shims: []
    - agent: false
      code_node_type: virtual-pure
      description: "This node combines two lists\u2014feature names and their descriptions\u2014\
        into a single, human-readable formatted string pairing each feature with its\
        \ respective description."
      implementation: "def format_features_with_descriptions(feature_names: str, feature_descriptions:\
        \ str) -> str:\n    \"\"\"\n    This node combines two lists\u2014feature\
        \ names and their descriptions\u2014into a single, human-readable formatted\
        \ string pairing each feature with its respective description.\n\n    Args:\n\
        \        feature_names: Input parameter of type str\nfeature_descriptions:\
        \ Input parameter of type str\n\n    Returns:\n        str: Output of type\
        \ str\n    \"\"\"\n    # --- PURE IMPLEMENTATION ---\n    import json\n  \
        \  \n    # Attempt to parse the inputs as JSON lists, or fallback to comma-split\n\
        \    def parse_list(s):\n        try:\n            lst = json.loads(s)\n \
        \           if isinstance(lst, list):\n                return lst\n      \
        \  except Exception:\n            pass\n        # fallback: comma separated\n\
        \        return [item.strip() for item in s.split(',') if item.strip()]\n\
        \    \n    names_list = parse_list(feature_names)\n    descriptions_list =\
        \ parse_list(feature_descriptions)\n\n    if len(names_list) != len(descriptions_list):\n\
        \        raise ValueError(f\"feature_names and feature_descriptions must be\
        \ of the same length (got {len(names_list)} and {len(descriptions_list)})\"\
        )\n\n    lines = []\n    for name, desc in zip(names_list, descriptions_list):\n\
        \        lines.append(f\"- {name}: {desc}\")\n    \n    formatted = \"\\n\"\
        .join(lines)\n    return formatted\n"
      name: format_features_with_descriptions
      nodes_depended_on: []
      nodes_dependent_on: []
      output_structure:
      - description: Output of type str
        key: output
        type: str
      - description: Input parameter of type str
        key: feature_names
        type: str
      - description: Input parameter of type str
        key: feature_descriptions
        type: str
      prd:
        bullets:
        - complexity: LOW
          impact: Ensures that PRD executive summaries are accurate, readable, and
            trustworthy to stakeholders.
          method: Iterate through both lists (feature_names and feature_descriptions)
            by index and concatenate each pair in a structured template, such as bullet
            points or a table.
          reason: Proper alignment of feature names and descriptions ensures clarity
            and prevents mismatched or missing information in the summary.
          text: Map each feature name to its respective description in the order provided
            and generate a combined formatted string.
        - complexity: LOW
          impact: Reduces risk of data misalignment and downstream errors in summary
            generation.
          method: Check the length of both lists at the start and raise an exception
            or return a clear error message if they differ.
          reason: Input mismatch (list length inequality) could result in missing
            features or descriptions, causing confusion.
          text: Validate that both input lists are of equal length before attempting
            to pair entries.
      prompt: Given two lists, one of feature names and one of feature descriptions,
        format them into a readable string where each feature is paired with its corresponding
        description, suitable for inclusion in an executive summary.
      shims: []
    - agent: false
      code_node_type: virtual-pure
      description: Generates a concise meeting note summarizing the purpose and main
        discussion topics for an upcoming meeting with senior managers, based on provided
        topics.
      implementation: "def compose_meeting_note(topics: str) -> str:\n    \"\"\"\n\
        \    Generates a concise meeting note summarizing the purpose and main discussion\
        \ topics for an upcoming meeting with senior managers, based on provided topics.\n\
        \n    Args:\n        topics: Input parameter of type str\n\n    Returns:\n\
        \        str: Output of type str\n    \"\"\"\n    # --- PURE IMPLEMENTATION\
        \ ---\n    # 1. Validate and sanitize the input topic list to prevent duplication\
        \ and ensure clarity before rendering in the note.\n    # 2. Create a succinct\
        \ explanation for the meeting and present the topics in a clear, bullet-point\
        \ or listed format.\n    \n    # We'll accept topics as a string, try to split\
        \ into a topic list by common delimiters\n    import re\n    \n    # Heuristically\
        \ split topics on newlines or commas or semicolons\n    split_patterns = r'\\\
        n|,|;'\n    raw_topics = [t.strip() for t in re.split(split_patterns, topics)\
        \ if t.strip()]\n    \n    # Remove duplicates while preserving order\n  \
        \  seen = set()\n    unique_topics = []\n    for topic in raw_topics:\n  \
        \      if topic not in seen:\n            seen.add(topic)\n            unique_topics.append(topic)\n\
        \    \n    # Meeting purpose statement (per PRD, standard for senior managers)\n\
        \    purpose = \"This note summarizes the purpose and main discussion topics\
        \ for the upcoming senior management meeting.\"\n    \n    # Present topics\
        \ as a bullet list\n    if unique_topics:\n        topics_section = \"\\nDiscussion\
        \ Topics:\"\n        for idx, topic in enumerate(unique_topics, 1):\n    \
        \        topics_section += f\"\\n  {idx}. {topic}\"\n    else:\n        topics_section\
        \ = \"\\n(No discussion topics were provided.)\"\n    \n    # Compose final\
        \ note\n    note = f\"{purpose}{topics_section}\"\n    return note\n"
      name: compose_meeting_note
      nodes_depended_on: []
      nodes_dependent_on: []
      output_structure:
      - description: Output of type str
        key: output
        type: str
      - description: Input parameter of type str
        key: topics
        type: str
      prd:
        bullets:
        - complexity: LOW
          impact: Improves alignment and preparedness among stakeholders by providing
            a focused and professional meeting agenda.
          method: Use string formatting or template-based text generation to assemble
            the meeting note from the provided topic list and a standard meeting purpose
            statement.
          reason: Ensures that the meeting note is easily understood by all senior
            managers and sets clear expectations for the meeting discussion.
          text: Create a natural language template that succinctly explains the meeting
            purpose and presents the topics in a clear, bullet-point or listed format.
        - complexity: LOW
          impact: Ensures the meeting note is easy to act upon and communicates the
            intended topics without redundancy.
          method: Transform the topics parameter into a unique, ordered list prior
            to inclusion in the output text.
          reason: Prevents confusion or ambiguity in the meeting agenda by guaranteeing
            each topic is clearly presented once.
          text: Validate and sanitize the input topic list to prevent duplication
            and ensure clarity before rendering in the meeting note.
      prompt: Compose a brief, executive-level meeting note addressed to senior managers.
        The note should summarize the primary purpose of the meeting and clearly list
        the main topics to be discussed. Use the list of topics provided as the discussion
        agenda. Ensure the tone is professional and suitable for inclusion as a note
        in a Product Requirements Document (PRD).
      shims: []
    - agent: false
      code_node_type: virtual-pure
      description: This node constructs a cohesive executive summary for a PRD by
        combining a lead-in, user roles, features, and a meeting note into a unified
        narrative.
      implementation: "def assemble_executive_summary(lead_in: str, user_roles_section:\
        \ str, feature_section: str, meeting_note: str) -> str:\n    \"\"\"\n    This\
        \ node constructs a cohesive executive summary for a PRD by combining a lead-in,\
        \ user roles, features, and a meeting note into a unified narrative.\n\n \
        \   Args:\n        lead_in: Input parameter of type str\nuser_roles_section:\
        \ Input parameter of type str\nfeature_section: Input parameter of type str\n\
        meeting_note: Input parameter of type str\n\n    Returns:\n        str: Output\
        \ of type str\n    \"\"\"\n    import re\n    \n    def harmonize_style(text:\
        \ str) -> str:\n        \"\"\"\n        Basic style harmonization: unify whitespace,\
        \ use consistent periods, standardize simple tense (e.g., present tense only\
        \ when possible), basic title case for headers if formatted.\n        This\
        \ is intentionally light-touch to meet 'OPTIONAL, MEDIUM' requirement - a\
        \ real stylistic aligner would require NLP libraries.\n        \"\"\"\n  \
        \      # Fix excessive whitespace\n        result = re.sub(r'[ \\t]+', ' ',\
        \ text)\n        # Standardize line breaks\n        result = re.sub(r' *\\\
        n{2,}', '\\n\\n', result)\n        # Remove accidental double periods/commas\n\
        \        result = re.sub(r'[\\.]{2,}', '.', result)\n        result = re.sub(r',[,]+',\
        \ ',', result)\n        # Standardize bullet points (if any)\n        result\
        \ = re.sub(r'^[ \\t]*[-\u2022][ \\t]*', '- ', result, flags=re.MULTILINE)\n\
        \        # Consistent spacing after periods\n        result = re.sub(r'\\\
        .([A-Za-z])', '. \\1', result)\n        # Consistency: use \"users\" instead\
        \ of \"user\" when referring to group roles\n        result = re.sub(r'\\\
        bUser Roles?\\b', 'User Roles', result)\n        result = re.sub(r'\\buser\
        \ roles?\\b', 'User Roles', result)\n        # Normalize casing for section\
        \ headers\n        def titlecase_header(match):\n            return match.group(1).title()\
        \ + match.group(2)\n        result = re.sub(r'^(Lead-In)(:?)', titlecase_header,\
        \ result, flags=re.IGNORECASE|re.MULTILINE)\n        result = re.sub(r'^(User\
        \ Roles)(:?)', titlecase_header, result, flags=re.IGNORECASE|re.MULTILINE)\n\
        \        result = re.sub(r'^(Features)(:?)', titlecase_header, result, flags=re.IGNORECASE|re.MULTILINE)\n\
        \        result = re.sub(r'^(Meeting Note)(:?)', titlecase_header, result,\
        \ flags=re.IGNORECASE|re.MULTILINE)\n        return result.strip()\n\n   \
        \ # Format each section with clear heading\n    sections = []\n    if lead_in.strip():\n\
        \        sections.append(\"Lead-In:\\n\" + lead_in.strip())\n    if user_roles_section.strip():\n\
        \        sections.append(\"User Roles:\\n\" + user_roles_section.strip())\n\
        \    if feature_section.strip():\n        sections.append(\"Features:\\n\"\
        \ + feature_section.strip())\n    if meeting_note.strip():\n        sections.append(\"\
        Meeting Note:\\n\" + meeting_note.strip())\n    \n    # Merge sections with\
        \ double line breaks for clarity\n    summary = \"\\n\\n\".join(sections)\n\
        \    # Optionally harmonize style if input is moderately inconsistent\n  \
        \  summary = harmonize_style(summary)\n    return summary\n"
      name: assemble_executive_summary
      nodes_depended_on: []
      nodes_dependent_on: []
      output_structure:
      - description: Output of type str
        key: output
        type: str
      - description: Input parameter of type str
        key: lead_in
        type: str
      - description: Input parameter of type str
        key: user_roles_section
        type: str
      - description: Input parameter of type str
        key: feature_section
        type: str
      - description: Input parameter of type str
        key: meeting_note
        type: str
      prd:
        bullets:
        - complexity: LOW
          impact: Produces a complete, stakeholder-ready summary that reflects all
            essential preliminary PRD items.
          method: Format the inputs with clear headings or transitions and merge them
            using string interpolation and straightforward templating.
          reason: Ensures all critical PRD elements are coherently presented in a
            single narrative for stakeholders.
          text: "Concatenate the four input sections\u2014lead-in, user roles, features,\
            \ and meeting note\u2014into a logically structured executive summary."
        - complexity: LOW
          impact: Facilitates quick understanding and sign-off from executive and
            product stakeholders.
          method: "Apply a standard section order\u2014lead-in, user roles, features,\
            \ and meeting note\u2014and use formatting to highlight section transitions."
          reason: Ordering and clarity are essential for stakeholder comprehension
            and review efficiency.
          text: Preserve the input section ordering and maintain content clarity within
            the executive summary.
        - complexity: MEDIUM
          impact: Improves the document's professionalism and increases trust among
            executive readers.
          method: Apply basic post-processing (such as simple NLP tools or template-based
            editing) to harmonize section tone and terminology.
          reason: Consistent style increases perceived professionalism and readability
            of the PRD summary.
          text: Optionally, ensure minor stylistic uniformity (tone, tense, terminology)
            across sections if input style is inconsistent.
      prompt: Assemble a polished executive summary for the PRD document by integrating
        the provided lead-in statement, a defined user roles section, a key features
        section, and a brief note outlining the agenda and purpose for an upcoming
        meeting with senior managers.
      shims: []
  extract_constraints:
    agent: false
    code_node_type: pure
    description: Extract explicit technical or business constraints from the requirements,
      and identify discussion points for a meeting with senior managers. Both sets
      of outputs will be used as key inputs for identifying success metrics and ensuring
      stakeholder alignment.
    implementation: "from pydantic import BaseModel, Field\nfrom typing import List\n\
      \n\nclass ExtractConstraintsOutput(BaseModel):\n    \"\"\"Pydantic model for\
      \ extract_constraints node outputs.\"\"\"\n    constraints: List[str] = Field(...,\
      \ description=\"List of explicit technical or business constraints applicable\
      \ to the workflow, structured for direct use in identifying and defining success\
      \ metrics.\")\n    manager_discussion_points: List[str] = Field(..., description=\"\
      List of items or constraints requiring clarification, approval, or input from\
      \ senior managers, to be used as an agenda for a dedicated meeting.\")\n\n\n\
      def extract_constraints_fx(general_input: str, **kwargs) -> ExtractConstraintsOutput:\n\
      \    \"\"\"Extract explicit technical or business constraints from the requirements,\
      \ and identify discussion points for a meeting with senior managers. Both sets\
      \ of outputs will be used as key inputs for identifying success metrics and\
      \ ensuring stakeholder alignment.\n\n    Args:\n        general_input: General\
      \ input string for the root node.\n        **kwargs: Additional keyword arguments.\n\
      \n    Returns:\n        ExtractConstraintsOutput: Object containing outputs\
      \ for this node.\n    \"\"\"\n    # Step 1: Clean and preprocess the input text\n\
      \    preprocessed_text: str = preprocess_requirements_text(raw_input=general_input)\n\
      \n    # Step 2: Extract candidate constraints using rule-based and pattern matching\n\
      \    candidate_constraints: List[str] = extract_explicit_constraints(text=preprocessed_text,\
      \ matchers=[\"must\", \"should\", \"shall\", \"limited to\", \"deadline\", \"\
      no more than\"])  # type: ignore\n\n    # Step 3: Filter out implied or ambiguous\
      \ constraints (retain only direct evidence)\n    explicit_constraints: List[str]\
      \ = filter_for_explicitness(constraints=candidate_constraints, text=preprocessed_text)\n\
      \n    # Step 4: Standardize/format constraints for structured output\n    structured_constraints:\
      \ List[str] = format_and_validate_constraints(constraints=explicit_constraints)\n\
      \n    # Step 5: Identify discussion points: constraints/items needing clarification\
      \ or executive approval\n    manager_discussion_points: List[str] = extract_manager_discussion_points(text=preprocessed_text,\
      \ \n        triggers=[\"to be confirmed\", \"subject to approval\", \"pending\
      \ clarification\", \"TBD\", \"to be decided\"])\n\n    # Step 6: Log/audit results\
      \ for compliance traceability\n    log_extraction_audit(extracted_constraints=structured_constraints,\
      \ discussion_points=manager_discussion_points, original_input=general_input)\n\
      \n    # Step 7 (for extensibility): optionally update/configure patterns for\
      \ future regulatory/technical categories\n    update_extraction_config_if_needed(kwargs=kwargs)\n\
      \n    return ExtractConstraintsOutput(constraints=structured_constraints, manager_discussion_points=manager_discussion_points)\n"
    name: extract_constraints
    nodes_depended_on: []
    nodes_dependent_on: []
    output_structure:
    - description: List of explicit technical or business constraints applicable to
        the workflow, structured for direct use in identifying and defining success
        metrics.
      key: constraints
      type: List[str]
    - description: List of items or constraints requiring clarification, approval,
        or input from senior managers, to be used as an agenda for a dedicated meeting.
      key: manager_discussion_points
      type: List[str]
    prd:
      bullets:
      - complexity: MEDIUM - Requires natural language processing for precision but
          focuses on explicitly stated constraints, not inference.
        impact: HIGH - Accurate extraction of constraints is critical for compliance,
          risk identification, and downstream metric definitions.
        method: Use rule-based keyword/phrase matching for common constraint types
          (e.g., 'must', 'deadline', 'limited to') combined with pattern recognition
          for structured requirements.
        reason: A deterministic parsing component ensures only clearly stated constraints
          are extracted, providing reliability and consistency across various types
          of input requirements.
        text: Implement a requirements parser that systematically scans input documents
          for all explicit technical or business constraints.
      - complexity: LOW - Filtering logic relies on direct textual evidence.
        impact: HIGH - Increases transparency and trust in the constraint list; avoids
          contaminating metrics and risk analysis.
        method: Apply confidence scoring or annotation logic, omitting any constraint
          not explicitly stated (i.e., require explicit textual justification for
          every extracted constraint).
        reason: Ensures that only validated, unambiguous constraints are propagated,
          reducing noise and risk of error in later workflow steps.
        text: Explicitly filter out implied or ambiguous constraints to prevent accidental
          inclusion in downstream processes.
      - complexity: LOW - Mainly involves formatting and standardization.
        impact: MEDIUM - Structured constraints enable downstream automation and reduce
          manual intervention.
        method: 'Standardize output fields with templates such as ''[Constraint Type]:
          [Requirement Text]''; perform validation to check for specificity and consistency.'
        reason: Granular, well-worded constraints facilitate traceability and reuse
          across the workflow, particularly for automated metric mapping.
        text: Format each constraint as a clearly structured, granular item for straightforward
          reference in success metrics and risk analysis.
      - complexity: MEDIUM - Requires semantic analysis and possibly a prompt to the
          user when intent or approval is unclear.
        impact: HIGH - Facilitates stakeholder alignment and prevents future blockers;
          creates accountability loops.
        method: Use heuristics to check for phrases such as 'to be confirmed', 'subject
          to approval', 'pending clarification', and aggregate these into a manager_discussion_points
          list; use flagged constraints as triggers for agenda items.
        reason: Bringing uncertainties and decision points to the attention of senior
          managers ensures fast resolution and shared alignment at the leadership
          level.
        text: Identify and surface any constraint or item that is unclear, requires
          additional context, or explicit executive approval, and consolidate these
          into a discussion agenda for senior managers.
      - complexity: MEDIUM - Needs modular coding practices and configuration-driven
          parsing.
        impact: MEDIUM - Future-proofs the node and reduces technical debt.
        method: Design modular extraction functions and maintain a configuration map
          for constraint categories and agenda triggers. Document extension points
          for new regulation types or review triggers.
        reason: "Workflows and requirements may evolve\u2014having extensible logic\
          \ allows the parser and agenda generator to scale with minimal redevelopment."
        text: Support easy extension for future regulatory or technical category inputs,
          enabling adaptability to changing business or platform constraints.
      - complexity: LOW - Leverages existing logging libraries and simple object persistence.
        impact: MEDIUM - Useful for compliance, transparency, and workflow tuning.
        method: Record each extracted item with timestamps and context; retain both
          lists (constraints, discussion points) as versioned records associated with
          each workflow execution.
        reason: Auditability is essential for compliance standards and facilitates
          debugging or retrospective reviews if requirements change.
        text: Integrate comprehensive logging and auditing of both extracted constraints
          and flagged manager discussion points.
    prompt: "Read the provided requirements and list all explicit technical or business\
      \ constraints (such as regulatory, platform limitations, deadlines, etc) that\
      \ must be satisfied by the PRD. Omit any implied or unstated constraints. Ensure\
      \ each listed constraint is clearly and specifically formulated so they can\
      \ be referenced as key inputs for identifying and defining success metrics later\
      \ in the workflow. \n\nAdditionally, after compiling the constraints, note any\
      \ items that require clarification or approval from senior managers. Prepare\
      \ a summary list of these discussion points to be covered in a dedicated meeting\
      \ with senior managers as part of the workflow. \n\nOutput the explicit technical\
      \ or business constraints first, followed by a short list of discussion items\
      \ needing senior manager input."
    shims:
    - agent: false
      code_node_type: virtual-pure
      description: This node standardizes and cleans raw requirements text by removing
        extraneous formatting, normalizing structure, and preparing it for accurate
        downstream constraint extraction.
      implementation: "def preprocess_requirements_text(raw_input: str) -> str:\n\
        \    \"\"\"\n    This node standardizes and cleans raw requirements text by\
        \ removing extraneous formatting, normalizing structure, and preparing it\
        \ for accurate downstream constraint extraction.\n\n    Args:\n        raw_input:\
        \ Input parameter of type str\n\n    Returns:\n        str: Output of type\
        \ str\n    \"\"\"\n    import re\n    import unicodedata\n\n    text = raw_input\n\
        \n    # Unicode normalization to NFC\n    text = unicodedata.normalize('NFC',\
        \ text)\n\n    # Remove non-printable and control Unicode characters\n   \
        \ # (keep tabs, newlines for now for structure)\n    text = re.sub(r'[\\x00-\\\
        x08\\x0b-\\x0c\\x0e-\\x1f\\x7f-\\x9f]', '', text)\n\n    # Typographic corrections:\
        \ replace fancy quotes/dashes with standard ones\n    quotes_map = {\n   \
        \     '\u201C': '\"', '\u201D': '\"', \"\u2018\": \"'\", \"\u2019\": \"'\"\
        , '\xAB': '\"', '\xBB': '\"',\n        '\u2013': '-', '\u2014': '-', '\u2212\
        ': '-', '\u2010': '-', '\u2011': '-',\n        '\u2026': '...',\n    }\n \
        \   for orig, repl in quotes_map.items():\n        text = text.replace(orig,\
        \ repl)\n\n    # Standardize bullet points and numbering schemes\n    # Convert\
        \ numbered lists and various bullets to a standard '- '\n    # Examples to\
        \ handle: '*', '\u2022', '-', '\u2013', 'a)', '1.', etc. at line starts\n\
        \    bullet_patterns = [\n        r'^[\\s]*(?:[\\*\\-\u2022\u2023])\\s+',\
        \    # e.g. * Text / \u2022 Text / - Text / \u2023 Text\n        r'^[\\s]*(\\\
        d+|[a-zA-Z])[.).]([ \\t])',  # 1. Text / 1) Text / a. Text / b) Text\n   \
        \     r'^[\\s]+-\\s+',                # indented bullets\n    ]\n    def bullet_sub(match):\n\
        \        return '- '\n    for pat in bullet_patterns:\n        text = re.sub(pat,\
        \ '- ', text, flags=re.MULTILINE)\n\n    # Standardize sentence delimiters:\
        \ Remove excess line breaks (collapse >2 newlines into 2)\n    text = re.sub(r'\\\
        n{3,}', '\\n\\n', text)\n\n    # Collapse multiple spaces/tabs into single\
        \ space (but preserve newlines)\n    text = re.sub(r'[ \\t]{2,}', ' ', text)\n\
        \n    # Remove trailing whitespace on each line\n    text = re.sub(r'[ \\\
        t]+$', '', text, flags=re.MULTILINE)\n\n    # Remove empty bullet lines (e.g.,\
        \ '-   ')\n    text = re.sub(r'^-\\s*$', '', text, flags=re.MULTILINE)\n\n\
        \    # Remove any residual non-standard ASCII (e.g., non-breaking space)\n\
        \    text = text.replace('\\u00A0', ' ')\n\n    # Strip leading/trailing whitespace/newlines\n\
        \    cleaned = text.strip()\n\n    return cleaned\n"
      name: preprocess_requirements_text
      nodes_depended_on: []
      nodes_dependent_on: []
      output_structure:
      - description: Output of type str
        key: output
        type: str
      - description: Input parameter of type str
        key: raw_input
        type: str
      prd:
        bullets:
        - complexity: LOW
          impact: Improves the reliability and accuracy of constraint and discussion
            point extraction by providing a clean, uniform text structure.
          method: Implement text preprocessing routines using standard regular expressions,
            Unicode normalization, and whitespace collapsing.
          reason: To ensure the requirements text is compatible with downstream NLP
            and pattern-matching modules, which can be disrupted by noisy or inconsistent
            formatting.
          text: Remove non-standard characters, excessive whitespace, and extraneous
            formatting from the input requirements text.
        - complexity: MEDIUM
          impact: Facilitates accurate splitting of requirements and ensures that
            each distinct constraint or point is clearly separated.
          method: Develop deterministic rules or use rule-based libraries to convert
            all common list/numbering formats and delimiters into a consistent structure.
          reason: Many requirements documents use varying schemes for organization,
            which can impede sentence/phrase-level analysis if not standardized.
          text: Standardize structural elements such as sentence delimiters, bullet
            points, and numbering schemes.
        - complexity: LOW
          impact: Ensures robust tokenization and parsing in follow-on logic, reducing
            error rates.
          method: Integrate open-source text tidying libraries or custom regex substitutions
            to handle frequent typographic issues.
          reason: Inconsistencies in typography can break tokenization and result
            in missed or fragmented extractions.
          text: Apply typographic corrections such as fixing common spacing errors,
            replacing fancy quotes or dashes, and collapsing inconsistent newlines.
      prompt: 'Clean, normalize, and preprocess a freeform requirements or specification
        input string: remove extraneous formatting (such as non-UTF characters, excess
        whitespace, repeated punctuation), standardize bullet points and sentence
        delimiters, correct common typographic issues, collapse multiple lines, and
        output a version of the input suitable for downstream NLP, entity extraction,
        or rule-based constraint recognition components.'
      shims: []
    - agent: false
      code_node_type: virtual-pure
      description: Extracts all explicit technical or business constraints from input
        text using pattern-based matchers, returning each constraint as a structured
        list of strings.
      implementation: "def extract_explicit_constraints(text: str, matchers: str)\
        \ -> List[str]:\n    \"\"\"\n    Extracts all explicit technical or business\
        \ constraints from input text using pattern-based matchers, returning each\
        \ constraint as a structured list of strings.\n\n    Args:\n        text:\
        \ Input parameter of type str\nmatchers: Input parameter of type str\n\n \
        \   Returns:\n        List[str]: Output of type List[str]\n    \"\"\"\n  \
        \  import re\n    from typing import List\n    \n    # --- Step 1: Parse the\
        \ matchers string ---\n    # Assume matchers are comma, semicolon, or newline-separated\
        \ (configurable string input)\n    if not matchers or not isinstance(matchers,\
        \ str):\n        matcher_list: List[str] = []\n    else:\n        matcher_list\
        \ = [m.strip() for m in re.split(r'[;,\\n]', matchers) if m.strip()]\n\n \
        \   if not matcher_list:\n        return []\n\n    # --- Step 2: Build regular\
        \ expressions for each matcher (case-insensitive, word-bound matched if needed)\
        \ ---\n    # Prefer to match full sentences or phrases containing the matcher\
        \ keyword\n    constraint_patterns = []\n    for keyword in matcher_list:\n\
        \        # Escape regex special chars in matcher keyword\n        escaped\
        \ = re.escape(keyword)\n        # Build patterns that capture sentences containing\
        \ the matcher (basic sentence boundaries, liberal)\n        # e.g., 'must',\
        \ 'shall', etc. -- match the whole sentence containing the keyword\n     \
        \   # Use (?i) for case-insensitivity, [^.!?]*? to stop at next sentence-ending\
        \ punct.\n        pattern = rf'(?i)([^.!?]*\\b{escaped}\\b[^.!?]*[.!?])'\n\
        \        constraint_patterns.append(pattern)\n    \n    # --- Step 3: Extract\
        \ candidate constraints using the patterns ---\n    explicit_constraints =\
        \ set()\n    for pattern in constraint_patterns:\n        for match in re.finditer(pattern,\
        \ text):\n            constraint_candidate = match.group(0).strip()\n    \
        \        # --- Step 4: Validation - Ensure unambiguous/explicit language ---\n\
        \            # Accept only constraints that use clear modal verbs ('must',\
        \ 'shall', 'is required to', 'cannot', etc.)\n            # and avoid those\
        \ starting with guesswork words (e.g. 'should', 'could', 'might')\n      \
        \      constraint_lc = constraint_candidate.lower()\n            explicit_keywords\
        \ = [\n                'must', 'shall', 'is required to', 'are required to',\n\
        \                'cannot', 'may not', 'will not', 'is not allowed', 'are not\
        \ allowed'\n            ]\n            ambiguous_filters = [\n           \
        \     'should', 'could', 'might', 'may', 'suggests', 'recommend', 'prefer'\n\
        \            ]\n            # Must contain at least one explicit keyword\n\
        \            if any(kw in constraint_lc for kw in explicit_keywords):\n  \
        \              # Exclude if it contains ambiguity words\n                if\
        \ not any(ambig in constraint_lc for ambig in ambiguous_filters):\n      \
        \              explicit_constraints.add(constraint_candidate)\n    \n    #\
        \ Return as a sorted list for determinism\n    return sorted(explicit_constraints)\n"
      name: extract_explicit_constraints
      nodes_depended_on: []
      nodes_dependent_on: []
      output_structure:
      - description: Output of type List[str]
        key: output
        type: List[str]
      - description: Input parameter of type str
        key: text
        type: str
      - description: Input parameter of type str
        key: matchers
        type: str
      prd:
        bullets:
        - complexity: MEDIUM
          impact: Ensures consistent and reliable extraction of actionable constraints
            necessary for workflow compliance and metric definition.
          method: Utilize regular expressions or natural language processing (NLP)
            rule-based methods to identify and extract sentences or phrases matching
            the specified matcher keywords.
          reason: Pattern-based extraction enables precise identification of constraints
            that adhere to specific business or technical language constructs.
          text: Implement pattern and keyword-based extraction of explicit constraints
            from the input text using a configurable set of matchers.
        - complexity: MEDIUM
          impact: Reduces false positives and aligns extracted constraints with compliance
            and success metric needs.
          method: Incorporate a post-processing validation step that checks extracted
            matches against the original text context for unambiguous language, possibly
            through regex validation and exclusion filters.
          reason: Distinguishing explicit constraints prevents the inclusion of uncertain
            or non-actionable requirements, improving downstream process reliability.
          text: "Design the function to return only those constraints with direct,\
            \ unambiguous evidence in the text\u2014excluding implied or ambiguous\
            \ statements."
        - complexity: LOW
          impact: Allows the extraction algorithm to remain effective and adaptable
            over time, minimizing manual rework.
          method: Accept matcher lists as function parameters and document interface
            for easy extension or integration with external configuration sources.
          reason: Matcher flexibility is required to accommodate changing language
            and evolving standards in requirements gathering.
          text: Support configurability in matchers, enabling updates for evolving
            business or regulatory patterns.
      prompt: Extract all explicit technical or business constraints from the provided
        requirements text, using the supplied matchers to guide identification, and
        output each distinct constraint as a string in a list.
      shims: []
    - agent: false
      code_node_type: virtual-pure
      description: This node filters a list of candidate constraint strings and returns
        only those constraints that are explicit (i.e., directly and unambiguously
        stated) within the provided input text.
      implementation: "def filter_for_explicitness(constraints: str, text: str) ->\
        \ List[str]:\n    \"\"\"\n    This node filters a list of candidate constraint\
        \ strings and returns only those constraints that are explicit (i.e., directly\
        \ and unambiguously stated) within the provided input text.\n\n    Args:\n\
        \        constraints: Input parameter of type str\ntext: Input parameter of\
        \ type str\n\n    Returns:\n        List[str]: Output of type List[str]\n\
        \    \"\"\"\n    # PURE IMPLEMENTATION (SHIMS REPLACED)\n    import re\n \
        \   from typing import List\n\n    # --- Step 1: Parse the input 'constraints'\
        \ string to get candidate constraints ---\n    # We'll try splitting by newlines\
        \ or semicolons. If comma-separated, split by commas as fallback.\n    # Assume\
        \ that constraints are separated by newline, semicolon or comma.\n    raw\
        \ = constraints.strip()\n    if not raw:\n        return []\n    if '\\n'\
        \ in raw:\n        constraint_list = [c.strip() for c in raw.split('\\n')\
        \ if c.strip()]\n    elif ';' in raw:\n        constraint_list = [c.strip()\
        \ for c in raw.split(';') if c.strip()]\n    else:\n        constraint_list\
        \ = [c.strip() for c in raw.split(',') if c.strip()]\n\n    # --- Step 2:\
        \ Define signal phrases to check explicitness ---\n    # Chosen based on the\
        \ PRD: strong, explicit, deontic language\n    signal_phrases = [\n      \
        \  r'\\bmust\\b', r'\\bshall\\b', r'\\brequired to\\b', r'\\bno more than\\\
        b',\n        r'\\bmay not\\b', r'\\bshould\\b', r'\\bis prohibited\\b', r'\\\
        bnot allowed\\b',\n        r'\\bhas to\\b', r'\\bare not permitted\\b', r'\\\
        bmay only\\b', r'\\bwill not\\b',\n        r'\\bare required\\b', r'\\bare\
        \ prohibited\\b', r'\\bordered to\\b'\n    ]\n    # Compile regex patterns,\
        \ case-insensitive\n    compiled_signal_patterns = [re.compile(pat, re.IGNORECASE)\
        \ for pat in signal_phrases]\n\n    # --- Step 3: For each candidate constraint,\
        \ check explicitness in the input text ---\n    explicit_constraints = []\n\
        \    for constraint in constraint_list:\n        # Check if the raw constraint\
        \ text appears explicitly (or a close variant) in the main text\n        constraint_clean\
        \ = ' '.join(constraint.lower().split())\n        text_clean = ' '.join(text.lower().split())\n\
        \        found_exact = False\n        # Try exact phrase match (ignoring case\
        \ and whitespace differences)\n        if constraint_clean and constraint_clean\
        \ in text_clean:\n            found_exact = True\n        # Otherwise, look\
        \ for the constraint as a fuzzy substring with a strong signal phrase\n  \
        \      found_pattern = False\n        for pattern in compiled_signal_patterns:\n\
        \            if pattern.search(constraint):\n                # If the constraint\
        \ uses a strong phrase, check if it appears in text\n                regex_match\
        \ = re.search(re.escape(constraint), text, re.IGNORECASE)\n              \
        \  if regex_match:\n                    found_pattern = True\n           \
        \         break\n                # Alternatively, check if the signal phrase\
        \ appears in text near a similar constraint phrase\n                # by searching\
        \ for sentences in text with the signal phrase and a majority of keywords\
        \ in the constraint\n                # (simple keyword intersection)\n   \
        \             # Break constraint and text into lower tokens\n            \
        \    constraint_words = set([w for w in re.findall(r'\\w+', constraint_clean)\
        \ if w not in {'must', 'shall', 'required', 'no', 'than', 'is', 'are', 'not',\
        \ 'will', 'to', 'be', 'may', 'should'}])\n                # Find sentences\
        \ in text with the signal phrase\n                for sentence in re.split(r'[.!?]',\
        \ text):\n                    if pattern.search(sentence):\n             \
        \           sent_clean = ' '.join(sentence.lower().split())\n            \
        \            sent_words = set(re.findall(r'\\w+', sent_clean))\n         \
        \               # At least half the constraint words must appear in the sentence\n\
        \                        if constraint_words and len(constraint_words & sent_words)\
        \ >= max(1, len(constraint_words)//2):\n                            found_pattern\
        \ = True\n                            break\n                if found_pattern:\n\
        \                    break\n        if found_exact or found_pattern:\n   \
        \         explicit_constraints.append(constraint)\n\n    # --- Step 4: Remove\
        \ constraints that are implied, ambiguous, or inferential ---\n    # Here,\
        \ we remove constraints that do not have strong phrasing or are too generic\
        \ (e.g., implied by context but not stated)\n    # For simplicity, if none\
        \ of the signal phrases occur in either the constraint or the matching text,\
        \ consider it ambiguous\n    final_constraints = []\n    for constraint in\
        \ explicit_constraints:\n        # Strong phrasing required\n        has_signal\
        \ = any(pat.search(constraint) for pat in compiled_signal_patterns)\n    \
        \    if has_signal:\n            final_constraints.append(constraint)\n  \
        \      else:\n            # See if the raw appeared in text with a strong\
        \ phrase\n            matched = False\n            for pat in compiled_signal_patterns:\n\
        \                # Look for a sentence in text that contains both the constraint\
        \ (loosely) and the signal phrase\n                for sentence in re.split(r'[.!?]',\
        \ text):\n                    if pat.search(sentence):\n                 \
        \       constraint_words = set(re.findall(r'\\w+', constraint.lower()))\n\
        \                        sent_words = set(re.findall(r'\\w+', sentence.lower()))\n\
        \                        if constraint_words and len(constraint_words & sent_words)\
        \ >= max(1, len(constraint_words)//2):\n                            matched\
        \ = True\n                            break\n                if matched:\n\
        \                    break\n            if matched:\n                final_constraints.append(constraint)\n\
        \            # If not matched, consider ambiguous; ignore\n\n    # --- Step\
        \ 5: Deduplicate and canonicalize output ---\n    deduped = list(dict.fromkeys(final_constraints))\
        \  # dedupe, preserve order\n    # Optionally, sort alphabetically if canonical\
        \ order required:\n    deduped_sorted = sorted(deduped, key=lambda x: x.lower())\n\
        \    return deduped_sorted\n"
      name: filter_for_explicitness
      nodes_depended_on: []
      nodes_dependent_on: []
      output_structure:
      - description: Output of type List[str]
        key: output
        type: List[str]
      - description: Input parameter of type str
        key: constraints
        type: str
      - description: Input parameter of type str
        key: text
        type: str
      prd:
        bullets:
        - complexity: MEDIUM
          impact: Improves accuracy and reliability of extracted constraints, supporting
            clear success metrics and traceability.
          method: Apply string matching and keyword pattern detection, possibly supplemented
            by regex or basic NLP to verify that each candidate is asserted explicitly
            in the text.
          reason: Ensures only constraints with direct textual evidence are included,
            reducing ambiguity in subsequent workflows.
          text: Implement a comparison mechanism that checks each candidate constraint
            against the input text for explicit language and direct statement.
        - complexity: MEDIUM
          impact: Clarifies the operational limits of the workflow, supporting compliance,
            auditability, and actionable requirements.
          method: Identify and remove constraints lacking strong signal phrases (e.g.,
            'must', 'shall', 'no more than'), possibly using rule-based filters and
            stop-phrase lists.
          reason: Prevents inclusion of assumptions or loosely supported requirements
            that could cause confusion or misalignment.
          text: Exclude constraints that are implied, ambiguous, or require inferential
            reasoning.
        - complexity: LOW
          impact: Enables seamless integration with formatting, validation, and reporting
            nodes.
          method: After filtering, de-duplicate the results and format as a plain
            list in canonical order.
          reason: Supports consistent, structured downstream processing without noise
            or redundancy.
          text: Ensure the output is a clean, deduplicated list containing only those
            constraints that passed explicitness checks.
      prompt: Given a list of candidate constraints and the original requirements
        text, return only those constraints that are explicitly and unambiguously
        stated in the text (i.e., those with direct textual evidence and no significant
        ambiguity or implication). For each candidate, verify its presence and explicitness
        in the text, and output a list of only those that qualify as explicit constraints.
      shims: []
    - agent: false
      code_node_type: virtual-pure
      description: This node standardizes the format and performs validation on a
        list of explicit technical or business constraints to ensure they are well-structured,
        unambiguous, and ready for downstream use.
      implementation: "def format_and_validate_constraints(constraints: str) -> List[str]:\n\
        \    \"\"\"\n    This node standardizes the format and performs validation\
        \ on a list of explicit technical or business constraints to ensure they are\
        \ well-structured, unambiguous, and ready for downstream use.\n\n    Args:\n\
        \        constraints: Input parameter of type str\n\n    Returns:\n      \
        \  List[str]: Output of type List[str]\n    \"\"\"\n    import re\n    from\
        \ typing import List\n\n    def split_constraints(raw: str) -> List[str]:\n\
        \        # Try splitting by newlines; fallback to semicolons if only one line\n\
        \        lines = [line.strip() for line in raw.strip().split('\\n') if line.strip()]\n\
        \        if len(lines) <= 1:\n            # Probably a semicolon/period separated\
        \ list\n            lines = [c.strip() for c in re.split(r'[;.]', raw) if\
        \ c.strip()]\n        return lines\n\n    def normalize_constraint(text: str)\
        \ -> str:\n        # Lowercase, strip, remove excess whitespace\n        text\
        \ = text.strip()\n        # Remove double spaces\n        text = re.sub(r'\\\
        s+', ' ', text)\n        # Capitalize for readability\n        text = text[0].upper()\
        \ + text[1:] if text else text\n        # Remove trailing periods (unless\
        \ marking abbreviations)\n        if text.endswith('.') and not re.search(r'\\\
        b(?:Inc|Ltd|e\\.g|i\\.e)\\.$', text):\n            text = text[:-1]\n    \
        \    \n        # Normalize common phrasing into imperative, e.g., 'Should\
        \ ...' -> 'Ensure ...'\n        text = re.sub(r\"^(Should|The system should|The\
        \ solution should|It should)\\\\s+\", \"Ensure \", text, flags=re.I)\n   \
        \     text = re.sub(r\"^(Must|The system must|The product must)\\\\s+\", \"\
        Ensure \", text, flags=re.I)\n        text = re.sub(r\"^(Required to|Is required\
        \ to)\\\\s+\", \"Ensure \", text, flags=re.I)\n        text = re.sub(r\"^(There\
        \ should be|There must be)\\\\s+\", \"Ensure there is \", text, flags=re.I)\n\
        \        text = re.sub(r\"^Ability to \", \"Allow to \", text, flags=re.I)\n\
        \        # Convert \"shall\" to imperative\n        text = re.sub(r'\\bshall\\\
        b', 'must', text, flags=re.I)\n        # Remove redundant phrases\n      \
        \  text = re.sub(r\"in order to \", \"to \", text, flags=re.I)\n        return\
        \ text\n\n    def is_explicit(clause: str) -> bool:\n        # Returns True\
        \ if the constraint avoids vague terms and is actionable\n        vague_patterns\
        \ = [r'\\bconsider\\b', r'\\btry to\\b', r'\\bas appropriate\\b', r'\\bwhere\
        \ possible\\b', r'\\bgenerally\\b', r'\\bas needed\\b', r'\\bif required\\\
        b', r'\\bshould aim to\\b', r'\\bif applicable\\b', r'\\bas necessary\\b']\n\
        \        for pattern in vague_patterns:\n            if re.search(pattern,\
        \ clause, flags=re.I):\n                return False\n        # Must contain\
        \ a verb (simple check: starts with verb-like word)\n        actions = [\"\
        ensure\", \"allow\", \"support\", \"prevent\", \"use\", \"limit\", \"maintain\"\
        , \"restrict\", \"provide\", \"enforce\", \"require\", \"implement\", \"log\"\
        , \"notify\", \"display\", \"keep\", \"validate\", \"comply\", \"store\",\
        \ \"encrypt\", \"authenticate\", \"authorize\", \"record\", \"archive\", \"\
        audit\"]\n        first_word = clause.split(' ')[0].lower()\n        if first_word\
        \ not in actions:\n            # Could be valid, but we'll want to be strict\n\
        \            return False\n        return True\n\n    def is_measurable(clause:\
        \ str) -> bool:\n        # Constraint is measurable if it has numeric/quantitative\
        \ or testable condition\n        quantitative_patterns = [\n            r'\\\
        bwithin \\d+\\s*(seconds|minutes|hours|days)\\b',\n            r'\\bno more\
        \ than \\d+\\b',\n            r'\\bat least \\d+\\b',\n            r'\\bnot\
        \ exceed(s|ing)? \\d+\\b',\n            r'\\bless than \\d+\\b',\n       \
        \     r'\\bgreater than \\d+\\b',\n            r'\\bbox\\b',\n           \
        \ r'\\bminimum\\b',\n            r'\\bmaximum\\b',\n            r'\\bper (user|second|request|day|month)\\\
        b',\n            r'\\bpass(es|ing)? (all|the) tests?\\b',\n            r'\\\
        bencrypt(ed|ion)?\\b',\n            r'\\blog(ged|ging)?\\b',\n           \
        \ r'\\benforce(d|ment)?\\b',\n            r'\\bcomply\\b',\n            r'\\\
        baudit(ed|ing)?\\b'\n        ]\n        for pattern in quantitative_patterns:\n\
        \            if re.search(pattern, clause, flags=re.I):\n                return\
        \ True\n        # If no quantitative pattern but a concrete action, we'll\
        \ call it measurable for now\n        if is_explicit(clause):\n          \
        \  return True\n        return False\n\n    def is_single(clause: str) ->\
        \ bool:\n        # Attempt to detect if clause contains more than one requirement\
        \ (compound)\n        # Look for 'and', 'or', multiple imperative sentences,\
        \ or comma-joined enumerations\n        if re.search(r'\\b(and|or)\\b', clause):\n\
        \            return False\n        if re.search(r',\\s*(and|or)?\\s*\\w+\\\
        b', clause):\n            return False\n        return True\n\n    # Step\
        \ 1: Split and clean constraints\n    raw_constraints = split_constraints(constraints)\n\
        \n    formatted_constraints = []\n    failed_constraints = []\n    for c in\
        \ raw_constraints:\n        orig = c\n        normed = normalize_constraint(c)\n\
        \        valid = True\n        # Step 2: Validate\n        if not is_explicit(normed):\n\
        \            valid = False\n        if not is_measurable(normed):\n      \
        \      valid = False\n        if not is_single(normed):\n            valid\
        \ = False\n        if valid:\n            formatted_constraints.append(normed)\n\
        \        else:\n            # For this interface, just skip; if logging, would\
        \ log/fail here\n            # Optionally: could append failed constraints\
        \ to a file or log\n            pass\n    # Step 3: Return the validated,\
        \ formatted list\n    return formatted_constraints\n"
      name: format_and_validate_constraints
      nodes_depended_on: []
      nodes_dependent_on: []
      output_structure:
      - description: Output of type List[str]
        key: output
        type: List[str]
      - description: Input parameter of type str
        key: constraints
        type: str
      prd:
        bullets:
        - complexity: MEDIUM
          impact: Enhances machine-readability and human reviewability, which is critical
            for defining success metrics and compliance traceability.
          method: Implement string normalization, grammatical correction (using NLP
            tools if needed), and templating logic for common constraint patterns.
          reason: Consistent formatting ensures that downstream processes can systematically
            interpret and apply constraints, reducing the risk of ambiguity or misinterpretation.
          text: Develop a standard formatting scheme for constraints, such as normalizing
            phrasing, removing redundancy, correcting grammar, and converting to a
            consistent structure (e.g., imperative statements).
        - complexity: MEDIUM
          impact: Improves the quality and applicability of input for subsequent workflow
            steps and reduces rework or decision delays.
          method: Apply rule-based validation checks for explicitness and measurability;
            optionally leverage NLP classification models for ambiguity detection.
          reason: Ensuring clarity and singularity in each constraint makes metric
            definition possible and avoids confusion in workflow implementation.
          text: Validate each constraint to confirm it is explicit, actionable, and
            measurable, flagging or excluding vague or compound constraints.
        - complexity: LOW
          impact: Prevents dropping important information and supports auditability
            of the constraint extraction workflow.
          method: Maintain two lists (pass/fail) and output or log issues for curation;
            use structured logging or return a tuple with status metadata.
          reason: Preserving failed/flagged constraints allows for iterative improvement
            and senior manager follow-up without data loss.
          text: Return the validated, formatted list while logging or handling any
            constraints that fail formatting or validation checks for human review.
      prompt: Given a list of explicit constraints in raw string format, output a
        standardized and validated list of constraints, ensuring each item is unambiguous,
        concise, and formatted for direct use in success metric definition or requirements
        traceability.
      shims: []
    - agent: false
      code_node_type: virtual-pure
      description: This node extracts a list of discussion points requiring senior
        manager input or clarification from the input text using specified trigger
        phrases.
      implementation: "def extract_manager_discussion_points(text: str, triggers:\
        \ str) -> List[str]:\n    \"\"\"\n    This node extracts a list of discussion\
        \ points requiring senior manager input or clarification from the input text\
        \ using specified trigger phrases.\n\n    Args:\n        text: Input parameter\
        \ of type str\ntriggers: Input parameter of type str\n\n    Returns:\n   \
        \     List[str]: Output of type List[str]\n    \"\"\"\n    import re\n   \
        \ \n    # --- Step 1: Parse triggers ---\n    # Accept triggers as comma,\
        \ semicolon, or newline separated list\n    trigger_list = [t.strip().lower()\
        \ for t in re.split(r'[;,\\n]', triggers) if t.strip()]\n    # Remove duplicates,\
        \ short triggers and empty entries\n    trigger_list = sorted(set(t for t\
        \ in trigger_list if len(t) > 1), key=len, reverse=True)\n    if not trigger_list:\n\
        \        return []\n\n    # --- Step 2: Split text into sentences/clauses\
        \ ---\n    # Simple sentence/segment splitter: split at period, exclamation,\
        \ question, or semicolon (keeps some clauses, too)\n    raw_segments = re.split(r'(?<=[.!?;])\\\
        s+|\\n', text)\n    # Further split on commas if necessary (to ensure shorter\
        \ actionable points)\n    segments = []\n    for seg in raw_segments:\n  \
        \      if len(seg.split()) > 20:  # Arbitrary cutoff for long sentence, may\
        \ be a paragraph\n            segments.extend([s.strip() for s in seg.split(',')\
        \ if s.strip()])\n        else:\n            stripped = seg.strip()\n    \
        \        if stripped:\n                segments.append(stripped)\n\n    #\
        \ --- Step 3: Find discussion points (segments containing any trigger phrase)\
        \ ---\n    discussion_points = []\n    for seg in segments:\n        lowered\
        \ = seg.lower()\n        if any(trigger in lowered for trigger in trigger_list):\n\
        \            discussion_points.append(seg)\n\n    if not discussion_points:\n\
        \        return []\n\n    # --- Step 4: Clean and normalize discussion points\
        \ ---\n    cleaned_points = []\n    seen = set()\n    for point in discussion_points:\n\
        \        # Trim whitespace\n        p = point.strip()\n        # Remove leading/trailing\
        \ punctuation\n        p = re.sub(r'^[\\s\\-\u2013\u2014\\*]+', '', p)\n \
        \       p = re.sub(r'[\\s.!?;,:-]+$', '', p)\n        # Remove internal excessive\
        \ whitespace\n        p = re.sub(r'\\s+', ' ', p)\n        # Remove duplicate\
        \ points (case-insensitive)\n        p_key = p.lower()\n        if p_key not\
        \ in seen and p:\n            cleaned_points.append(p)\n            seen.add(p_key)\n\
        \n    # --- Optionally, enforce each is a requirement/question (could further\
        \ split on \", and \"/or \". Also, ...\")\n    # For simplicity, split on\
        \ \" and \" to separate conjoined requirements\n    final_points = []\n  \
        \  for p in cleaned_points:\n        subpoints = [sp.strip() for sp in re.split(r'\\\
        band\\b', p, flags=re.IGNORECASE)]\n        for sp in subpoints:\n       \
        \     ssp = sp.strip()\n            if ssp and ssp.lower() not in seen:\n\
        \                final_points.append(ssp)\n                seen.add(ssp.lower())\n\
        \n    return final_points\n"
      name: extract_manager_discussion_points
      nodes_depended_on: []
      nodes_dependent_on: []
      output_structure:
      - description: Output of type List[str]
        key: output
        type: List[str]
      - description: Input parameter of type str
        key: text
        type: str
      - description: Input parameter of type str
        key: triggers
        type: str
      prd:
        bullets:
        - complexity: MEDIUM
          impact: Reduces risk of overlooked discussion items, aligns team with management,
            and improves project communication by capturing dependencies and open
            questions.
          method: Use regular expressions or phrase-based string search (with configurable
            triggers) to parse the input text and extract sentences or segments containing
            those triggers.
          reason: Ensures that all items needing further clarification or managerial
            decision-making are proactively identified and gathered for meeting agendas.
          text: Implement a text scanning and pattern matching function that identifies
            sentences or clauses containing specified trigger phrases indicating the
            need for managerial discussion or approval.
        - complexity: LOW
          impact: Improves quality and usability of outputs, reducing time spent reformatting
            or clarifying discussion points for meetings.
          method: Apply basic NLP text cleaning (e.g., trimming, deduplication, removing
            trailing punctuation and excessive whitespace) and limit each output to
            a single requirement or question.
          reason: Provides clear, unambiguous agenda items to facilitate efficient
            executive review and decision-making.
          text: Normalize and clean extracted discussion points to ensure each is
            concise, actionable, and free from irrelevant context.
      prompt: Given the provided requirements text and a list of discussion point
        trigger phrases, extract and return a list of discussion points or constraints
        that require clarification, approval, or executive input, with each item represented
        as a concise string suitable for inclusion in a meeting agenda.
      shims: []
    - agent: false
      code_node_type: virtual-pure
      description: This node securely logs the extracted constraints, identified discussion
        points, and original input to provide compliance auditability and traceability
        for future reference.
      implementation: "def log_extraction_audit(extracted_constraints: str, discussion_points:\
        \ str, original_input: str) -> str:\n    \"\"\"\n    This node securely logs\
        \ the extracted constraints, identified discussion points, and original input\
        \ to provide compliance auditability and traceability for future reference.\n\
        \n    Args:\n        extracted_constraints: Input parameter of type str\n\
        discussion_points: Input parameter of type str\noriginal_input: Input parameter\
        \ of type str\n\n    Returns:\n        str: Output of type Any\n    \"\"\"\
        \n    import json\n    import datetime\n    import threading\n    import os\n\
        \n    # --- PURE IMPLEMENTATION ---\n    # Define the path of the audit log\
        \ (in production, this would be a secure, append-only database)\n    AUDIT_LOG_PATH\
        \ = os.environ.get('EXTRACTION_AUDIT_LOG_PATH', 'extraction_audit.log')\n\n\
        \    # Prepare the audit record\n    audit_record = {\n        'timestamp':\
        \ datetime.datetime.utcnow().isoformat() + 'Z',\n        'extracted_constraints':\
        \ extracted_constraints,\n        'discussion_points': discussion_points,\n\
        \        'original_input': original_input,\n        # Placeholder for additional\
        \ metadata (user, context, etc.)\n    }\n    serialized_record = json.dumps(audit_record,\
        \ separators=(',', ':'))\n\n    # Ensure batched/asynchronous logging does\
        \ not block main process\n    def _write_audit_record(audit_entry: str):\n\
        \        try:\n            # Append log entry in append-only manner\n    \
        \        # Open in append & text mode. In production use OS-level permissions\
        \ & file locking for safety.\n            with open(AUDIT_LOG_PATH, 'a', encoding='utf-8')\
        \ as f:\n                f.write(audit_entry + '\\n')\n        except Exception\
        \ as e:\n            # On failure, attempt to log to a fallback file\n   \
        \         fallback_path = AUDIT_LOG_PATH + '.fallback'\n            try:\n\
        \                with open(fallback_path, 'a', encoding='utf-8') as f:\n \
        \                   f.write(audit_entry + '\\n')\n            except Exception\
        \ as fallback_e:\n                # If all logging fails, print error (could\
        \ be replaced with alerting system)\n                print(f\"Audit logging\
        \ failed: {e}; Fallback also failed: {fallback_e}\")\n\n    # Launch the log\
        \ write as a daemon thread to avoid blocking main processing\n    threading.Thread(target=_write_audit_record,\
        \ args=(serialized_record,), daemon=True).start()\n\n    # Optionally, could\
        \ return an identifier, timestamp, or status\n    return audit_record['timestamp']\n"
      name: log_extraction_audit
      nodes_depended_on: []
      nodes_dependent_on: []
      output_structure:
      - description: Output of type Any
        key: output
        type: str
      - description: Input parameter of type str
        key: extracted_constraints
        type: str
      - description: Input parameter of type str
        key: discussion_points
        type: str
      - description: Input parameter of type str
        key: original_input
        type: str
      prd:
        bullets:
        - complexity: MEDIUM
          impact: Enables thorough governance, compliance, and review of the extraction
            pipeline process.
          method: Implement using append-only logs in a database (e.g., PostgreSQL
            with audit tables) or cloud-based logging system, ensuring serialization
            of lists (e.g., as JSON strings), and storing timestamps and user/context
            metadata where appropriate.
          reason: Audit compliance and traceability require that all extraction actions
            be defensibly recorded for potential future review by auditors or stakeholders.
          text: Persistently log all critical outputs (constraints, discussion points,
            and raw input) to a secure and queryable audit storage backend.
        - complexity: LOW
          impact: Maintains reliability of the main extraction process while guaranteeing
            audit attempts are made consistently.
          method: Use asynchronous, batched, or background logging mechanisms, implement
            transactional writes, and fallback/error logging strategies for critical
            failures.
          reason: Extraction pipelines must be fault-tolerant, and audit failures
            should not prevent processing or compromise data integrity.
          text: Ensure write operations to the audit log are robust (transactional)
            and do not hinder main processing in case of failure.
      prompt: Log the extracted constraints and manager discussion points, along with
        the original input, into an audit system or persistent store in a manner that
        supports compliance traceability, retrieval for future audits, and review
        by authorized stakeholders.
      shims: []
    - agent: false
      code_node_type: virtual-pure
      description: Determines if the extraction configuration needs to be updated
        based on input parameters and modifies rules or patterns for future constraint
        extraction if required.
      implementation: "def update_extraction_config_if_needed(kwargs: str) -> str:\n\
        \    \"\"\"\n    Determines if the extraction configuration needs to be updated\
        \ based on input parameters and modifies rules or patterns for future constraint\
        \ extraction if required.\n\n    Args:\n        kwargs: Input parameter of\
        \ type str\n\n    Returns:\n        str: Output of type Any\n    \"\"\"\n\
        \    import json\n    import os\n    from copy import deepcopy\n\n    # Parse\
        \ input kwargs (expecting a JSON string)\n    try:\n        input_kwargs =\
        \ json.loads(kwargs)\n    except Exception as e:\n        return json.dumps({\n\
        \            'status': 'error',\n            'message': f'Failed to parse\
        \ input kwargs: {e}'\n        })\n\n    # --- CONFIGURATION STORE ---\n  \
        \  CONFIG_FILE = 'extraction_config.json'\n\n    # Helper: Load existing config\
        \ (if file exists), else start with default\n    def load_config():\n    \
        \    if os.path.exists(CONFIG_FILE):\n            try:\n                with\
        \ open(CONFIG_FILE, 'r') as f:\n                    return json.load(f)\n\
        \            except Exception:\n                # Corrupt file or read error;\
        \ fallback to default empty\n                return {'categories': [], 'patterns':\
        \ [], 'regulatory_constraints': []}\n        else:\n            return {'categories':\
        \ [], 'patterns': [], 'regulatory_constraints': []}\n\n    # Helper: Save\
        \ config to file\n    def save_config(config):\n        with open(CONFIG_FILE,\
        \ 'w') as f:\n            json.dump(config, f, indent=2)\n\n    # --- Recognized\
        \ triggers ---\n    # Accept new categories, patterns, or regulatory_constraints.\n\
        \    TRIGGERS = ['categories', 'patterns', 'regulatory_constraints']\n\n \
        \   config = load_config()\n    config_before = deepcopy(config)\n    updates_made\
        \ = False\n    updates = {}\n\n    # Check for recognized triggers and update\
        \ if new info is present\n    for trigger in TRIGGERS:\n        new_values\
        \ = input_kwargs.get(trigger)\n        if new_values:\n            if not\
        \ isinstance(new_values, list):\n                # Convert single string or\
        \ value to list\n                new_values = [new_values]\n            existing_values\
        \ = set(config.get(trigger, []))\n            additional = [v for v in new_values\
        \ if v not in existing_values]\n            if additional:\n             \
        \   config.setdefault(trigger, [])\n                config[trigger].extend(additional)\n\
        \                updates[trigger] = additional\n                updates_made\
        \ = True\n\n    # Apply changes and persist if needed\n    if updates_made:\n\
        \        save_config(config)\n        status_msg = {\n            'status':\
        \ 'updated',\n            'details': {\n                'added': updates,\n\
        \                'config_before': config_before,\n                'config_after':\
        \ config\n            }\n        }\n    else:\n        status_msg = {\n  \
        \          'status': 'no_update',\n            'message': 'No new triggers\
        \ or values detected that require changes.',\n            'config': config\n\
        \        }\n    return json.dumps(status_msg, indent=2)\n"
      name: update_extraction_config_if_needed
      nodes_depended_on: []
      nodes_dependent_on: []
      output_structure:
      - description: Output of type Any
        key: output
        type: str
      - description: Input parameter of type str
        key: kwargs
        type: str
      prd:
        bullets:
        - complexity: MEDIUM
          impact: Ensures continual relevance and accuracy of constraint extraction,
            minimizing manual updates and maintaining alignment with organizational
            or external changes.
          method: Implement logic to scan kwargs for recognized triggers, compare
            against existing config, and flag discrepancies for update.
          reason: It's necessary to enable dynamic extension and maintenance of rule-based
            extraction logic to stay compliant and effective as requirements evolve.
          text: Detect if incoming kwargs reflect new categories, patterns, or regulatory
            constraints that require the extraction configuration to be updated.
        - complexity: MEDIUM
          impact: Automates maintenance of configuration, reducing errors and ensuring
            the system uses the most appropriate rules.
          method: Modify configuration store (e.g., in-memory dict or persisted config
            file) by merging new patterns or settings, and log the changes for auditability.
          reason: Acting on detected deficiencies or new needs allows the node to
            autonomously adapt its extraction capabilities without external intervention.
          text: Apply updates to the extraction configuration or rule set if a gap
            or new requirement is found.
        - complexity: LOW
          impact: Improves operational observability, allowing downstream processes
            or users to react appropriately.
          method: Aggregate the decision outcome into a well-formatted string or JSON
            message for downstream consumption.
          reason: Providing explicit feedback on configuration status increases system
            transparency and supports debugging or audit trails.
          text: Return a confirmation or status message detailing whether an update
            was performed or not.
      prompt: Check current extraction configuration and, if necessary based on incoming
        kwargs or detected relevant changes (such as new regulatory rules or extraction
        patterns), update or extend the system's rule sets and extraction logic for
        future constraint identification tasks.
      shims: []
  extract_key_features:
    agent: false
    code_node_type: pure
    description: Generate a list of key functional features required by the workflow,
      explicitly adding a workflow node for scheduling and conducting a meeting with
      senior managers prior to determining user roles, as per updated requirements.
      Each feature must be a unique functional capability required by the PRD, sequenced
      to reflect this critical ordering.
    implementation: "from pydantic import BaseModel, Field\nfrom typing import List\n\
      \n\nclass IdentifyCoreObjectiveOutput(BaseModel):\n    \"\"\"Pydantic model\
      \ for identify_core_objective node outputs.\"\"\"\n    core_objective: str =\
      \ Field(..., description=\"The primary objective statement for the workflow\
      \ as described in the requirements, reflecting the need to include a meeting\
      \ with senior managers prior to determining user roles, and noting that constraints\
      \ should be available for use by dependent nodes, such as those identifying\
      \ success metrics.\")\n\n\nclass ExtractKeyFeaturesOutput(BaseModel):\n    \"\
      \"\"Pydantic model for extract_key_features node outputs.\"\"\"\n    feature_names:\
      \ List[str] = Field(..., description=\"Names of fundamental workflow features\
      \ required in the PRD, in sequence starting with a node to schedule and conduct\
      \ a meeting with senior managers prior to any role assignment, followed by other\
      \ required workflow features.\")\n\n\ndef extract_key_features_fx(identify_core_objective_input:\
      \ IdentifyCoreObjectiveOutput, **kwargs) -> ExtractKeyFeaturesOutput:\n    \"\
      \"\"Generate a list of key functional features required by the workflow, explicitly\
      \ adding a workflow node for scheduling and conducting a meeting with senior\
      \ managers prior to determining user roles, as per updated requirements. Each\
      \ feature must be a unique functional capability required by the PRD, sequenced\
      \ to reflect this critical ordering.\n\n    Args:\n        identify_core_objective_input:\
      \ Input from the 'identify_core_objective' node.\n        **kwargs: Additional\
      \ keyword arguments.\n\n    Returns:\n        ExtractKeyFeaturesOutput: Object\
      \ containing outputs for this node.\n    \"\"\"\n    # --- SHIM IMPLEMENTATION\
      \ ---\n    # 1. Parse requirements and core objective for candidate features\n\
      \    candidate_features: List[str] = extract_feature_candidates_from_objective(text=identify_core_objective_input.core_objective)\n\
      \    \n    # 2. Prepend 'Schedule and conduct a meeting with senior managers'\
      \ as the first feature\n    feature_list_with_meeting: List[str] = ensure_meeting_node_first(features=candidate_features)\n\
      \    \n    # 3. Enforce that no user role/determination related features appear\
      \ before the meeting node\n    ordered_features: List[str] = block_or_reorder_role_features(features=feature_list_with_meeting)\n\
      \n    # 4. Deduplicate and split compound features into atomic, unique units\n\
      \    atomic_features: List[str] = normalize_and_deduplicate_features(features=ordered_features)\n\
      \n    # 5. Sequence list according to all workflow constraints (meeting node\
      \ first, roles after, etc.)\n    sequenced_features: List[str] = sequence_features_per_workflow_order(features=atomic_features)\n\
      \n    # 6. Validate and serialize output list\n    validated_features: List[str]\
      \ = validate_and_serialize_feature_list(features=sequenced_features)\n\n   \
      \ return ExtractKeyFeaturesOutput(feature_names=validated_features)\n"
    name: extract_key_features
    nodes_depended_on:
    - identify_core_objective
    nodes_dependent_on: []
    output_structure:
    - description: Names of fundamental workflow features required in the PRD, in
        sequence starting with a node to schedule and conduct a meeting with senior
        managers prior to any role assignment, followed by other required workflow
        features.
      key: feature_names
      type: List[str]
    prd:
      bullets:
      - complexity: "LOW \u2013 Standard extraction and parsing pattern."
        impact: "HIGH \u2013 Sets the foundation for all subsequent functional decomposition,\
          \ affects accuracy of downstream nodes."
        method: Apply deterministic parsing rules to requirement and objective text;
          extract sentences or clauses representing functional needs.
        reason: Ensures that the extraction of features is comprehensive and directly
          aligned with the documented purpose and boundaries of the workflow.
        text: Parse input requirements and core objective to identify all candidate
          workflow features.
      - complexity: "LOW \u2013 Insertion of static feature node at a fixed sequence\
          \ location."
        impact: "HIGH \u2013 Explicitly satisfies a gating business process and makes\
          \ workflow constraints executable."
        method: Programmatically prepend the meeting node to the feature list, using
          string matching or ID tagging to guarantee correct placement.
        reason: Critical to compliance with updated requirements and proper sequencing
          for stakeholder alignment before defining user roles or other features.
        text: Enforce inclusion of a 'Schedule and conduct a meeting with senior managers'
          node as the first actionable feature in the sequence.
      - complexity: "MEDIUM \u2013 Requires careful ordering and validation of all\
          \ identified features."
        impact: "HIGH \u2013 Integral to contractual and process compliance; prevents\
          \ downstream logic errors."
        method: Scan feature candidates for role-determination-related keywords; block
          or reorder them after confirming the meeting node is in place.
        reason: Guarantees that workflow sequencing matches explicit business requirements
          and prevents accidental misordering.
        text: Apply logic to ensure no features related to user roles or their determination
          appear before the senior manager meeting node.
      - complexity: "MEDIUM \u2013 Requires normalization and deduplication across\
          \ extracted features."
        impact: "MEDIUM \u2013 Reduces ambiguity and rework for design, implementation,\
          \ and QA teams."
        method: Apply set-based or hash-based filtering to remove near-duplicate features
          and split compound features into atomic units.
        reason: Ensures clarity and atomicity, preventing confusion or functional
          redundancy in the PRD and its implementation.
        text: Verify that each feature represents a unique, orthogonal functional
          capability with no overlaps.
      - complexity: "LOW \u2013 Ordering is deterministic after constraints are applied."
        impact: "HIGH \u2013 Directly affects workflow orchestration and all nodes\
          \ that operate sequentially."
        method: Sort features by prescribed business rules; implement checks to enforce
          meeting-with-senior-managers node as first, role-determination node(s) next,
          followed by other features.
        reason: Reflects business logic, ensures downstream steps (like descriptions,
          prioritization) work on correctly ordered inputs.
        text: Sequence the complete list of feature names in strict workflow order
          per the requirements and core objective.
      - complexity: "LOW \u2013 Standard formatting and validation."
        impact: "MEDIUM \u2013 Prevents errors and mismatches in dependent node execution."
        method: Serialize the feature list as a type-safe List[str], confirming compliance
          with output schema before release.
        reason: Ensures type safety, data interoperability, and system stability throughout
          the serial workflow.
        text: Output the final, validated feature list in the required structure for
          downstream nodes.
    prompt: Examining the requirements and core objective, extract the main features
      that the PRD should address. Each feature should be a standalone, functional
      capability with no overlap. Explicitly include 'Schedule and conduct a meeting
      with senior managers as a separate workflow node that must occur prior to determining
      user roles.' Ensure this feature is listed before any features related to role
      determination. List the features in a strictly sequential order that reflects
      this workflow constraint.
    shims:
    - agent: false
      code_node_type: virtual-pure
      description: This node analyzes the provided core objective text to extract
        a list of candidate workflow features described or implied within the objective
        statement.
      implementation: "def extract_feature_candidates_from_objective(text: str) ->\
        \ List[str]:\n    \"\"\"\n    This node analyzes the provided core objective\
        \ text to extract a list of candidate workflow features described or implied\
        \ within the objective statement.\n\n    Args:\n        text: Input parameter\
        \ of type str\n\n    Returns:\n        List[str]: Output of type List[str]\n\
        \    \"\"\"\n    import re\n\n    # 1. Basic sentence segmentation to attempt\
        \ splitting on ; . and newlines, but also consider 'and', 'or', etc for compound\
        \ statements\n    # 2. Named entity-like extraction: parse for verbs and likely\
        \ actions, possibly via regex for verbs/verb phrases followed by direct objects.\n\
        \    # 3. Deduplicate and clean up phrases: strip whitespace, remove trailing\
        \ punctuation, deduplicate identical or near-identical statements.\n\n   \
        \ # Step 1: Preprocessing - normalize and split text into candidate sentences/fragments\n\
        \    text_clean = re.sub(r'\\s+', ' ', text.strip())  # Normalize whitespace\n\
        \    candidate_fragments = re.split(r'[.;\\n]+', text_clean)\n\n    features\
        \ = []\n    for fragment in candidate_fragments:\n        fragment = fragment.strip()\n\
        \        if not fragment:\n            continue\n\n        # Step 2: Split\
        \ on common conjunctions when appropriate for compound sentences\n       \
        \ # (e.g., \"generate a summary and send it ...\" -> [\"generate a summary\"\
        , \"send it ...\"])\n        subfrags = re.split(r'\\band\\b|\\bor\\b|\\bthen\\\
        b|,', fragment, flags=re.IGNORECASE)\n        for subf in subfrags:\n    \
        \        phrase = subf.strip()\n            if not phrase:\n             \
        \   continue\n\n            # Step 3: Try extracting verb-object patterns\
        \ (quick NLP-inspired heuristic)\n            # E.g., 'Detect user intent',\
        \ 'Extract phone number', etc.\n            # Simplistic regex: Match a verb\
        \ (likely starting word), rest of the phrase is the feature\n            verb_phrase_match\
        \ = re.match(r'^(to |should )?(\\w+)( .+)', phrase, flags=re.IGNORECASE)\n\
        \            if verb_phrase_match:\n                # Use the full phrase\
        \ for maximum descriptiveness as atomized capability\n                core_feature\
        \ = phrase\n            else:\n                # If not matching pattern,\
        \ but phrase is not empty, take it as is\n                core_feature = phrase\n\
        \            features.append(core_feature)\n\n    # Step 4: Post-process -\
        \ remove duplicates and ensure atomicity\n    # Remove duplicates (case-insensitive)\n\
        \    deduped = []\n    seen = set()\n    for feat in features:\n        norm\
        \ = feat.lower().strip()\n        # Remove trailing punctuation for clean\
        \ features\n        norm = re.sub(r'[^\\w\\s]$', '', norm)\n        # Minimal\
        \ filtering for atomic/unique features\n        if norm and norm not in seen:\n\
        \            deduped.append(feat.strip().rstrip('.').rstrip(';'))\n      \
        \      seen.add(norm)\n\n    return deduped\n"
      name: extract_feature_candidates_from_objective
      nodes_depended_on: []
      nodes_dependent_on: []
      output_structure:
      - description: Output of type List[str]
        key: output
        type: List[str]
      - description: Input parameter of type str
        key: text
        type: str
      prd:
        bullets:
        - complexity: MEDIUM
          impact: Provides a foundational set of features for further workflow analysis,
            enabling downstream nodes to build structured PRD elements.
          method: Use natural language processing techniques such as named entity
            recognition and pattern-based parsing to identify and segment individual
            features.
          reason: Accurate feature extraction ensures that all workflow steps required
            or suggested in the objective are considered for downstream requirements
            definition.
          text: Parse the core objective text and identify all discrete workflow features
            described or implied.
        - complexity: LOW
          impact: Ensures no compound or duplicated features proceed, supporting high
            accuracy in workflow mapping.
          method: Post-process initial feature candidates by applying string deduplication
            and splitting/joining logic to isolate and clarify individual items.
          reason: Features must be presented as atomic capabilities to allow for correct
            workflow structuring and deduplication in subsequent nodes.
          text: Return each feature as a single, unique, and descriptive string in
            a flat list.
      prompt: Extract from the supplied core objective statement all explicit or implicit
        workflow features required for the product requirements document, listing
        each unique feature as a separate item.
      shims: []
    - agent: false
      code_node_type: virtual-pure
      description: This node ensures that the feature list always has 'Schedule and
        conduct a meeting with senior managers' as the first entry, placing all other
        features after it.
      implementation: "def ensure_meeting_node_first(features: str) -> List[str]:\n\
        \    \"\"\"\n    This node ensures that the feature list always has 'Schedule\
        \ and conduct a meeting with senior managers' as the first entry, placing\
        \ all other features after it.\n\n    Args:\n        features: Input parameter\
        \ of type str\n\n    Returns:\n        List[str]: Output of type List[str]\n\
        \    \"\"\"\n    # PURE IMPLEMENTATION\n    # 1. Split the incoming feature\
        \ list string into features (assuming one per line or comma-separated)\n \
        \   # 2. Normalize features to strip whitespace\n    # 3. Ensure the required\
        \ meeting feature is present and first, maintaining order of other features\n\
        \    meeting_feature = 'Schedule and conduct a meeting with senior managers'\n\
        \    \n    # Try handling comma or newline separated\n    import re\n    raw_features\
        \ = [f for f in re.split(r'\\r?\\n|,', features) if f.strip()]\n    # Normalize\
        \ features (strip whitespace from each)\n    stripped_features = [f.strip()\
        \ for f in raw_features]\n    \n    # Remove ALL occurrences of the meeting\
        \ feature (normalize for safety)\n    normalized_meeting = meeting_feature.strip().lower()\n\
        \    other_features = [f for f in stripped_features if f.strip().lower() !=\
        \ normalized_meeting]\n    \n    # Insert the meeting feature at the front\n\
        \    result = [meeting_feature] + other_features\n    return result\n"
      name: ensure_meeting_node_first
      nodes_depended_on: []
      nodes_dependent_on: []
      output_structure:
      - description: Output of type List[str]
        key: output
        type: List[str]
      - description: Input parameter of type str
        key: features
        type: str
      prd:
        bullets:
        - complexity: LOW
          impact: Guarantees compliance with business requirements and proper workflow
            sequencing for downstream logic.
          method: Implement string matching or normalized comparison to check for
            the specific meeting feature and insert or move it to the first position;
            add it if absent.
          reason: The workflow requirements mandate a meeting with senior management
            precedes any other feature or role-determination steps.
          text: Detect and ensure the presence of the 'Schedule and conduct a meeting
            with senior managers' feature at the beginning of the input feature list.
        - complexity: LOW
          impact: Minimizes side effects and preserves requirements traceability in
            the workflow.
          method: Rebuild the list by removing the meeting feature if present, then
            prepending it, followed by all other features in their original order.
          reason: "Preserving the original intention and sequence of the workflow\
            \ features\u2014except for the mandatory leading meeting\u2014ensures\
            \ correct functional decomposition."
          text: Maintain the relative order of all other features after placing the
            meeting feature first.
      prompt: Given a list of feature strings, move or insert the 'Schedule and conduct
        a meeting with senior managers' feature as the first element in the list,
        ensuring it precedes any role-determination or other workflow features, and
        output the resulting ordered list.
      shims: []
    - agent: false
      code_node_type: virtual-pure
      description: Ensures that any user role determination or related features do
        not appear before the scheduled senior manager meeting in the workflow feature
        list, blocking or reordering such features as necessary to respect workflow
        constraints.
      implementation: "def block_or_reorder_role_features(features: str) -> List[str]:\n\
        \    \"\"\"\n    Ensures that any user role determination or related features\
        \ do not appear before the scheduled senior manager meeting in the workflow\
        \ feature list, blocking or reordering such features as necessary to respect\
        \ workflow constraints.\n\n    Args:\n        features: Input parameter of\
        \ type str\n\n    Returns:\n        List[str]: Output of type List[str]\n\
        \    \"\"\"\n    import re\n    # Split the input features string into a list\
        \ of feature entries (by line, semi-colon, or numbered list)\n    # We'll\
        \ assume the list is either line separated or split by known delimiters.\n\
        \    # Try to split by \\n first, and remove empty entries\n    feature_lines\
        \ = [line.strip() for line in features.split('\\n') if line.strip()]\n   \
        \ if len(feature_lines) == 1 and (';' in feature_lines[0]):\n        # Maybe\
        \ the features are delimited by semicolons\n        feature_lines = [seg.strip()\
        \ for seg in feature_lines[0].split(';') if seg.strip()]\n    \n    # Remove\
        \ any numbering like '1. ', '2) ', etc.\n    def strip_numbering(entry: str)\
        \ -> str:\n        return re.sub(r'^\\s*(\\d+\\.\\s*|\\d+\\)\\s*)', '', entry)\n\
        \    feature_lines = [strip_numbering(line) for line in feature_lines]\n\n\
        \    # Keywords to detect role assignment/role determination related features\n\
        \    role_keywords = [\n        r\"role determination\",\n        r\"assign\
        \ user\", r\"assign users\", r\"user assignment\",\n        r\"assign to user\"\
        , r\"user roles\", r\"assign roles\",\n        r\"define role\", r\"determine\
        \ role\", r\"set role\", r\"assigning\", r\"designate user\", r\"user permissions\"\
        ,\n        r\"role-based\", r\"allocate user\", r\"select user\", r\"participant\
        \ assignment\", r\"identify user\", r\"person responsible\"\n    ]\n    role_regex\
        \ = re.compile(r\"|\".join([f\"({k})\" for k in role_keywords]), flags=re.IGNORECASE)\n\
        \n    # Find the index of the schedule/conduct meeting with senior managers\
        \ feature\n    # Try to match loosely so variant phrasing is included\n  \
        \  meeting_regex = re.compile(r\"(schedule|conduct|hold|organize|arrange).{0,40}(senior\
        \ manager|manager|executive|leadership|directors)\", re.IGNORECASE)\n    meeting_idx\
        \ = None\n    for i, feat in enumerate(feature_lines):\n        if meeting_regex.search(feat):\n\
        \            meeting_idx = i\n            break\n    # If meeting line is\
        \ not found, default to 0 (meaning put role features after first feature)\n\
        \    if meeting_idx is None:\n        meeting_idx = 0\n\n    # Split features\
        \ into three groups:\n    #   - features before (and including) the meeting\
        \ (pre_meeting)\n    #   - role-related features that occur before the meeting\
        \ (role_before)\n    #   - all others\n    pre_meeting = feature_lines[:meeting_idx+1]\n\
        \    post_meeting = feature_lines[meeting_idx+1:]\n\n    role_features = []\n\
        \    non_role_features = []\n\n    # Find and separate role-related features\
        \ from post-meeting features\n    for feat in post_meeting:\n        if role_regex.search(feat):\n\
        \            role_features.append(feat)\n        else:\n            non_role_features.append(feat)\n\
        \n    # Now check if there are any role features mis-placed before the meeting\
        \ (should be moved after meeting)\n    i = 0\n    while i < len(pre_meeting):\n\
        \        if i == meeting_idx:\n            # Don't move meeting itself\n \
        \           i += 1\n            continue\n        if role_regex.search(pre_meeting[i]):\n\
        \            role_features.append(pre_meeting.pop(i))\n            # Adjust\
        \ meeting_idx since the list shrank\n            meeting_idx -= 1\n      \
        \      continue\n        i += 1\n\n    # Final ordering is: pre_meeting, then\
        \ role_features, then remaining post-meeting features\n    reordered_features\
        \ = pre_meeting + role_features + non_role_features\n    return reordered_features\n"
      name: block_or_reorder_role_features
      nodes_depended_on: []
      nodes_dependent_on: []
      output_structure:
      - description: Output of type List[str]
        key: output
        type: List[str]
      - description: Input parameter of type str
        key: features
        type: str
      prd:
        bullets:
        - complexity: LOW
          impact: Prevents incorrect sequencing that could result in workflow tasks
            occurring before essential approvals or meetings.
          method: Use regular expressions or keyword matching to flag features that
            involve roles or user assignments.
          reason: It is necessary to detect features that pertain to user roles in
            order to control their placement within the workflow.
          text: Scan the provided feature list and identify all entries mentioning
            role determination, user assignment, or related keywords.
        - complexity: MEDIUM
          impact: Maintains integrity of business process requirements and ensures
            downstream nodes receive features in the mandated order.
          method: Restructure the feature list by splitting it into pre-meeting and
            post-meeting segments, moving flagged features after the meeting entry.
          reason: Enforces compliance with specified workflow order where managerial
            meetings precede role-based actions.
          text: Ensure that any identified role-related features are moved to follow
            the 'schedule and conduct a meeting with senior managers' feature, which
            must remain first.
      prompt: 'Description: Typed node for shim block_or_reorder_role_features'
      shims: []
    - agent: false
      code_node_type: virtual-pure
      description: This node processes an input list of workflow features to split
        compound statements into atomic features, remove duplicates, and standardize
        the phrasing for consistency.
      implementation: "def normalize_and_deduplicate_features(features: str) -> List[str]:\n\
        \    \"\"\"\n    This node processes an input list of workflow features to\
        \ split compound statements into atomic features, remove duplicates, and standardize\
        \ the phrasing for consistency.\n\n    Args:\n        features: Input parameter\
        \ of type str\n\n    Returns:\n        List[str]: Output of type List[str]\n\
        \    \"\"\"\n    import re\n    from typing import List\n    \n    # --- 1.\
        \ Preprocess (split by newlines, strip) ---\n    # Assume initial input is\
        \ either a multi-line string or semi-colon/comma separated\n    raw_items\
        \ = []\n    for line in features.splitlines():\n        # Try splitting by\
        \ ';' or ',' in addition to lines\n        line = line.strip()\n        if\
        \ not line:\n            continue\n        # Split by ';' or ',' if present,\
        \ else keep whole line\n        if ';' in line or ',' in line:\n         \
        \   for chunk in re.split(r'[;,]', line):\n                chunk = chunk.strip()\n\
        \                if chunk:\n                    raw_items.append(chunk)\n\
        \        else:\n            raw_items.append(line)\n\n    # Helpers for splitting\
        \ compound statements\n    def split_compound(statement: str) -> List[str]:\n\
        \        # Split by conjunctions: 'and', 'or', 'as well as', 'plus'\n    \
        \    # Only split if it's likely to be truly atomic\n        # Example: \"\
        Allow users to reset passwords and update email\"\n        # Should be split\
        \ into two actions\n        # We'll use a regex to break on \"and\", \"or\"\
        , etc., but not inside quoted text\n        # Also try to handle serial lists:\
        \ \"Export as CSV, PDF, or Excel\"\n        \n        # Normalize separators:\
        \ ensure commas before 'and', 'or', etc. are handled\n        s = statement.strip()\n\
        \        # If none of the splitters, just return as is\n        # We use word\
        \ boundaries to avoid splitting e.g. \"random\"\n        splitters = [r'\\\
        band\\b', r'\\bor\\b', r'\\bas well as\\b', r'\\bplus\\b']\n        splitter_pattern\
        \ = '|'.join(splitters)\n        # Also, sometimes patters are \"foo, bar\
        \ and baz\"\n        # So first split by \",\" and then further by conjunctions\n\
        \        # First split by comma\n        comma_split = [chunk.strip() for\
        \ chunk in re.split(r',', s) if chunk.strip()]\n        result = []\n    \
        \    for segment in comma_split:\n            # Now break on conjunctions\
        \ if present\n            parts = [part.strip() for part in re.split(splitter_pattern,\
        \ segment, flags=re.IGNORECASE) if part.strip()]\n            result.extend(parts)\n\
        \        # To avoid oversplitting e.g. \"Analyze origin and destination addresses\"\
        \n        # If it's <7 words or no verbs appear in both pieces, probably should\
        \ not split\n        if len(result) == 1:\n            return result\n   \
        \     # Only split if all parts look like plausible commands (must have verb)\n\
        \        # Simplified check: begin with verb or modal verb\n        def looks_like_command(x):\n\
        \            # Tokenize first word\n            m = re.match(r'^(to |allow\
        \ user[s]* to )?(\\w+)', x.strip().lower())\n            if m:\n         \
        \       verb_candidate = m.group(2)\n                # Heuristic: basic list\
        \ of common verbs\n                return verb_candidate in [\n          \
        \          'allow','enable','support','add','remove','delete','update','edit','view',\n\
        \                    'create','analyze','send','receive','reset','normalize','export','import',\n\
        \                    'assign','find','choose','split','deduplicate','track','apply','implement',\n\
        \                    'clean','detect', 'order'\n                ]\n      \
        \      return False\n        if all(looks_like_command(r) for r in result):\n\
        \            return result\n        # Else, merged back, don't split\n   \
        \     return [s]\n\n    # --- Step 2: Normalize each statement and split into\
        \ atomic features ---\n    atomics = []\n    for item in raw_items:\n    \
        \    atomic_feats = split_compound(item)\n        atomics.extend(atomic_feats)\n\
        \n    # --- Step 3: Normalize phrasing: clean whitespace, verb-object format,\
        \ capitalization, etc. ---\n    def normalize_text(text: str) -> str:\n  \
        \      # Remove leading/trailing whitespace\n        s = text.strip()\n  \
        \      # Remove trailing punctuation\n        s = re.sub(r'[.?!,;:]+$', '',\
        \ s)\n        # Replace multiple spaces\n        s = re.sub(r'\\s+', ' ',\
        \ s)\n        # Lowercase verb, capitalize object\n        # Try to rephrase\
        \ into Verb-object form\n        # Heuristic: If starts with \"Allow/Enable/Support/Let\
        \ user[s] to\", strip that\n        m = re.match(r'^(allow|enable|support|let)(\
        \ user(s)?(s)?( to)? )?(.*)$', s, re.IGNORECASE)\n        if m:\n        \
        \    rest = m.group(6).strip() if m.group(6) else s\n            # Try to\
        \ identify verb-object pattern\n            # e.g., \"reset passwords\" from\
        \ \"Allow users to reset passwords\"\n            s = rest\n        # Lowercase\
        \ first character, unless acronym\n        s = s[0].upper() + s[1:] if s else\
        \ s\n        # Optionally, could stem/present-tense normalize verbs\n    \
        \    # For now, ensure imperative (no trailing \".\") and consistent spacing\n\
        \        return s\n\n    normalized_atoms = [normalize_text(f) for f in atomics\
        \ if f and f.strip()]\n\n    # --- Step 4: Deduplicate: exact match, then\
        \ basic case-insensitive and fuzzy\n    def unique_by_semantics(strings: List[str])\
        \ -> List[str]:\n        # Case-insensitive exact deduplication\n        seen\
        \ = set()\n        result = []\n        for s in strings:\n            canon\
        \ = s.strip().lower()\n            if canon not in seen:\n               \
        \ seen.add(canon)\n                result.append(s)\n        # Optionally,\
        \ lightweight fuzzy/synonym check: collapse variants like 'Export as CSV'\
        \ vs 'Export to CSV'\n        # Here, implement a minimal Jaccard-like deduplication\
        \ based on token overlap\n        final = []\n        token_seen = []\n  \
        \      for s in result:\n            tokens = set(re.findall(r'\\w+', s.lower()))\n\
        \            duplicate = False\n            for prev_tokens in token_seen:\n\
        \                intersection = tokens & prev_tokens\n                union\
        \ = tokens | prev_tokens\n                if union and len(intersection)/len(union)\
        \ > 0.85:  # High overlap\n                    duplicate = True\n        \
        \            break\n            if not duplicate:\n                token_seen.append(tokens)\n\
        \                final.append(s)\n        return final\n\n    unique_features\
        \ = unique_by_semantics(normalized_atoms)\n    return unique_features\n"
      name: normalize_and_deduplicate_features
      nodes_depended_on: []
      nodes_dependent_on: []
      output_structure:
      - description: Output of type List[str]
        key: output
        type: List[str]
      - description: Input parameter of type str
        key: features
        type: str
      prd:
        bullets:
        - complexity: MEDIUM
          impact: Enables clear feature traceability and prevents ambiguity or overlap
            in later PRD stages.
          method: Apply rule-based or NLP-based parsing to identify conjunctions,
            separators, or compound verbs, then split complex statements accordingly.
          reason: Atomic features are necessary to ensure precise assignment and sequencing
            in subsequent workflow processing.
          text: Split compound feature statements into individual atomic functional
            units.
        - complexity: LOW
          impact: Reduces inconsistencies or errors caused by multiple representations
            of the same requirement.
          method: Perform case-insensitive string matching and, optionally, employ
            lightweight semantic similarity (e.g., fuzzy matching or synonym checks)
            to find near-duplicates.
          reason: Uniqueness ensures each feature is only tracked and implemented
            once, avoiding wasteful duplication in workflow logic.
          text: Deduplicate feature strings by identifying and removing exact repeats
            and normalizing semantically redundant entries.
        - complexity: LOW
          impact: Improves feature interpretability and integration in downstream
            systems that rely on pattern matching or logical ordering.
          method: 'Implement simple text normalization routines: trim whitespace,
            apply consistent capitalization, and rephrase to a standard task format.'
          reason: Consistent phrasing makes the list more readable, easier to compare,
            and simplifies integration with other automation components.
          text: Normalize feature text to a standard convention (e.g., 'Verb-object'
            format) and clean up whitespace, tense, and punctuation.
      prompt: Given a list of workflow feature descriptions as input, process the
        list to (1) split any compound or multi-part feature statements into atomic,
        indivisible functional units, (2) eliminate any duplicate or semantically
        redundant features, and (3) normalize the textual representation for consistency,
        outputting a clean, ordered list of unique atomic features.
      shims: []
    - agent: false
      code_node_type: virtual-pure
      description: Reorders and sequences a list of workflow feature names to ensure
        strict adherence to workflow constraints, such as enforcing that a senior
        manager meeting occurs before user-role determination and that all features
        are arranged according to required process flow.
      implementation: "def sequence_features_per_workflow_order(features: str) ->\
        \ List[str]:\n    \"\"\"\n    Reorders and sequences a list of workflow feature\
        \ names to ensure strict adherence to workflow constraints, such as enforcing\
        \ that a senior manager meeting occurs before user-role determination and\
        \ that all features are arranged according to required process flow.\n\n \
        \   Args:\n        features: Input parameter of type str\n\n    Returns:\n\
        \        List[str]: Output of type List[str]\n    \"\"\"\n    import re\n\
        \    from typing import List\n\n    # Step 1: Parse the input string into\
        \ features list (assuming comma separated or newline separated)\n    # Try\
        \ to be robust: split on comma or newline, then strip whitespace\n    if '\\\
        n' in features:\n        raw_features = features.strip().split('\\n')\n  \
        \  else:\n        raw_features = features.strip().split(',')\n    feature_list\
        \ = [f.strip() for f in raw_features if f.strip()]\n\n    # Step 2: Normalize\
        \ feature names (case and spacing, e.g. trim, collapse whitespace, title case)\n\
        \    normalized_features = []\n    for f in feature_list:\n        # Remove\
        \ extra spaces, normalize spaces, title case\n        clean_f = re.sub(r'\\\
        s+', ' ', f.strip())\n        normalized_features.append(clean_f)\n\n    #\
        \ Step 3: Enforce explicit ordering constraints\n    # Constraint 1: 'Schedule\
        \ and conduct a meeting with senior managers' MUST be first and no role-related\
        \ features before it\n    # Identify the target feature string (normalize\
        \ as above for match)\n    meeting_feature_target = 'Schedule and conduct\
        \ a meeting with senior managers'\n    meeting_feature_index = None\n    for\
        \ i, f in enumerate(normalized_features):\n        if re.sub(r'\\s+', ' ',\
        \ f.strip()).lower() == meeting_feature_target.lower():\n            meeting_feature_index\
        \ = i\n            break\n    # Remove all instances of the meeting feature\n\
        \    normalized_features_wo_meeting = [f for f in normalized_features if f.lower()\
        \ != meeting_feature_target.lower()]\n    # Determine role-related features\
        \ via regex (heuristic: features mentioning 'role', 'assign role', 'determine\
        \ user role', etc.)\n    role_related_indices = []\n    role_related_pattern\
        \ = re.compile(r'(role|user role|assign.*role|determine.*role|role\\W)', re.IGNORECASE)\n\
        \    role_features = []\n    non_role_features = []\n    for f in normalized_features_wo_meeting:\n\
        \        if role_related_pattern.search(f):\n            role_features.append(f)\n\
        \        else:\n            non_role_features.append(f)\n\n    # The order\
        \ should be:\n    #   [meeting_feature, ...non-role features (excluding meeting),\
        \ ...role_related_features]\n    # Remove duplicates while preserving order\
        \ (meeting_feature, then non-role, then role)\n    seq = []\n    if meeting_feature_index\
        \ is not None:\n        seq.append(meeting_feature_target)\n    else:\n  \
        \      # If not found, just skip (to avoid error)\n        pass\n    # Add\
        \ non-role, non-meeting features in their original order\n    seen = set(s.lower()\
        \ for s in seq)  # Already added\n    for f in non_role_features:\n      \
        \  lcf = f.lower()\n        if lcf not in seen:\n            seq.append(f)\n\
        \            seen.add(lcf)\n    # Add all role features after meeting and\
        \ others\n    for f in role_features:\n        lcf = f.lower()\n        if\
        \ lcf not in seen:\n            seq.append(f)\n            seen.add(lcf)\n\
        \    # Step 4: Remove accidental duplicates (already done above via seen set),\
        \ force one last time for sanity\n    out = []\n    out_seen = set()\n   \
        \ for f in seq:\n        lcf = f.lower()\n        if lcf not in out_seen:\n\
        \            out.append(f)\n            out_seen.add(lcf)\n    # Step 5: Normalize\
        \ formatting of feature names again (already normalized, but just to be strict)\n\
        \    final_features = [re.sub(r'\\s+', ' ', f.strip()) for f in out]\n   \
        \ return final_features\n"
      name: sequence_features_per_workflow_order
      nodes_depended_on: []
      nodes_dependent_on: []
      output_structure:
      - description: Output of type List[str]
        key: output
        type: List[str]
      - description: Input parameter of type str
        key: features
        type: str
      prd:
        bullets:
        - complexity: MEDIUM
          impact: Guarantees the generated workflow sequence is valid, actionable,
            and compliant with defined business logic, reducing risk of process violations.
          method: Implement pattern matching or rule-based logic to detect feature
            categories (e.g., regex for role-related features) and reorder the list
            by applying a prioritization or dependency graph.
          reason: Explicit ordering is needed to reflect business requirements and
            workflow constraints, particularly the mandated sequence for meetings
            and role assignments.
          text: Parse the input workflow features to identify and enforce explicitly
            defined ordering constraints, such as placing the 'Schedule and conduct
            a meeting with senior managers' feature first and ensuring no role-related
            features precede it.
        - complexity: LOW
          impact: Improves workflow clarity, reliability, and ensures unique actionable
            units in subsequent workflow nodes.
          method: Use set operations or hashing for deduplication and standardize
            feature name formatting (e.g., string trimming and normalization) before
            final serialization.
          reason: Ensures the sequenced output is clean, atomic, and free from redundancy
            or ambiguity which could disrupt downstream processing.
          text: Remove any remaining accidental duplicates and normalize the formatting
            of feature names in the reordered list.
      prompt: "Given a list of atomic, deduplicated workflow feature names (features:\
        \ str, each a feature per line), reorder the list to satisfy all known workflow\
        \ constraints\u2014for example, ensuring that 'Schedule and conduct a meeting\
        \ with senior managers' is always first and that any feature related to determining\
        \ or assigning user roles appears only after the meeting node. Output a strictly\
        \ ordered list (one feature per line, no duplicates, no extraneous whitespace)\
        \ that matches the intended workflow sequence, reflecting all given business\
        \ rules."
      shims: []
    - agent: false
      code_node_type: virtual-pure
      description: Validates a list of workflow feature names for correctness (atomicity,
        uniqueness, sequencing, compliance with workflow constraints) and serializes
        them as a clean, ordered List[str] suitable for downstream use.
      implementation: "def validate_and_serialize_feature_list(features: str) -> List[str]:\n\
        \    \"\"\"\n    Validates a list of workflow feature names for correctness\
        \ (atomicity, uniqueness, sequencing, compliance with workflow constraints)\
        \ and serializes them as a clean, ordered List[str] suitable for downstream\
        \ use.\n\n    Args:\n        features: Input parameter of type str\n\n   \
        \ Returns:\n        List[str]: Output of type List[str]\n    \"\"\"\n    #\
        \ --- PURE IMPLEMENTATION ---\n    import re\n    from typing import List\n\
        \n    # 1. Parse input\n    # If features is given as a comma-separated string,\
        \ e.g., 'a, b, c', parse accordingly\n    if isinstance(features, str):\n\
        \        # Remove surrounding brackets or quotes, if any\n        features_str\
        \ = features.strip()\n        # Try to parse if it's a literal list-string\
        \ (e.g. [\"a\", \"b\"])\n        if (features_str.startswith('[') and features_str.endswith(']'))\
        \ or (features_str.startswith('(') and features_str.endswith(')')):\n    \
        \        import ast\n            try:\n                features_list = ast.literal_eval(features_str)\n\
        \                if not isinstance(features_list, list):\n               \
        \     raise ValueError(\"Parsed features input is not a list.\")\n       \
        \     except Exception as e:\n                raise ValueError(f\"Failed to\
        \ parse features list: {e}\")\n        else:\n            # Assume comma-separated\n\
        \            features_list = [f.strip() for f in features_str.split(',') if\
        \ f.strip()]\n    else:\n        raise ValueError(\"Input features must be\
        \ a string.\")\n\n    # 2. Validate atomicity and type\n    atomic_features\
        \ = []\n    for i, f in enumerate(features_list):\n        # Must be a string\
        \ and not empty/non-whitespace\n        if not isinstance(f, str):\n     \
        \       raise ValueError(f\"Feature at index {i} is not a string: {f!r}\"\
        )\n        if not f.strip():\n            raise ValueError(f\"Feature at index\
        \ {i} is empty or blank.\")\n        # For atomicity: ensure does not appear\
        \ compound (e.g., contain commas, 'and', ';', or slashes, or is too long)\n\
        \        compound_pattern = re.compile(r'(,|;|\\band\\b|/)', re.IGNORECASE)\n\
        \        if compound_pattern.search(f):\n            raise ValueError(f\"\
        Feature '{f}' at index {i} is not atomic: appears compound.\")\n        atomic_features.append(f.strip())\n\
        \n    # 3. Check for duplicates\n    seen = set()\n    duplicates = set()\n\
        \    for f in atomic_features:\n        if f in seen:\n            duplicates.add(f)\n\
        \        seen.add(f)\n    if duplicates:\n        raise ValueError(f\"Duplicate\
        \ features found: {sorted(list(duplicates))}\")\n\n    # 4. Enforce sequencing\
        \ constraints\n    # Example: 'Schedule and conduct a meeting with senior\
        \ managers' must precede user role determination features\n    meeting_pattern\
        \ = re.compile(r'schedule and conduct a meeting with senior managers', re.IGNORECASE)\n\
        \    user_role_patterns = [\n        re.compile(r'role determination', re.IGNORECASE),\n\
        \        re.compile(r'determine user', re.IGNORECASE),\n        re.compile(r'user\
        \ role', re.IGNORECASE),\n    ]\n\n    meeting_indices = [i for i, f in enumerate(atomic_features)\
        \ if meeting_pattern.search(f)]\n    user_role_indices = [i for i, f in enumerate(atomic_features)\
        \ if any(p.search(f) for p in user_role_patterns)]\n    if user_role_indices:\n\
        \        if not meeting_indices:\n            raise ValueError(\"'Schedule\
        \ and conduct a meeting with senior managers' must appear before any user\
        \ role determination features, but is missing.\")\n        min_user_role_idx\
        \ = min(user_role_indices)\n        max_meeting_idx = max(meeting_indices)\n\
        \        if max_meeting_idx >= min_user_role_idx:\n            raise ValueError(\"\
        'Schedule and conduct a meeting with senior managers' must precede all user\
        \ role determination features.\")\n    # 5. Return the validated list (cleaned\
        \ from whitespace)\n    return atomic_features\n"
      name: validate_and_serialize_feature_list
      nodes_depended_on: []
      nodes_dependent_on: []
      output_structure:
      - description: Output of type List[str]
        key: output
        type: List[str]
      - description: Input parameter of type str
        key: features
        type: str
      prd:
        bullets:
        - complexity: LOW
          impact: Reduces risk of cascading errors, ambiguity, and misinterpretation
            by later nodes; provides stronger guarantees about feature list cleanliness.
          method: Iterate over the feature list, check type and content for each entry,
            and use a set to filter or flag duplicates.
          reason: Ensuring atomicity and uniqueness of features is critical to correctly
            downstream processing and to avoid errors from compound or redundant features.
          text: Validate that every element in the input feature list is a non-empty,
            atomic (not compound) string and that the list contains no duplicates.
        - complexity: MEDIUM
          impact: Guarantees the workflow follows specified logic, preventing functional
            errors or policy violations downstream.
          method: Parse and identify key features in the list, then check or enforce
            their relative positions based on pattern matching and explicit rules.
          reason: Maintains compliance with business rules and success metrics that
            depend on strict ordering of workflow activities.
          text: Enforce sequencing and constraint rules, particularly that specified
            nodes such as 'Schedule and conduct a meeting with senior managers' precede
            any user role determination features.
        - complexity: LOW
          impact: Provides clear, actionable errors for integrators and maintains
            system robustness.
          method: Return the clean list as-is if all checks pass, otherwise throw
            exceptions with meaningful messages upon validation failure.
          reason: Consistent serialization ensures interoperability with downstream
            components and early error raising avoids hard-to-debug issues.
          text: Serialize the validated list into a standard List[str] format and
            raise descriptive errors if validation fails.
      prompt: Validate that the input 'features' list of workflow feature strings
        is fully normalized, atomic, unique, properly sequenced according to workflow
        and business rules (e.g., specified meetings occur before user role determination),
        and serialize it into a clean List[str] suitable for output or downstream
        use, raising an error on any invalid formatting, duplicates, or ordering violations.
      shims: []
  generate_feature_descriptions:
    agent: false
    code_node_type: pure
    description: Write a concise, one-sentence description for each feature, explicitly
      detailing any features related to scheduling and conducting a meeting with senior
      managers that must occur prior to determining user roles.
    implementation: "from pydantic import BaseModel, Field\nfrom typing import List\n\
      \n\nclass ExtractKeyFeaturesOutput(BaseModel):\n    \"\"\"Pydantic model for\
      \ extract_key_features node outputs.\"\"\"\n    feature_names: List[str] = Field(...,\
      \ description=\"Names of fundamental workflow features required in the PRD,\
      \ in sequence starting with a node to schedule and conduct a meeting with senior\
      \ managers prior to any role assignment, followed by other required workflow\
      \ features.\")\n\n\nclass GenerateFeatureDescriptionsOutput(BaseModel):\n  \
      \  \"\"\"Pydantic model for generate_feature_descriptions node outputs.\"\"\"\
      \n    feature_descriptions: List[str] = Field(..., description=\"Concise description\
      \ for each feature, including a clear specification of the meeting with senior\
      \ managers as a required step prior to determining user roles, aligned with\
      \ feature_names.\")\n\n\ndef generate_feature_descriptions_fx(extract_key_features_input:\
      \ ExtractKeyFeaturesOutput, **kwargs) -> GenerateFeatureDescriptionsOutput:\n\
      \    \"\"\"Write a concise, one-sentence description for each feature, explicitly\
      \ detailing any features related to scheduling and conducting a meeting with\
      \ senior managers that must occur prior to determining user roles.\n\n    Args:\n\
      \        extract_key_features_input: Input from the 'extract_key_features' node.\n\
      \        **kwargs: Additional keyword arguments.\n\n    Returns:\n        GenerateFeatureDescriptionsOutput:\
      \ Object containing outputs for this node.\n    \"\"\"\n    # 1. Parse and validate\
      \ feature_names order, ensuring 'schedule and conduct meeting with senior managers'\
      \ appears before any user role features\n    feature_names: List[str] = extract_key_features_input.feature_names\n\
      \    validate_feature_order(feature_names=feature_names)\n\n    # 2. Generate\
      \ concise, context-aware one-sentence description for each feature, handling\
      \ meeting sequencing requirement\n    feature_descriptions: List[str] = generate_descriptions_for_features(\n\
      \        feature_names=feature_names,\n        sequencing_node='schedule and\
      \ conduct meeting with senior managers'\n    )\n\n    # 3. Apply mandatory sequencing\
      \ clause and rationale to any feature matching the meeting node\n    feature_descriptions\
      \ = apply_sequencing_clauses_to_descriptions(\n        feature_names=feature_names,\n\
      \        descriptions=feature_descriptions,\n        sequencing_node='schedule\
      \ and conduct meeting with senior managers',\n        rationale='to solicit\
      \ executive input and ensure process alignment before any user role determination\
      \ occurs'\n    )\n\n    # 4. Perform 1:1 mapping and order consistency validation\
      \ between feature_names and feature_descriptions\n    validate_mapping_consistency(\n\
      \        features=feature_names, descriptions=feature_descriptions\n    )\n\n\
      \    # 5. Disambiguate and refine descriptions to minimize functional overlaps\n\
      \    feature_descriptions = disambiguate_descriptions_if_needed(\n        feature_names=feature_names,\n\
      \        descriptions=feature_descriptions\n    )\n\n    # Return output matching\
      \ signature\n    return GenerateFeatureDescriptionsOutput(feature_descriptions=feature_descriptions)\n"
    name: generate_feature_descriptions
    nodes_depended_on:
    - extract_key_features
    nodes_dependent_on: []
    output_structure:
    - description: Concise description for each feature, including a clear specification
        of the meeting with senior managers as a required step prior to determining
        user roles, aligned with feature_names.
      key: feature_descriptions
      type: List[str]
    prd:
      bullets:
      - complexity: LOW - Basic parsing and validation task, but essential for all
          subsequent logic.
        impact: HIGH - Enforces workflow integrity and compliance with the mandated
          process.
        method: Implement sequence verification logic using simple list/index lookups
          to enforce required feature order.
        reason: Ensures alignment with business requirements and enforces the critical
          workflow step where executive input is obtained before allocation of user
          roles, which may significantly alter downstream steps.
        text: Parse the ordered list of key feature names from the dependency node
          output to ensure proper feature sequencing, especially verifying that the
          'schedule and conduct meeting with senior managers' node appears ahead of
          any user role determination features.
      - complexity: MEDIUM - Requires succinct, precise language; may require context-awareness
          for technical/procedural details.
        impact: MEDIUM - Supports clarity and traceability, ensures features are well-understood
          across stakeholders.
        method: Use template-based sentence generation with dependency injection for
          context; employ NLP summarization or pattern templates for consistent phrasing.
        reason: Well-written, specific descriptions are necessary for downstream nodes
          (executive summary, documentation, developer handoff) and for unambiguous
          requirement tracking.
        text: Iterate through each feature name and generate a concise, one-sentence
          description that clearly articulates the purpose and functionality of the
          feature within the workflow.
      - complexity: LOW - Requires flagging and conditional formatting for features
          matching this criteria.
        impact: HIGH - Directly fulfills a regulatory/process requirement and downstream
          gating condition.
        method: 'Apply conditional logic: if feature name matches ''schedule and conduct
          meeting with senior managers'', prepend/apply a clause that references required
          sequencing and meeting purpose.'
        reason: Critical path sequencing must be unambiguous or business logic downstream
          could fail or be misaligned with compliance requirements.
        text: For features pertaining to scheduling and conducting the senior manager
          meeting, ensure the generated description explicitly indicates its mandatory
          occurrence before any process involving user role determination, including
          rationale (e.g., soliciting executive input or alignment).
      - complexity: LOW - Simple consistency and length checks.
        impact: HIGH - Prevents ambiguity and cross-reference errors in subsequent
          workflow nodes.
        method: Implement automated checks for list length equality and order consistency
          between feature_names and feature_descriptions; raise errors or warnings
          as appropriate.
        reason: Maintaining strict alignment between features and their descriptions
          is foundational for referential integrity and later linking (priority, metrics,
          traceability).
        text: Validate that each generated description is mapped 1:1 with the feature
          name list, ensuring no omissions and order consistency, so that each downstream
          node can reliably make aligned references.
      - complexity: MEDIUM - Requires context-awareness and possibly additional information
          from parent nodes or requirements.
        impact: MEDIUM - Reduces future technical debt and supports easier extension
          or modification.
        method: Incorporate disambiguation checks (NLP-based semantic similarity scoring)
          across generated descriptions and prompt revision if high overlap is detected.
        reason: Ensures that each feature is uniquely actionable and supports modularity,
          clarity, and non-redundancy in system design.
        text: Where applicable, tailor descriptions to clearly distinguish functional
          scope and avoid overlaps between features, to support modular implementation
          and future workflow maintenance.
    prompt: For each key feature, provide a one-sentence description that clearly
      and concisely describes the purpose or functionality of that feature within
      the workflow. Be sure to explain that the meeting with senior managers must
      take place prior to any process for determining user roles, with its purpose
      and sequencing clearly described.
    shims:
    - agent: false
      code_node_type: virtual-pure
      description: Validates that the required meeting with senior managers is scheduled
        and conducted before any user role-related features in the workflow feature
        list.
      implementation: "def validate_feature_order(feature_names: str) -> str:\n  \
        \  \"\"\"\n    Validates that the required meeting with senior managers is\
        \ scheduled and conducted before any user role-related features in the workflow\
        \ feature list.\n\n    Args:\n        feature_names: Input parameter of type\
        \ str\n\n    Returns:\n        str: Output of type Any\n    \"\"\"\n    #\
        \ --- PURE IMPLEMENTATION ---\n    # Parse the stringified feature_names to\
        \ a list (assuming standard Python list string, e.g., '[\"foo\", \"bar\"]')\n\
        \    import ast\n\n    try:\n        feature_list = ast.literal_eval(feature_names)\n\
        \    except Exception:\n        return \"Error: feature_names is not a valid\
        \ Python list string.\"\n    if not isinstance(feature_list, list):\n    \
        \    return \"Error: feature_names is not a list.\"\n\n    # Lowercase all\
        \ names for easier matching\n    feature_list_lower = [str(f).lower() for\
        \ f in feature_list]\n\n    # Identify meeting feature index\n    meeting_phrases\
        \ = [\n        \"schedule and conduct meeting with senior managers\",\n  \
        \      \"meeting with senior managers\"\n    ]\n    meeting_idx = None\n \
        \   for idx, fname in enumerate(feature_list_lower):\n        for phrase in\
        \ meeting_phrases:\n            if phrase in fname:\n                meeting_idx\
        \ = idx\n                break\n        if meeting_idx is not None:\n    \
        \        break\n    if meeting_idx is None:\n        return (\"Error: The\
        \ required 'schedule and conduct meeting with senior managers' feature \"\n\
        \                \"is missing from the feature list.\")\n\n    # Identify\
        \ all user role-related feature indices\n    # Key words/phrases for role-related\
        \ features\n    role_keywords = [\n        \"assign user role\",\n       \
        \ \"determine user role\",\n        \"user role assignment\",\n        \"\
        role assignment\",\n        \"user role determination\",\n        \"role determination\"\
        ,\n        \"assign role\",\n        \"set user role\",\n        \"set role\"\
        \n    ]\n    role_related_indices = []\n    for idx, fname in enumerate(feature_list_lower):\n\
        \        for kw in role_keywords:\n            if kw in fname:\n         \
        \       role_related_indices.append(idx)\n                break\n\n    # Check\
        \ for role feature(s) before the meeting feature\n    for role_idx in role_related_indices:\n\
        \        if role_idx < meeting_idx:\n            return (\n              \
        \  f\"Validation error: Role-related feature (index {role_idx}) appears before\
        \ the required meeting \"\n                f\"feature (index {meeting_idx}).\
        \ Please ensure scheduling/conducting the senior managers meeting \"\n   \
        \             f\"occurs before any user role-related processing in your feature\
        \ list.\"\n            )\n    # If we reach here, order is valid.\n    return\
        \ \"valid\"\n"
      name: validate_feature_order
      nodes_depended_on: []
      nodes_dependent_on: []
      output_structure:
      - description: Output of type Any
        key: output
        type: str
      - description: Input parameter of type str
        key: feature_names
        type: str
      prd:
        bullets:
        - complexity: LOW
          impact: Ensures critical workflow design integrity by enforcing prerequisites
            mandated by organizational policy or stakeholder requirements.
          method: Iterate through the feature_names list, match strings to pre-defined
            keywords/phrases for the meeting and role features, and record their positions.
          reason: This parsing is required to determine if the mandated feature ordering
            constraint is being upheld in the feature list.
          text: Parse the feature_names input list and identify the indices of 'schedule
            and conduct meeting with senior managers' and all features related to
            user role determination or assignment.
        - complexity: LOW
          impact: Prevents downstream logic and requirements errors by catching sequencing
            problems at an early validation step.
          method: If any role-related index is less than the meeting index, raise
            an exception with a concise, actionable error message indicating the violation.
          reason: This comparison enforces correct workflow sequencing and provides
            clear developer or user feedback during validation failures.
          text: Compare the index of the meeting feature to those of any user role-related
            features, raising a descriptive error if any role-related feature appears
            before the meeting.
        - complexity: LOW
          impact: Provides simple interface for upstream and downstream system components
            to confidently rely on feature order correctness.
          method: Return None or 'valid' when constraints are satisfied; otherwise
            return or raise a human-readable validation error.
          reason: Callers need a clear indication of the validation result to proceed
            with workflow generation or halt on error.
          text: Return either a success signal (e.g., null or confirmation string)
            upon validation pass, or the explicit error message upon validation failure.
      prompt: Ensure that within the provided list of feature names, the feature 'schedule
        and conduct meeting with senior managers' appears before any feature related
        to determining or assigning user roles; if this rule is violated, raise an
        explicit error describing the required sequencing.
      shims: []
    - agent: false
      code_node_type: virtual-pure
      description: Generates concise, context-aware one-sentence descriptions for
        each workflow feature name provided, ensuring sequencing and specification
        requirements are met for nodes such as required meetings before user role
        determination.
      implementation: "def generate_descriptions_for_features(feature_names: str,\
        \ sequencing_node: str) -> List[str]:\n    \"\"\"\n    Generates concise,\
        \ context-aware one-sentence descriptions for each workflow feature name provided,\
        \ ensuring sequencing and specification requirements are met for nodes such\
        \ as required meetings before user role determination.\n\n    Args:\n    \
        \    feature_names: Input parameter of type str\nsequencing_node: Input parameter\
        \ of type str\n\n    Returns:\n        List[str]: Output of type List[str]\n\
        \    \"\"\"\n    import re\n    from typing import List\n    \n    # Split\
        \ input feature_names string into a list (comma, semicolon, or new line delimited)\n\
        \    if isinstance(feature_names, str):\n        # Accept commas, semicolons,\
        \ newlines as delimiters\n        names_raw = re.split(r'[\\n;,]+', feature_names)\n\
        \        names = [n.strip() for n in names_raw if n.strip()]\n    else:\n\
        \        # assume list-like, fallback for robustness\n        names = list(feature_names)\n\
        \    \n    # Deduplicate and preserve order, minimal functional overlap: exact\
        \ string match deduplication for low complexity\n    seen = set()\n    deduped_names\
        \ = []\n    for n in names:\n        if n not in seen:\n            deduped_names.append(n)\n\
        \            seen.add(n)\n    names = deduped_names\n    \n    # Template\
        \ for regular features\n    def describe_feature(name: str) -> str:\n    \
        \    # Template-based, concise description for the role (assume product owner/writer)\n\
        \        return f\"The '{name}' feature enables its intended workflow step,\
        \ providing clear functionality for business process implementation.\"\n \
        \   \n    # Specialized template for sequencing_node\n    def describe_sequencing_feature(name:\
        \ str) -> str:\n        return (f\"The '{name}' feature must be completed\
        \ prior to any user role determination, enforcing process sequencing and ensuring\
        \ that required steps are followed before roles are assigned; this prevents\
        \ premature role allocation and maintains workflow integrity.\")\n    \n \
        \   output_descriptions: List[str] = []\n    for n in names:\n        if n\
        \ == sequencing_node:\n            desc = describe_sequencing_feature(n)\n\
        \        else:\n            desc = describe_feature(n)\n        output_descriptions.append(desc)\n\
        \    \n    # Output must preserve 1:1 mapping and order\n    return output_descriptions\n"
      name: generate_descriptions_for_features
      nodes_depended_on: []
      nodes_dependent_on: []
      output_structure:
      - description: Output of type List[str]
        key: output
        type: List[str]
      - description: Input parameter of type str
        key: feature_names
        type: str
      - description: Input parameter of type str
        key: sequencing_node
        type: str
      prd:
        bullets:
        - complexity: LOW
          impact: Improves specification quality and alignment, reducing the risk
            of ambiguity in implementation.
          method: Use a templated or rule-based text generation approach that systematically
            produces descriptions, such as string formatting or deterministic prompt
            logic.
          reason: "Clear and accurate descriptions ensure all stakeholders understand\
            \ each feature\u2019s purpose and prevent misinterpretation during PRD\
            \ handoff."
          text: For each feature name, generate a concise, one-sentence, and role-specific
            description that clearly communicates the feature's intent and functionality.
        - complexity: MEDIUM
          impact: Preserves workflow integrity and avoids improper role assignments
            prior to mandated review.
          method: 'Implement a conditional check: when feature_names[n] == sequencing_node,
            append or embed the mandatory sequencing clause and rationale in the generated
            description.'
          reason: Highlighting critical workflow steps and their required order ensures
            compliance with process constraints and executive requirements.
          text: For any feature matching the 'sequencing_node', add explicit language
            in its description to specify that this step must occur prior to any user
            role determination, including its rationale for sequencing.
        - complexity: MEDIUM
          impact: Guarantees output reliability and direct usability for downstream
            PRD steps.
          method: Validate list lengths, enforce consistent ordering, and apply deduplication/disambiguation
            logic where needed, possibly by comparing semantic similarity between
            generated sentences.
          reason: Maintaining order and uniqueness directly supports traceability
            and implementation mapping processes.
          text: Ensure 1:1 mapping and order consistency between input feature_names
            and output descriptions, including de-duplication and minimal functional
            overlap.
      prompt: Given a list of feature names (feature_names) and a sequencing node
        (sequencing_node), generate a one-sentence, concise, and unambiguous description
        for each feature, explicitly specifying that any feature related to the 'sequencing_node'
        (e.g., 'schedule and conduct meeting with senior managers') includes language
        about its required placement and purpose before user role assignment. Ensure
        that each description directly corresponds in order and intent to its feature
        name and avoid redundancy or overlap between descriptions.
      shims: []
    - agent: false
      code_node_type: virtual-pure
      description: This node modifies feature descriptions by appending or integrating
        mandatory sequencing clauses and their rationale for any feature matching
        a specified sequencing node.
      implementation: "def apply_sequencing_clauses_to_descriptions(feature_names:\
        \ str, descriptions: str, sequencing_node: str, rationale: str) -> str:\n\
        \    \"\"\"\n    This node modifies feature descriptions by appending or integrating\
        \ mandatory sequencing clauses and their rationale for any feature matching\
        \ a specified sequencing node.\n\n    Args:\n        feature_names: Input\
        \ parameter of type str\ndescriptions: Input parameter of type str\nsequencing_node:\
        \ Input parameter of type str\nrationale: Input parameter of type str\n\n\
        \    Returns:\n        str: Output of type Any\n    \"\"\"\n    import json\n\
        \n    # Parse the input strings into Python lists\n    try:\n        names_list\
        \ = json.loads(feature_names)\n    except Exception as e:\n        raise ValueError(\"\
        feature_names input must be a JSON-formatted list of strings\") from e\n\n\
        \    try:\n        desc_list = json.loads(descriptions)\n    except Exception\
        \ as e:\n        raise ValueError(\"descriptions input must be a JSON-formatted\
        \ list of strings\") from e\n\n    # Make sure both lists are aligned\n  \
        \  if not isinstance(names_list, list) or not isinstance(desc_list, list):\n\
        \        raise TypeError(\"feature_names and descriptions must parse to lists\
        \ of strings\")\n    if len(names_list) != len(desc_list):\n        raise\
        \ ValueError(\"feature_names and descriptions must have the same length\"\
        )\n\n    # Define sequencing clause template\n    # Example: \"This step must\
        \ occur before <sequencing_node>. Rationale: <rationale>.\"\n    sequencing_clause\
        \ = f\" This step must occur before '{sequencing_node}'. Rationale: {rationale}\"\
        \ if rationale else f\" This step must occur before '{sequencing_node}'.\"\
        \n\n    updated_desc_list = []\n    for i, (fname, dsc) in enumerate(zip(names_list,\
        \ desc_list)):\n        if sequencing_node.lower() in fname.lower():\n   \
        \         # Feature matches the sequencing node, append the sequencing clause\n\
        \            # Use punctuation and conjunction per PRD\n            if dsc.strip().endswith('.'):\n\
        \                new_desc = f\"{dsc.strip()} {sequencing_clause.strip()}\"\
        \n            else:\n                new_desc = f\"{dsc.strip()}. {sequencing_clause.strip()}\"\
        \n            updated_desc_list.append(new_desc)\n        else:\n        \
        \    # Leave as is\n            updated_desc_list.append(dsc)\n    # Output\
        \ as JSON-formatted string for downstream consumption\n    return json.dumps(updated_desc_list)\n"
      name: apply_sequencing_clauses_to_descriptions
      nodes_depended_on: []
      nodes_dependent_on: []
      output_structure:
      - description: Output of type Any
        key: output
        type: str
      - description: Input parameter of type str
        key: feature_names
        type: str
      - description: Input parameter of type str
        key: descriptions
        type: str
      - description: Input parameter of type str
        key: sequencing_node
        type: str
      - description: Input parameter of type str
        key: rationale
        type: str
      prd:
        bullets:
        - complexity: MEDIUM
          impact: Prevents ambiguity in workflow order and reduces downstream implementation
            errors by clarifying required process steps.
          method: Iterate over feature_names, identify matches with sequencing_node,
            and update the corresponding index of descriptions to append or integrate
            the rationale and sequencing requirement text.
          reason: Ensures that critical workflow sequencing is clearly communicated
            in feature documentation.
          text: Detect occurrences of the sequencing_node within feature_names and
            update the associated descriptions to explicitly include the sequencing
            constraint and rationale.
        - complexity: LOW
          impact: Allows seamless downstream validation and ensures that sequencing
            information is tightly coupled to the correct feature.
          method: Return a transformed description list of the same length as feature_names,
            mapping updates by matching indexes.
          reason: Alignment is essential for downstream validation and to avoid mismatched
            instructions.
          text: Maintain a one-to-one alignment between the updated feature descriptions
            and the original feature_names to preserve their order and mapping integrity.
        - complexity: LOW
          impact: Enhances comprehension for both technical and non-technical stakeholders.
          method: Use punctuation, conjunctions (such as 'before'), or templates to
            append the sequencing clause in a standardized way.
          reason: Clarity in documentation avoids misinterpretation during handoff
            or implementation.
          text: Format the updated descriptions clearly to distinguish the sequencing
            clause from the base feature statement.
      prompt: Modify the input list of feature descriptions so that for any feature
        in feature_names matching the sequencing_node, the corresponding feature description
        includes a requirement specifying that this action must occur before subsequent
        features, with the provided rationale clearly stated; return the updated list
        of descriptions as a string.
      shims: []
    - agent: false
      code_node_type: virtual-pure
      description: This node verifies that the mapping between a list of workflow
        feature names and their corresponding descriptions is both one-to-one and
        order-consistent, raising an explicit error if any mismatch or misalignment
        is found.
      implementation: "def validate_mapping_consistency(features: str, descriptions:\
        \ str) -> str:\n    \"\"\"\n    This node verifies that the mapping between\
        \ a list of workflow feature names and their corresponding descriptions is\
        \ both one-to-one and order-consistent, raising an explicit error if any mismatch\
        \ or misalignment is found.\n\n    Args:\n        features: Input parameter\
        \ of type str\ndescriptions: Input parameter of type str\n\n    Returns:\n\
        \        str: Output of type Any\n    \"\"\"\n    # --- PURE PYTHON IMPLEMENTATION\
        \ ---\n    import ast\n    \n    # Try to parse the input strings into lists\n\
        \    try:\n        features_list = ast.literal_eval(features)\n        descriptions_list\
        \ = ast.literal_eval(descriptions)\n    except Exception as e:\n        raise\
        \ ValueError(f\"Failed to parse features or descriptions as Python lists.\
        \ Error: {e}\\nfeatures: {features}\\ndescriptions: {descriptions}\")\n\n\
        \    if not isinstance(features_list, list) or not isinstance(descriptions_list,\
        \ list):\n        raise ValueError(f\"Input data is not a valid list.\\nParsed\
        \ features: {features_list}\\nParsed descriptions: {descriptions_list}\")\n\
        \n    # 1. Ensure lengths are equal\n    if len(features_list) != len(descriptions_list):\n\
        \        return (\n            f\"Mismatch in number of features and descriptions.\
        \ \"\n            f\"features has {len(features_list)}, descriptions has {len(descriptions_list)}.\\\
        n\"\n            f\"First 5 features: {features_list[:5]}\\nFirst 5 descriptions:\
        \ {descriptions_list[:5]}\"\n        )\n\n    # 2. Validate strict order preservation\n\
        \    # We'll assume that the intended mapping is that features[i] should match\
        \ descriptions[i]\n    # If stricter matching is needed, e.g., by an identifier\
        \ inside the string, logic could be added here.\n    mismatches = []\n   \
        \ for idx, (feature, description) in enumerate(zip(features_list, descriptions_list)):\n\
        \        # If more context for matching is needed, adjust here. For now, we\
        \ just check both exist for the index.\n        if not isinstance(feature,\
        \ str) or not isinstance(description, str):\n            mismatches.append(f\"\
        Index {idx}: Invalid types. Feature type: {type(feature)}, Description type:\
        \ {type(description)}\")\n        # Optionally implement additional consistency\
        \ checks, e.g., if feature is a substring of description, etc.\n\n    if mismatches:\n\
        \        # Collate all mismatch info into output string\n        mismatch_report\
        \ = \"Order or type mismatch detected at the following indices:\\n\" + \"\\\
        n\".join(mismatches)\n        return mismatch_report\n\n    # If we made it\
        \ here, mapping is valid and order is preserved\n    return \"Mapping between\
        \ features and descriptions is valid and order-consistent.\"\n"
      name: validate_mapping_consistency
      nodes_depended_on: []
      nodes_dependent_on: []
      output_structure:
      - description: Output of type Any
        key: output
        type: str
      - description: Input parameter of type str
        key: features
        type: str
      - description: Input parameter of type str
        key: descriptions
        type: str
      prd:
        bullets:
        - complexity: LOW
          impact: Prevents data misalignment which could lead to misinterpretation
            of system requirements or incorrect PRD documentation.
          method: Check list lengths with `len()` and immediately return or raise
            an error if they differ.
          reason: A one-to-one mapping requires that each feature has exactly one
            matching description in the same ordinal position.
          text: Ensure the lengths of the features and descriptions lists are equal.
        - complexity: MEDIUM
          impact: Guarantees feature documentation remains unambiguous and sequenced
            correctly for both human and downstream automated consumption.
          method: Iterate over both lists in parallel (via `zip` or index) and ensure
            each pair is logically matched, possibly using additional identifiers
            if necessary.
          reason: Maintaining order is crucial so each feature's description accurately
            corresponds to its functional intent and process sequence.
          text: Validate strict order preservation between features and descriptions.
        - complexity: LOW
          impact: Improves developer/user experience and reliability of further processing
            steps dependent on the mapping.
          method: Raise a ValueError or return a formatted string describing the index/location
            and type of mismatch (e.g., extra/missing item or order violation).
          reason: Precise diagnostics facilitate rapid correction and prevent silent
            failures that could propagate bugs or misalignments.
          text: Provide clear error output specifying the nature and location of any
            detected mismatch.
      prompt: 'Description: Typed node for shim validate_mapping_consistency'
      shims: []
    - agent: false
      code_node_type: virtual-pure
      description: This node reviews and refines a list of feature descriptions to
        ensure each is clearly worded, uniquely mapped to its feature, and free of
        functional overlaps or ambiguities.
      implementation: "def disambiguate_descriptions_if_needed(feature_names: str,\
        \ descriptions: str) -> str:\n    \"\"\"\n    This node reviews and refines\
        \ a list of feature descriptions to ensure each is clearly worded, uniquely\
        \ mapped to its feature, and free of functional overlaps or ambiguities.\n\
        \n    Args:\n        feature_names: Input parameter of type str\ndescriptions:\
        \ Input parameter of type str\n\n    Returns:\n        str: Output of type\
        \ Any\n    \"\"\"\n    import re\n    import json\n    from typing import\
        \ List\n    \n    # Helper: Parse input (assume comma or newline separated)\n\
        \    def parse_list(s: str) -> List[str]:\n        s = s.strip()\n       \
        \ if s.startswith('[') and s.endswith(']'):\n            try:\n          \
        \      return json.loads(s)\n            except Exception:\n             \
        \   pass\n        # Split on newlines or commas, strip whitespace\n      \
        \  items = re.split(r'\\n|,', s)\n        return [item.strip() for item in\
        \ items if item.strip()]\n\n    feature_names_list = parse_list(feature_names)\n\
        \    descriptions_list = parse_list(descriptions)\n\n    # Validate strict\
        \ 1:1 mapping\n    if len(feature_names_list) != len(descriptions_list):\n\
        \        raise ValueError(f\"Feature names ({len(feature_names_list)}) and\
        \ descriptions ({len(descriptions_list)}) are not 1:1 mapped.\")\n    \n \
        \   # Helper: Basic similarity (Jaccard overlap)\n    def text_overlap_score(a:\
        \ str, b: str) -> float:\n        set_a = set(re.findall(r'\\w+', a.lower()))\n\
        \        set_b = set(re.findall(r'\\w+', b.lower()))\n        if not set_a\
        \ or not set_b:\n            return 0.0\n        intersection = set_a & set_b\n\
        \        union = set_a | set_b\n        return len(intersection) / len(union)\n\
        \n    # Pairwise: find overlaps, redundancies, or ambiguities\n    n = len(descriptions_list)\n\
        \    overlap_threshold = 0.7  # tweak for sensitivity\n    ambiguous_pairs\
        \ = []  # Pairs with high overlap\n    for i in range(n):\n        for j in\
        \ range(i + 1, n):\n            sim = text_overlap_score(descriptions_list[i],\
        \ descriptions_list[j])\n            if sim > overlap_threshold:\n       \
        \         ambiguous_pairs.append((i, j, sim))\n\n    # Rephrase/rewrite ambiguous\
        \ descriptions\n    refined_descriptions = descriptions_list.copy()\n    for\
        \ i, j, sim in ambiguous_pairs:\n        # We'll attempt to add clarification\
        \ using the feature names\n        name_i = feature_names_list[i]\n      \
        \  name_j = feature_names_list[j]\n\n        def clarify(desc, name, idx):\n\
        \            # If name already mentioned, skip\n            if name.lower()\
        \ in desc.lower():\n                return desc\n            return f\"[{name}]\
        \ {desc}\"\n        # Disambiguate i and j\n        refined_descriptions[i]\
        \ = clarify(refined_descriptions[i], name_i, i)\n        refined_descriptions[j]\
        \ = clarify(refined_descriptions[j], name_j, j)\n\n    # Optionally, ensure\
        \ each description is unique\n    # If not, append feature name or index\n\
        \    seen = {}\n    for idx, desc in enumerate(refined_descriptions):\n  \
        \      if desc in seen:\n            # Make unique by appending feature name\n\
        \            refined_descriptions[idx] = f\"{desc} (Feature: {feature_names_list[idx]})\"\
        \n        seen[desc] = idx\n\n    # Assemble final output: ensure strict 1:1\
        \ mapping, same order\n    if len(refined_descriptions) != len(feature_names_list):\n\
        \        raise RuntimeError(\"Refinement broke 1:1 mapping or dropped items.\"\
        )\n    # Return as JSON list for clarity\n    return json.dumps(refined_descriptions,\
        \ ensure_ascii=False, indent=2)\n"
      name: disambiguate_descriptions_if_needed
      nodes_depended_on: []
      nodes_dependent_on: []
      output_structure:
      - description: Output of type Any
        key: output
        type: str
      - description: Input parameter of type str
        key: feature_names
        type: str
      - description: Input parameter of type str
        key: descriptions
        type: str
      prd:
        bullets:
        - complexity: MEDIUM
          impact: Yields a set of feature descriptions that are non-conflicting and
            facilitate precise development/specification.
          method: Use pairwise comparison and semantic similarity algorithms (such
            as cosine similarity on embeddings, or simple text overlap heuristics)
            to identify overlapping or ambiguous phrasing, then rewrite or rephrase
            offending descriptions.
          reason: Ensures the PRD is unambiguous and that developers or stakeholders
            can easily distinguish between individual features without confusion.
          text: Implement logic to detect and eliminate any functional overlaps, redundancies,
            or ambiguities across feature descriptions, ensuring each description
            clearly represents a unique workflow step.
        - complexity: LOW
          impact: Maintains structural integrity of the PRD and avoids specification
            drift.
          method: Count and index-check lists before and after disambiguation, raising
            errors or warnings if mappings are lost or reordered.
          reason: Prevents misalignment in documentation or implementation that could
            cause errors in feature interpretation.
          text: Validate strict 1:1 mapping between the feature names and their refined
            descriptions, maintaining sequence and correspondence.
        - complexity: MEDIUM
          impact: Ensures each feature remains fully and clearly described for its
            intended purpose.
          method: Use string matching and context preservation techniques in rewriting;
            optionally leverage NLP summarization tools to phrase unique but faithful
            descriptions.
          reason: Retains crucial information required for downstream PRD fidelity
            and avoids loss of non-overlapping context.
          text: Preserve the intent and essential details of each original description
            while refining for clarity and uniqueness.
      prompt: Given a list of feature names and their corresponding descriptions,
        review the feature descriptions for any overlaps, ambiguities, or functional
        redundancies, and refine each description so that every feature is uniquely
        and unambiguously described with minimal overlap or confusion between descriptions.
      shims: []
  identify_core_objective:
    agent: false
    code_node_type: pure
    description: Extract the single core objective of the workflow based on provided
      requirements, ensuring that extracted constraints will be available as input
      for downstream nodes such as identification of success metrics. Additionally,
      explicitly incorporate the requirement to add a node for a meeting with senior
      managers prior to determining user roles as part of the workflow, ensuring this
      sequencing is considered in defining the core objective.
    implementation: "from pydantic import BaseModel, Field\n\n\nclass IdentifyCoreObjectiveOutput(BaseModel):\n\
      \    \"\"\"Pydantic model for identify_core_objective node outputs.\"\"\"\n\
      \    core_objective: str = Field(..., description=\"The primary objective statement\
      \ for the workflow as described in the requirements, reflecting the need to\
      \ include a meeting with senior managers prior to determining user roles, and\
      \ noting that constraints should be available for use by dependent nodes, such\
      \ as those identifying success metrics.\")\n\n\ndef identify_core_objective_fx(general_input:\
      \ str, **kwargs) -> IdentifyCoreObjectiveOutput:\n    \"\"\"Extract the single\
      \ core objective of the workflow based on provided requirements, ensuring that\
      \ extracted constraints will be available as input for downstream nodes such\
      \ as identification of success metrics. Additionally, explicitly incorporate\
      \ the requirement to add a node for a meeting with senior managers prior to\
      \ determining user roles as part of the workflow, ensuring this sequencing is\
      \ considered in defining the core objective.\n\n    Args:\n        general_input:\
      \ General input string for the root node.\n        **kwargs: Additional keyword\
      \ arguments.\n\n    Returns:\n        IdentifyCoreObjectiveOutput: Object containing\
      \ outputs for this node.\n    \"\"\"\n    # Step 1: Extract and summarize the\
      \ core objective from the input requirements\n    core_objective_base: str =\
      \ extract_core_objective_from_requirements(requirements_text=general_input)\n\
      \    # Step 2: Detect requirement for sequencing (senior manager meeting before\
      \ user role assignment)\n    sequencing_clause: str = extract_sequencing_clause_for_meeting(requirements_text=general_input)\n\
      \    # Step 3: Extract references to technical/business constraints\n    constraints_reference:\
      \ str = extract_constraint_references(requirements_text=general_input)\n   \
      \ # Step 4: Synthesize the final objective statement integrating all elements\n\
      \    core_objective: str = compose_final_objective_statement(\n        base_objective=core_objective_base,\n\
      \        sequencing=sequencing_clause,\n        constraints=constraints_reference\n\
      \    )\n    # Step 5: Enforce output schema/type safety\n    validate_core_objective_output_schema(core_objective=core_objective)\n\
      \    # (Optional) Store in shared context for downstream availability\n    store_node_output_in_context(key=\"\
      identify_core_objective\", value=core_objective)\n    # Step 6: Return typed\
      \ output\n    return IdentifyCoreObjectiveOutput(core_objective=core_objective)\n"
    name: identify_core_objective
    nodes_depended_on: []
    nodes_dependent_on: []
    output_structure:
    - description: The primary objective statement for the workflow as described in
        the requirements, reflecting the need to include a meeting with senior managers
        prior to determining user roles, and noting that constraints should be available
        for use by dependent nodes, such as those identifying success metrics.
      key: core_objective
      type: str
    prd:
      bullets:
      - complexity: LOW - Relies on standardized NLP summarization or rules-based
          extraction.
        impact: HIGH - This objective frames the focus for all downstream workflow
          nodes and aligns stakeholders on purpose.
        method: Employ NLP techniques (e.g., sentence extraction/summarization) or
          a domain-specific rules-based approach to identify and extract the core
          objective from the requirements text.
        reason: A concise and unambiguous articulation of the PRD's core objective
          is foundational and guides all subsequent design, implementation, and validation
          steps.
        text: Parse the provided workflow requirements to extract the singular, definitive
          core objective for the PRD document, distilling it into a single, precise
          sentence.
      - complexity: MEDIUM - Requires detection of business logic in requirement text
          and logical integration with objective extraction.
        impact: HIGH - Sequencing affects workflow logic, and errors here could invalidate
          the PRD's integrity.
        method: Integrate a logic checker or pattern matcher within the NLP pipeline
          to detect explicit requirements for node sequencing and inject this as a
          conditional clause in the core objective.
        reason: Capturing this sequencing reflects an essential business rule and
          ensures downstream processes properly respect the mandated workflow order.
        text: Explicitly incorporate the requirement to add a node for a meeting with
          senior managers that must occur prior to user role determination; ensure
          this sequencing is included as a critical dependency within the objective
          statement.
      - complexity: LOW - Involves tagging or referencing extracted constraints in
          the data object.
        impact: MEDIUM - Ensures system consistency and supports robust, constraint-aware
          downstream processing.
        method: Use metadata tagging or structured output to link/expose the set of
          constraints alongside the core objective in the resulting data structure.
        reason: This ensures traceability and availability of critical boundary conditions
          for later steps like metric definition, feature extraction, and risk analysis.
        text: Ensure that the extracted objective includes a reference to capturing
          all explicit technical or business constraints, which must be available
          to all dependent downstream nodes for further use (e.g., success metrics).
      - complexity: LOW - Leverages existing type-safe data structures and serialization
          methods.
        impact: HIGH - Critical to type-safety and system robustness.
        method: Implement output structure enforcement (e.g., type validation) and
          write to a shared context or object store accessible to all dependent nodes.
        reason: Downstream nodes depend on this output for logic branching, feature
          configuration, and stakeholder communications; schema violations can break
          the DAG.
        text: Design the node output to strictly conform to the output schema (a single
          string with necessary references), and guarantee its availability for dependent
          nodes by storing it in a shared, type-safe structure.
      - complexity: LOW - Involves preparing example artifacts and explanatory comments.
        impact: MEDIUM - Reduces ambiguity and prevents misinterpretation or misimplementation.
        method: Draft inline documentation and provide at least one canonical example
          of the 'core_objective' output (e.g., as a docstring or in an external reference
          file).
        reason: Documentation supports easier onboarding, maintenance, and troubleshooting
          for future developers and workflow users.
        text: Provide clear documentation, with an example output, illustrating how
          the core objective is derived and how sequencing (meeting prior to user
          role assignment) and constraints are referenced.
    prompt: "Read the workflow requirements and extract the singular core objective\
      \ that this PRD document should accomplish. Focus the summary on the main purpose\
      \ in a single sentence. Explicitly state that all extracted constraints must\
      \ be identified for reference by downstream nodes, such as when defining success\
      \ metrics. Ensure your analysis accounts for the inclusion of a node for a meeting\
      \ with senior managers PRIOR to determining user roles\u2014as this step must\
      \ occur first in the workflow\u2014and that this sequencing is reflected as\
      \ integral to the workflow objective."
    shims:
    - agent: false
      code_node_type: virtual-pure
      description: This node analyzes the requirements text to identify and articulate
        the primary workflow objective, integrating explicit sequencing requirements
        and ensuring reference to constraints is made for future workflow steps.
      implementation: "def extract_core_objective_from_requirements(requirements_text:\
        \ str) -> str:\n    \"\"\"\n    This node analyzes the requirements text to\
        \ identify and articulate the primary workflow objective, integrating explicit\
        \ sequencing requirements and ensuring reference to constraints is made for\
        \ future workflow steps.\n\n    Args:\n        requirements_text: Input parameter\
        \ of type str\n\n    Returns:\n        str: Output of type str\n    \"\"\"\
        \n    import re\n\n    def extract_main_objective(text):\n        # Try to\
        \ extract the main goal using summary heuristics\n        # 1. Look for sentences\
        \ containing verbs like \"define\", \"develop\", \"design\", \"implement\"\
        , \"establish\" or similar main action verbs.\n        # 2. Fall back to first\
        \ (longest relevant) sentence if not found.\n        objective_verbs = [\n\
        \            'define', 'develop', 'design', 'implement', 'establish',\n  \
        \          'create', 'build', 'achieve', 'deliver', 'execute', 'launch', 'produce'\n\
        \        ]\n        sentences = re.split(r'[\\.!?\\n]', text)\n        sentences\
        \ = [s.strip() for s in sentences if s.strip()]\n        scored = []\n   \
        \     for s in sentences:\n            score = 0\n            for v in objective_verbs:\n\
        \                if re.search(r'\\b' + re.escape(v) + r'(ing|e[sd]?|s)?\\\
        b', s, re.IGNORECASE):\n                    score += 2\n            # Nouns\
        \ like 'workflow', 'process', 'system', etc, boost score\n            if re.search(r'\\\
        b(workflow|process|system|solution|procedure|framework|plan|objective|goal|outcome)\\\
        b', s, re.IGNORECASE):\n                score += 1\n            scored.append((score,\
        \ -len(s), s))\n        # Select highest scoring sentence\n        scored.sort(reverse=True)\
        \  # score, length (prefer longer), text\n        if scored and scored[0][0]\
        \ > 0:\n            return scored[0][2]\n        elif sentences:\n       \
        \     # Just take the first non-empty, longest candidate\n            return\
        \ max(sentences, key=len)\n        else:\n            return text.strip()\n\
        \n    def extract_sequencing_clauses(text):\n        # Find temporal/dependency\
        \ cues like 'before', 'after', 'prior to', 'once', 'following', etc.\n   \
        \     sequencing_keywords = r\"before|after|prior to|once|following|when|until|upon|preceding|precedes|required\
        \ prior to|must first|only after\"\n        clauses = []\n        sequencing_regex\
        \ = rf'([^.]*\\b(?:{sequencing_keywords})\\b[^.]*)'\n        matches = re.findall(sequencing_regex,\
        \ text, re.IGNORECASE)\n        for m in matches:\n            # Remove leading\
        \ connective words: e.g. 'Before starting,', 'After the meeting,' -> 'the\
        \ meeting'\n            cleaned = re.sub(r'^[A-Z]?[a-z]+\\b\\,?\\s*', '',\
        \ m).strip(',;:. ')\n            if cleaned and cleaned.lower() not in [c.lower()\
        \ for c in clauses]:\n                clauses.append(cleaned)\n        return\
        \ clauses\n\n    def detect_constraints(text):\n        # Look for phrases\
        \ like 'must', 'should', 'cannot', 'may not', 'required to', 'have to', etc.\n\
        \        constraint_markers = [\n            r'\\bmust\\b', r'\\bshould\\\
        b', r'\\bcannot\\b', r'\\bmay not\\b', r'\\brequired to\\b', r'\\bhave to\\\
        b', r'\\bneed to\\b', r'\\bare required to\\b', r'\\bshall\\b', r'\\bprohibited\\\
        b', r'\\bnot allowed\\b', r'\\benforced\\b'\n        ]\n        constraint_regex\
        \ = '|'.join(constraint_markers)\n        return bool(re.search(constraint_regex,\
        \ text, re.IGNORECASE))\n\n    # --- Main logic ---\n    main_objective =\
        \ extract_main_objective(requirements_text)\n    # Ensure main_objective is\
        \ a concise statement.\n    main_objective = main_objective.rstrip('.;')\n\
        \    additions = []\n\n    # Add sequencing clause if present\n    sequencing_clauses\
        \ = extract_sequencing_clauses(requirements_text)\n    if sequencing_clauses:\n\
        \        # Combine into a readable clause, but avoid redundancy\n        clause\
        \ = ', '.join([c for c in sequencing_clauses if c.lower() not in main_objective.lower()])\n\
        \        if clause:\n            # Prepend as dependency phrase\n        \
        \    # e.g., \"Objective statement, after X and before Y, \"\n           \
        \ additions.append(f\"Note: {clause}.\")\n\n    # Add constraint visibility\
        \ if constraints detected\n    if detect_constraints(requirements_text):\n\
        \        additions.append(\"(subject to specified constraints)\")\n\n    #\
        \ Construct final statement\n    result = main_objective\n    # Append additions\
        \ for explicitness and visibility\n    if additions:\n        # Add as footnote\
        \ or append to main clause, separated by '; '\n        result = result + ';\
        \ ' + ' '.join(additions)\n    # Capitalize and return\n    return result.strip()\n"
      name: extract_core_objective_from_requirements
      nodes_depended_on: []
      nodes_dependent_on: []
      output_structure:
      - description: Output of type str
        key: output
        type: str
      - description: Input parameter of type str
        key: requirements_text
        type: str
      prd:
        bullets:
        - complexity: MEDIUM
          impact: Ensures a clear, unambiguous statement of workflow intent, facilitating
            focused and relevant design and planning.
          method: Apply Natural Language Processing (NLP) techniques, such as keyword
            extraction and summarization (e.g., using sentence transformers or LLM
            prompt engineering), to distill the intent from requirement narratives.
          reason: Accurately capturing the workflow's core objective is foundational
            for maintaining alignment in downstream processes.
          text: Parse the requirements text to identify the primary purpose or end-goal
            of the workflow in a single concise statement.
        - complexity: LOW
          impact: Prevents design or execution errors by making process order transparent
            to downstream implementers.
          method: Search for temporal or dependency cues (e.g., 'before', 'prior to',
            'after') in the text and append an appropriate clause to the objective.
          reason: Explicitly including sequencing ensures critical process dependencies
            are respected in workflow design.
          text: Detect and incorporate explicit sequencing instructions, such as prerequisites
            involving meetings or decision points, within the core objective statement.
        - complexity: LOW
          impact: Smooths data flow and context continuity across workflow stages,
            reducing rework and missed dependencies.
          method: Detect mentions of constraints (e.g., 'must', 'cannot', 'should')
            and add a general clause to the objective, such as 'subject to specified
            constraints'.
          reason: Flagging constraints early allows dependent nodes to access and
            leverage them in subsequent workflow steps.
          text: Reference the existence of constraints in the core objective to ensure
            visibility for downstream nodes requiring this information (such as success
            metric identification).
      prompt: Given a detailed requirements text, extract and succinctly state the
        single core objective of the specified workflow, ensuring the statement incorporates
        any explicit sequencing requirements (such as the necessity of a senior management
        meeting prior to user role determination) and recognizes that relevant constraints
        must be made available for use by downstream processes such as success metric
        identification.
      shims: []
    - agent: false
      code_node_type: virtual-pure
      description: Extracts from the input requirements any explicit or implicit statements
        specifying that a meeting with senior managers must occur before the determination
        of user roles, and returns this sequencing clause as a single string.
      implementation: "def extract_sequencing_clause_for_meeting(requirements_text:\
        \ str) -> str:\n    \"\"\"\n    Extracts from the input requirements any explicit\
        \ or implicit statements specifying that a meeting with senior managers must\
        \ occur before the determination of user roles, and returns this sequencing\
        \ clause as a single string.\n\n    Args:\n        requirements_text: Input\
        \ parameter of type str\n\n    Returns:\n        str: Output of type str\n\
        \    \"\"\"\n    import re\n    \n    # Normalize text for easier pattern\
        \ matching\n    text = requirements_text.strip().replace('\\n', ' ')\n   \
        \ \n    # 1. Attempt explicit pattern extraction: search for sentences/clauses\
        \ containing both \"meeting\" with \"senior manager\" and sequencing indicator\
        \ before role determination\n    explicit_patterns = [\n        r\"(meeting\
        \ with senior managers?[^.]*?(before|prior to|precede[sd]?|ahead of|preceding)[^.]*?(determination|assignment|definition|deciding|establishing)[^.]*?(user\
        \ role[s]?|user access|user responsibility|user permission[s]?)[^.]*\\.)\"\
        ,\n        r\"(meet(?:ing)? with (the )?senior managers?[^.]*?(must|should|will|is\
        \ to) (occur|happen|be held) (before|prior to|precede[sd]?|ahead of)[^.]*determin(ing|ation)\
        \ [^.]*user role[s]?[.]*\\.)\",\n        r\"(before[\u02C6.]*user role[s]?\
        \ [^.]*,? [^.]*meeting[s]? with [^.]*senior managers?)\",\n        r\"(must|should|needs\
        \ to|is required to) (hold|have|conduct) (a )?meeting with senior managers?\
        \ (before|prior to|precede[sd]?|ahead of|preceding)[^.]*user role[s]?\",\n\
        \    ]\n    \n    # Try all explicit patterns\n    for patt in explicit_patterns:\n\
        \        match = re.search(patt, text, re.IGNORECASE)\n        if match:\n\
        \            clause = match.group(0).strip()\n            # Ensure clause\
        \ ends with a period\n            if not clause.endswith('.'):\n         \
        \       clause += '.'\n            return clause\n    \n    # 2. If no explicit\
        \ clause found, check for implicit sequencing:\n    # Strategy: Look for mention\
        \ of meeting with senior managers and user roles, and if both present, synthesize\
        \ a statement if there's sequencing language like 'then', 'after', etc.\n\
        \    sentences = re.split(r'(?<=[.!?]) +', text)\n    meeting_sent_idx = -1\n\
        \    role_sent_idx = -1\n    for idx, sent in enumerate(sentences):\n    \
        \    if re.search(r'meet(?:ing)? with (the )?senior managers?|meeting of senior\
        \ managers?|discussion with senior managers?', sent, re.IGNORECASE):\n   \
        \         meeting_sent_idx = idx\n        if re.search(r'(user role[s]?|user\
        \ access|user responsibilit|user permission)', sent, re.IGNORECASE):\n   \
        \         role_sent_idx = idx if role_sent_idx == -1 else role_sent_idx\n\
        \    if meeting_sent_idx != -1 and role_sent_idx != -1:\n        # Synthesize\
        \ only if meeting comes before role mention\n        if meeting_sent_idx <\
        \ role_sent_idx:\n            synthesized = \"A meeting with senior managers\
        \ must take place before determining user roles.\"\n            return synthesized\n\
        \    \n    # 3. Fallback: Search for any temporal/step language connecting\
        \ the two concepts, even across sentences\n    # e.g., 'First, the team meets\
        \ with senior managers. Afterwards, user roles are decided.'\n    temporal_markers\
        \ = [\"first\", \"then\", \"after that\", \"afterwards\", \"next\", \"subsequently\"\
        ]\n    meeting_sentence = None\n    role_sentence = None\n    for sent in\
        \ sentences:\n        if meeting_sentence is None and re.search(r'senior managers?',\
        \ sent, re.IGNORECASE):\n            meeting_sentence = sent.strip()\n   \
        \     if role_sentence is None and re.search(r'user role[s]?|user access|user\
        \ responsibilit|user permission', sent, re.IGNORECASE):\n            role_sentence\
        \ = sent.strip()\n    if meeting_sentence and role_sentence:\n        # Synthesize\
        \ if the order matches\n        if sentences.index(meeting_sentence) < sentences.index(role_sentence):\n\
        \            synthesized = \"A meeting with senior managers must occur before\
        \ the determination of user roles.\"\n            return synthesized\n   \
        \ \n    # If nothing is found\n    return \"\"\n"
      name: extract_sequencing_clause_for_meeting
      nodes_depended_on: []
      nodes_dependent_on: []
      output_structure:
      - description: Output of type str
        key: output
        type: str
      - description: Input parameter of type str
        key: requirements_text
        type: str
      prd:
        bullets:
        - complexity: MEDIUM
          impact: Downstream nodes can reliably depend on this output to accurately
            model required meeting and user role assignment sequencing, preventing
            logic or process errors.
          method: Implement a rule-based parser, leveraging regular expressions or
            dependency parsing (e.g., SpaCy), to find patterns like 'meet with senior
            managers before...,' and extract or summarize matching clauses.
          reason: This ensures that workflow sequencing constraints are captured correctly
            and can be used to enforce process order in downstream workflow logic.
          text: Parse the requirements input and extract any explicit clause or sentence
            that describes the need for a meeting with senior managers before determining
            user roles.
        - complexity: HIGH
          impact: Improves robustness and completeness, reducing manual oversight
            and ambiguity in interpreting requirements.
          method: Utilize NLP techniques such as semantic role labeling and temporal
            relation extraction to infer and reformulate implicit sequencing requirements
            where necessary.
          reason: Implicit requirements are common in natural language specifications
            and must be made explicit for consistent automation.
          text: Synthesize an explicit sequencing statement if no direct sentence
            exists but an implicit sequencing is present in the text.
      prompt: Given the input requirements_text, extract any sentence, clause, or
        clear instruction indicating that a meeting with senior managers must be held
        prior to assignment or determination of user roles, returning the relevant
        extracted sequencing statement verbatim or as a concise synthesized clause.
      shims: []
    - agent: false
      code_node_type: virtual-pure
      description: This node analyzes provided requirements text to extract references
        to technical, business, or process constraints that should be made available
        for downstream workflow design and decision-making.
      implementation: "def extract_constraint_references(requirements_text: str) ->\
        \ str:\n    \"\"\"\n    This node analyzes provided requirements text to extract\
        \ references to technical, business, or process constraints that should be\
        \ made available for downstream workflow design and decision-making.\n\n \
        \   Args:\n        requirements_text: Input parameter of type str\n\n    Returns:\n\
        \        str: Output of type str\n    \"\"\"\n    import re\n    import json\n\
        \n    # Keywords for different constraint categories\n    tech_keywords =\
        \ [\n        'API', 'integration', 'system', 'platform', 'database', 'technology',\
        \ 'architecture', 'compatib', 'deploy',\n        'infrastructure', 'cloud',\
        \ 'tech', 'software', 'hardware', 'protocol', 'service', 'framework', 'environment',\
        \ 'tool'\n    ]\n    biz_keywords = [\n        'budget', 'cost', 'revenue',\
        \ 'profit', 'quota', 'pricing', 'ROI', 'business', 'stakeholder', 'approval',\n\
        \        'market', 'customer', 'sales', 'commercial', 'financial', 'grant',\
        \ 'fund', 'expense', 'payment', 'invoicing', 'forecast'\n    ]\n    timeline_keywords\
        \ = [\n        'deadline', 'timeline', 'delivery', 'milestone', 'phase', 'timeframe',\
        \ 'due', 'launch', 'sprint', 'duration', 'completion',\n        'date', 'asap',\
        \ 'by', 'before', 'after', 'within', 'no later than', 'quarter', 'month',\
        \ 'week', 'day'\n    ]\n    resource_keywords = [\n        'resource', 'staff',\
        \ 'capacity', 'personnel', 'manpower', 'hours', 'team', 'developer', 'contractor',\n\
        \        'allocation', 'availability', 'effort', 'headcount', 'skill', 'expertise',\
        \ 'equipment', 'tool', 'machine'\n    ]\n    regulatory_keywords = [\n   \
        \     'compliance', 'legal', 'regulation', 'licensed', 'law', 'standard',\
        \ 'GDPR', 'HIPAA', 'PCI',\n        'security', 'privacy', 'audit', 'traceability',\
        \ 'risk', 'policy', 'certificate', 'accreditation', 'requirement', 'restriction',\
        \ 'governance'\n    ]\n\n    # Helper to flatten and lowercase keyword lists\n\
        \    def keyword_finder_factory(keywords):\n        return lambda s: any(k.lower()\
        \ in s.lower() for k in keywords)\n\n    category_map = [\n        ('technology',\
        \ keyword_finder_factory(tech_keywords)),\n        ('business', keyword_finder_factory(biz_keywords)),\n\
        \        ('timeline', keyword_finder_factory(timeline_keywords)),\n      \
        \  ('resource', keyword_finder_factory(resource_keywords)),\n        ('regulatory',\
        \ keyword_finder_factory(regulatory_keywords)),\n    ]\n\n    # Sentence splitter\
        \ (basic)\n    sentences = re.split(r'(?<=[.!?])\\s+(?=[A-Z])', requirements_text.strip())\n\
        \    sentences = [s.strip() for s in sentences if s.strip()]\n\n    extracted\
        \ = []\n    seen_statements = set()\n    for sent in sentences:\n        #\
        \ Check if the sentence contains any constraint indicator\n        # Search\
        \ for: (1) explicit, (2) implied constraints\n        lowered = sent.lower()\n\
        \        found_category = None\n        for cat, finder in category_map:\n\
        \            if finder(sent):\n                found_category = cat\n    \
        \            break\n        # Look for generic constraint phrases\n      \
        \  constraint_phrases = [\n            'must', 'should', 'required', 'shall',\
        \ 'is subject to', 'need to', 'needs to', 'have to',\n            'no later\
        \ than', 'cannot', 'not allowed', 'forbidden', 'prohibited'\n        ]\n \
        \       is_constraint = any(phrase in lowered for phrase in constraint_phrases)\n\
        \        # Also, if the category is regulatory, treat any mention of regulatory_keywords\
        \ as a constraint\n        if found_category or is_constraint:\n         \
        \   norm = sent.strip()\n            # Simple deduplication\n            norm_key\
        \ = re.sub(r'\\s+', ' ', norm.lower())\n            if norm_key not in seen_statements:\n\
        \                item = {\n                    'constraint': norm,\n     \
        \               'category': found_category if found_category else 'unspecified'\n\
        \                }\n                extracted.append(item)\n             \
        \   seen_statements.add(norm_key)\n    # Additional pass: extract implicit\
        \ enumerated constraints in list items (bullets)\n    # E.g., \"The system\
        \ must: (a) support SSO; (b) restrict access by region; (c) log all activity.\"\
        \n    for sent in sentences:\n        # Match patterns like a) ..., b) ...,\
        \ or (a)... (b)...\n        bullet_fragments = re.findall(r'\\([a-zA-Z0-9]\\\
        )\\s*([A-Z0-9][^.;:!\\n]+)', sent)\n        if bullet_fragments:\n       \
        \     for _, frag in bullet_fragments:\n                # Reapply category\
        \ and constraint checks to fragment\n                found_category = None\n\
        \                for cat, finder in category_map:\n                    if\
        \ finder(frag):\n                        found_category = cat\n          \
        \              break\n                lowered_frag = frag.lower()\n      \
        \          is_constraint = any(phrase in lowered_frag for phrase in constraint_phrases)\n\
        \                if found_category or is_constraint:\n                   \
        \ norm = frag.strip()\n                    norm_key = re.sub(r'\\s+', ' ',\
        \ norm.lower())\n                    if norm_key not in seen_statements:\n\
        \                        item = {\n                            'constraint':\
        \ norm,\n                            'category': found_category if found_category\
        \ else 'unspecified'\n                        }\n                        extracted.append(item)\n\
        \                        seen_statements.add(norm_key)\n    # Final normalization:\
        \ sort, ensure concise output\n    output = []\n    for c in extracted:\n\
        \        s = c['constraint'].strip()\n        # Remove trailing punctuation\
        \ for programmatic clarity\n        if s and s[-1] in '.;:':\n           \
        \ s = s[:-1].strip()\n        # Output as clear, concise statement\n     \
        \   entry = {'category': c['category'], 'statement': s}\n        output.append(entry)\n\
        \    # Structure as JSON string for downstream consumption\n    return json.dumps(output,\
        \ ensure_ascii=False, indent=2)\n"
      name: extract_constraint_references
      nodes_depended_on: []
      nodes_dependent_on: []
      output_structure:
      - description: Output of type str
        key: output
        type: str
      - description: Input parameter of type str
        key: requirements_text
        type: str
      prd:
        bullets:
        - complexity: MEDIUM
          impact: Provides all constraint information in one place, reducing the risk
            of missed dependencies or ignored requirements during workflow synthesis.
          method: Employ both rule-based keyword searches (for known constraint terms)
            and contextual NLP techniques (e.g., entity/relation recognition or transformer-based
            models) to identify and summarize constraints from the input text.
          reason: Comprehensive extraction ensures that downstream workflow nodes
            have full context on all operating limitations, which is essential for
            robust workflow design and accurate metric identification.
          text: Parse the requirements text to detect and extract both explicit and
            implicit references to constraints pertaining to technology, business
            rules, timelines, resources, and regulatory compliance.
        - complexity: LOW
          impact: Enables automated, reliable transfer of constraint information between
            workflow stages.
          method: Apply string post-processing, deduplication, and formatting routines
            to ensure output consistency and usefulness.
          reason: Normalized constraint statements facilitate unambiguous hand-off
            to downstream processing and minimize manual interpretation.
          text: Normalize and structure the extracted constraints into clear, concise
            statements suitable for programmatic use and downstream consumption.
      prompt: 'Given the following requirements text, identify and extract all explicit
        and implicit references to technical, business, or process constraints (e.g.,
        budget limitations, timeline deadlines, system restrictions, compliance requirements,
        resource availabilities, etc.), phrasing them in concise and contextually
        accurate statements suitable for input to downstream nodes:'
      shims: []
    - agent: false
      code_node_type: virtual-pure
      description: Synthesizes a final, clear, and comprehensive core objective statement
        by integrating the base objective, required sequencing (such as mandated meeting
        order), and relevant constraints as described in workflow requirements.
      implementation: "def compose_final_objective_statement(base_objective: str,\
        \ sequencing: str, constraints: str) -> str:\n    \"\"\"\n    Synthesizes\
        \ a final, clear, and comprehensive core objective statement by integrating\
        \ the base objective, required sequencing (such as mandated meeting order),\
        \ and relevant constraints as described in workflow requirements.\n\n    Args:\n\
        \        base_objective: Input parameter of type str\nsequencing: Input parameter\
        \ of type str\nconstraints: Input parameter of type str\n\n    Returns:\n\
        \        str: Output of type str\n    \"\"\"\n    # 1. Trim all inputs and\
        \ check for non-empty elements\n    base = base_objective.strip() if base_objective\
        \ else ''\n    seq = sequencing.strip() if sequencing else ''\n    cons =\
        \ constraints.strip() if constraints else ''\n\n    # 2. Validate presence\
        \ of all non-empty elements\n    missing = []\n    if not base:\n        missing.append('base_objective')\n\
        \    if not seq:\n        missing.append('sequencing')\n    if not cons:\n\
        \        missing.append('constraints')\n    if missing:\n        raise ValueError(f\"\
        Missing required statement fragments: {', '.join(missing)}\")\n\n    # 3.\
        \ Compose following a logical order:\n    #    (a) State sequencing/temporal\
        \ order first (if non-empty),\n    #    (b) then the base objective (the main\
        \ action),\n    #    (c) then constraints after (framed as conditions or caveats).\n\
        \    # Compose sentences so it's fluent as English.\n    import re\n\n   \
        \ # Capitalization and punctuation helpers\n    def _ensure_ends_with_period(text):\n\
        \        text = text.rstrip()\n        return text if text.endswith('.') else\
        \ text + '.'\n\n    def _lower_first(text):\n        if not text: return text\n\
        \        return text[0].lower() + text[1:]\n\n    # Compose parts\n    # 1.\
        \ Sequencing: \"First, ...\" or \"Before X, ...\"\n    sequencing_phrase =\
        \ ''\n    if seq:\n        seq_clean = seq.rstrip('.')\n        # If sequencing\
        \ already starts with a temporal keyword, just capitalize\n        if re.match(r\"\
        ^(first|before|after|then|when|once|prior to|following|upon)\\b\", seq_clean,\
        \ re.IGNORECASE):\n            sequencing_phrase = seq_clean[0].upper() +\
        \ seq_clean[1:]\n        else:\n            sequencing_phrase = f\"First,\
        \ {seq_clean}\"   # Fallback\n        sequencing_phrase = _ensure_ends_with_period(sequencing_phrase)\n\
        \n    # 2. Base Objective\n    base_phrase = base.rstrip('.')\n    # Start\
        \ base objective sentence with uppercase\n    base_phrase = base_phrase[0].upper()\
        \ + base_phrase[1:]\n    base_phrase = _ensure_ends_with_period(base_phrase)\n\
        \n    # 3. Constraints\n    constraints_phrase = ''\n    if cons:\n      \
        \  cons_clean = cons.rstrip('.')\n        # Try to start as a dependent clause,\
        \ e.g. \"Ensure ...\", \"Making sure ...\", or \"while ...\"\n        # If\
        \ it starts with 'must', 'ensure', 'only if', etc., keep as is\n        if\
        \ re.match(r\"^(must|ensure|making|only if|unless|while|provided that|so that)\\\
        b\", cons_clean, re.IGNORECASE):\n            constraints_phrase = cons_clean[0].upper()\
        \ + cons_clean[1:]\n        else:\n            constraints_phrase = f\"Ensure\
        \ {cons_clean}\" if not cons_clean.lower().startswith('ensure') else cons_clean\n\
        \        constraints_phrase = _ensure_ends_with_period(constraints_phrase)\n\
        \n    # 4. Combine logically: [Sequencing.] [Base Objective.] [Constraints.]\n\
        \    # If sequencing is non-empty, prepend; always include base; append constraints\
        \ if any.\n    parts = []\n    if sequencing_phrase:\n        parts.append(sequencing_phrase)\n\
        \    parts.append(base_phrase)\n    if constraints_phrase:\n        parts.append(constraints_phrase)\n\
        \n    statement = ' '.join(parts)\n\n    # 5. Post-validation: ensure all\
        \ key fragments appear at least approximately in the statement\n    # (Case-insensitive\
        \ containment test)\n    for frag, label in [(base, 'base_objective'), (seq,\
        \ 'sequencing'), (cons, 'constraints')]:\n        if frag and frag.lower()\
        \ not in statement.lower():\n            raise ValueError(f\"Fragment from\
        \ {label} missing in composed statement.\")\n\n    return statement\n"
      name: compose_final_objective_statement
      nodes_depended_on: []
      nodes_dependent_on: []
      output_structure:
      - description: Output of type str
        key: output
        type: str
      - description: Input parameter of type str
        key: base_objective
        type: str
      - description: Input parameter of type str
        key: sequencing
        type: str
      - description: Input parameter of type str
        key: constraints
        type: str
      prd:
        bullets:
        - complexity: MEDIUM
          impact: Downstream nodes and humans will have a clear, holistic, and actionable
            summary statement reflecting all necessary conditions.
          method: Use string templating or a rules-based composition method to assemble
            and rephrase the input fragments into one or two clear, fluent sentences.
          reason: Combining these fragments ensures all workflow requirements are
            explicitly captured and communicated in the most actionable way.
          text: Integrate the base objective, required sequencing, and constraints
            into a single, grammatically correct statement.
        - complexity: LOW
          impact: Reduces risk of misinterpretation or overlooked requirements in
            workflow generation or automation.
          method: Apply simple ordering heuristics or hard-coded templates to position
            sequencing, the main objective, and constraints appropriately.
          reason: Maintaining correct logical/temporal order improves comprehension,
            prevents ambiguity, and ensures the process can be correctly followed.
          text: Ensure logical ordering and clarity within the composed statement,
            e.g., sequencing before the action, constraints after.
        - complexity: LOW
          impact: Improves reliability for downstream tasks that depend on the core
            objective statement.
          method: After composition, check that non-empty values from base_objective,
            sequencing, and constraints appear within the statement; error or warn
            if any are missing.
          reason: Guarantees completeness so that no critical requirement or implicit
            dependency is omitted in the objective.
          text: Validate that all key elements are present and correctly referenced
            within the generated statement.
      prompt: 'Given a base objective statement, a sequencing requirement (such as
        "a meeting with senior managers must occur prior to determining user roles"),
        and a string summarizing relevant technical or business constraints, compose
        a single, clear, and comprehensive final objective statement that integrates
        all three elements into a logically ordered sentence or two.


        Output the final, human-readable objective statement as a string.'
      shims: []
    - agent: false
      code_node_type: virtual-pure
      description: This node validates that the given core objective output string
        conforms to the required schema and type specifications before passing it
        downstream in the workflow.
      implementation: "def validate_core_objective_output_schema(core_objective: str)\
        \ -> str:\n    \"\"\"\n    This node validates that the given core objective\
        \ output string conforms to the required schema and type specifications before\
        \ passing it downstream in the workflow.\n\n    Args:\n        core_objective:\
        \ Input parameter of type str\n\n    Returns:\n        str: Output of type\
        \ Any\n    \"\"\"\n    # --- PURE IMPLEMENTATION ---\n    if not isinstance(core_objective,\
        \ str):\n        raise TypeError(f\"core_objective must be a string, got {type(core_objective).__name__}\"\
        )\n    if not core_objective or not core_objective.strip():\n        raise\
        \ ValueError(\"core_objective must be a non-empty string.\")\n    # Optionally,\
        \ add further string validation/per schema constraints here if required in\
        \ future\n    return core_objective\n"
      name: validate_core_objective_output_schema
      nodes_depended_on: []
      nodes_dependent_on: []
      output_structure:
      - description: Output of type Any
        key: output
        type: str
      - description: Input parameter of type str
        key: core_objective
        type: str
      prd:
        bullets:
        - complexity: LOW
          impact: Prevents type or schema violations that could cause system failures
            or incorrect workflow behavior.
          method: Use type checking (isinstance) and/or string validation logic to
            confirm the core objective is a valid, non-empty string, and raise a descriptive
            exception if it is not.
          reason: Ensures downstream nodes receive correctly typed and meaningful
            data, preventing processing errors and ambiguity.
          text: Validate that the core objective is a non-empty string conforming
            to requirements.
        - complexity: LOW
          impact: Improves system robustness and debuggability by catching issues
            immediately rather than at later stages.
          method: Apply input sanitization and, if needed, leverage schema libraries
            (like Pydantic or custom logic) to verify adherence to expected output
            structure.
          reason: Early validation reduces downstream bug propagation and maintains
            data integrity across the workflow.
          text: Enforce schema constraints strictly before data is shared downstream.
      prompt: Given a core objective string, ensure that it matches the required output
        schema (i.e., is a non-empty string) and raises an appropriate error if validation
        fails.
      shims: []
    - agent: false
      code_node_type: virtual-pure
      description: This node stores the output of another node in a shared context
        using a specified key so that it is accessible for downstream nodes.
      implementation: "def store_node_output_in_context(key: str, value: str) -> str:\n\
        \    \"\"\"\n    This node stores the output of another node in a shared context\
        \ using a specified key so that it is accessible for downstream nodes.\n\n\
        \    Args:\n        key: Input parameter of type str\nvalue: Input parameter\
        \ of type str\n\n    Returns:\n        str: Output of type Any\n    \"\"\"\
        \n    # --- PURE IMPLEMENTATION ---\n    # Shared context will be a module-level\
        \ dictionary\n    # We use a special attribute on the function to simulate\
        \ singleton/session-level storage\n    import threading\n    import json\n\
        \n    # Use attached attribute on the function for thread-safe global context\n\
        \    if not hasattr(store_node_output_in_context, \"_context\"):\n       \
        \ store_node_output_in_context._context = {}\n        store_node_output_in_context._lock\
        \ = threading.Lock()\n\n    # Cast value to JSON-serializable string for type\
        \ consistency/serializability\n    try:\n        serialized_value = json.dumps(value)\n\
        \    except TypeError:\n        # If not serializable, fall back to str\n\
        \        serialized_value = str(value)\n\n    # Store the value with thread\
        \ safety\n    with store_node_output_in_context._lock:\n        store_node_output_in_context._context[key]\
        \ = serialized_value\n\n    return serialized_value\n"
      name: store_node_output_in_context
      nodes_depended_on: []
      nodes_dependent_on: []
      output_structure:
      - description: Output of type Any
        key: output
        type: str
      - description: Input parameter of type str
        key: key
        type: str
      - description: Input parameter of type str
        key: value
        type: str
      prd:
        bullets:
        - complexity: LOW
          impact: Facilitates proper data propagation between workflow nodes, making
            complex multi-step processes possible.
          method: Use a singleton or context manager (e.g., a Python dictionary at
            module or session level) to hold and update key-value pairs.
          reason: This enables outputs from one node to be made available to any downstream
            nodes that depend on that information, supporting data dependencies and
            modular composition.
          text: Implement a mechanism to store key-value pairs in a shared or global
            context object accessible throughout the workflow lifecycle.
        - complexity: LOW
          impact: Reduces runtime errors and data corruption, thus improving workflow
            robustness.
          method: Cast or serialize values to strings (or JSON) before storing and
            provide easy retrieval and deserialization when accessed by other nodes.
          reason: Standardizing types helps prevent serialization/deserialization
            issues and guarantees expected downstream behavior.
          text: Ensure type consistency and serializability for stored values.
      prompt: Store the provided value in the shared or global context with the given
        key so this output can be retrieved by subsequent nodes as input.
      shims: []
  identify_major_risks:
    agent: false
    code_node_type: pure
    description: List significant risks or uncertainties associated with the workflow,
      explicitly considering constraints as potential sources of risk or uncertainty.
      Before determining user roles in the workflow, plan and conduct a meeting with
      senior managers to review, validate, and discuss the risks, ensuring executive
      visibility and input at an early stage.
    implementation: "from pydantic import BaseModel, Field\nfrom typing import List\n\
      \n\nclass IdentifyMajorRisksOutput(BaseModel):\n    \"\"\"Pydantic model for\
      \ identify_major_risks node outputs.\"\"\"\n    major_risks: List[str] = Field(...,\
      \ description=\"Significant risks or uncertainties for delivery or adoption\
      \ of the workflow, informed by and linked to any identified constraints.\")\n\
      \    senior_manager_meeting_steps: List[str] = Field(..., description=\"Actionable\
      \ steps for organizing and conducting a meeting with senior managers to review\
      \ and validate identified risks prior to determining user roles.\")\n\n\ndef\
      \ identify_major_risks_fx(general_input: str, **kwargs) -> IdentifyMajorRisksOutput:\n\
      \    \"\"\"List significant risks or uncertainties associated with the workflow,\
      \ explicitly considering constraints as potential sources of risk or uncertainty.\
      \ Before determining user roles in the workflow, plan and conduct a meeting\
      \ with senior managers to review, validate, and discuss the risks, ensuring\
      \ executive visibility and input at an early stage.\n\n    Args:\n        general_input:\
      \ General input string for the root node.\n        **kwargs: Additional keyword\
      \ arguments.\n\n    Returns:\n        IdentifyMajorRisksOutput: Object containing\
      \ outputs for this node.\n    \"\"\"\n    # Step 1: Collate all constraints\
      \ previously extracted by upstream nodes\n    constraints: List[str] = extract_constraints_from_inputs(inputs=kwargs)\n\
      \n    # Step 2: Identify and analyze significant risks, linking them to constraints\n\
      \    raw_risks: List[dict] = assess_risks_with_constraints(\n        workflow_description=general_input,\n\
      \        constraints=constraints,\n        framework=\"FMEA|risk_matrix\"\n\
      \    )\n\n    # Step 3: Document each risk and associate rationale and constraint\
      \ references\n    documented_risks: List[dict] = document_risks_with_rationale(\n\
      \        risks=raw_risks,\n        constraints=constraints\n    )\n\n    # Step\
      \ 4: Tag each risk as actionable or non-actionable\n    tagged_risks: List[dict]\
      \ = tag_risks_actionable(risks=documented_risks)\n\n    # Step 5: Filter out\
      \ only actionable or critical risks for output\n    filtered_major_risks: List[dict]\
      \ = filter_major_risks(\n        risks=tagged_risks,\n        filter_mode=\"\
      actionable_or_critical\"\n    )\n\n    # Step 6: Generate formatted output strings\
      \ for each major risk\n    major_risk_strings: List[str] = format_major_risks(risks=filtered_major_risks)\n\
      \n    # Step 7: Create and schedule a senior manager meeting before roles assignment\n\
      \    meeting_steps: List[str] = generate_senior_manager_meeting_steps(\n   \
      \     risks=filtered_major_risks,\n        constraints=constraints,\n      \
      \  agenda_notes=True,\n        calendar_integration=True,\n        notification_queue=True\n\
      \    )\n\n    # Step 8: Embed workflow checks to enforce meeting occurs before\
      \ user role assignment\n    enforce_meeting_before_roles(\n        meeting_steps=meeting_steps,\n\
      \        workflow_state=kwargs.get(\"workflow_state\"),\n        gating_enabled=True\n\
      \    )\n\n    # Step 9: Maintain audit trail for risk documentation and decisions\n\
      \    log_risk_audit_trail(\n        risks=filtered_major_risks,\n        constraints=constraints,\n\
      \        action=\"initial_risk_identification_meeting\"\n    )\n\n    return\
      \ IdentifyMajorRisksOutput(\n        major_risks=major_risk_strings,\n     \
      \   senior_manager_meeting_steps=meeting_steps\n    )\n"
    name: identify_major_risks
    nodes_depended_on: []
    nodes_dependent_on: []
    output_structure:
    - description: Significant risks or uncertainties for delivery or adoption of
        the workflow, informed by and linked to any identified constraints.
      key: major_risks
      type: List[str]
    - description: Actionable steps for organizing and conducting a meeting with senior
        managers to review and validate identified risks prior to determining user
        roles.
      key: senior_manager_meeting_steps
      type: List[str]
    prd:
      bullets:
      - complexity: MEDIUM - Requires cross-referencing multiple inputs but follows
          standard risk management practices.
        impact: HIGH - Guarantees a holistic risk view, fostering informed decision-making
          for all stakeholders and impacting downstream workflow robustness.
        method: Collate all extracted constraints; apply a risk assessment framework
          (e.g., FMEA, risk matrices); generate a structured list of risks mapping
          each to relevant constraints.
        reason: Explicitly linking risks to constraints ensures that all limitation-based
          risks are considered and that dependencies from previous analysis are used
          for comprehensive risk identification.
        text: Develop a structured process to identify, analyze, and document significant
          workflow risks and uncertainties, with explicit linkage to any previously
          extracted constraints.
      - complexity: LOW - Filtering/categorization can be automated or handled via
          tagging in the risk documentation tool.
        impact: MEDIUM - Results in targeted and actionable risk management, improving
          the focus for both the workflow and leadership review.
        method: Tag or sort risks as 'actionable' or 'non-actionable'; use filters
          in the output pipeline to include only actionable or critical risks.
        reason: Focusing on actionable and noteworthy risks ensures clarity and relevance
          in the PRD, preventing unnecessary escalation of minor or uncontrollable
          risks.
        text: Implement a mechanism for clearly differentiating risks that are actionable
          within the project from those that are external or non-actionable, and filter
          output accordingly.
      - complexity: MEDIUM - Involves calendar integration, automated agenda creation,
          and notification logic.
        impact: HIGH - Aligns risk management with leadership priorities at the outset,
          directly influencing subsequent workflow configuration.
        method: Integrate with enterprise calendaring (e.g., Outlook, Google Calendar);
          auto-create agenda items from the risk registry; queue notifications to
          senior managers.
        reason: This early touchpoint ensures that executive leadership is aware of
          potential pitfalls and can influence mitigation or role-assignment based
          on risk profile.
        text: Automatically schedule and generate preparatory materials for a meeting
          with senior managers to review, validate, and discuss all identified major
          risks before any user roles are defined.
      - complexity: LOW - Adheres to the output schema; primarily involves formatting
          output lists with relevant metadata.
        impact: MEDIUM - Enhances maintainability and traceability for both risks
          and meeting organization.
        method: Generate the risks and meeting steps as distinct lists; validate output
          types; include references to originating constraints in risk entries.
        reason: Type-safe, well-structured outputs enable downstream nodes to consume
          and act on risk data without re-processing, supporting end-to-end workflow
          integrity.
        text: Design a clearly structured output listing for 'major_risks' and 'senior_manager_meeting_steps',
          ensuring each is actionable and presented in the correct workflow sequence.
      - complexity: MEDIUM - Requires state tracking or step gating in the overall
          workflow engine.
        impact: HIGH - Ensures compliance with workflow requirements, reduces risk
          of misalignment or unapproved role allocation.
        method: Implement workflow control logic (e.g., dependency checking, gating
          functions) to block user-role assignment-related processes until the senior
          manager meeting node is marked complete.
        reason: This enforces the required workflow sequencing, preventing accidental
          bypass of critical risk validation before responsibility assignment.
        text: Include built-in checks to ensure the senior manager risk validation
          meeting occurs before any process or node related to determining user roles.
      - complexity: LOW - Involves structured annotation and metadata fields on each
          risk entry.
        impact: MEDIUM - Improves auditability and transferable knowledge for future
          workflows.
        method: Include detailed notes and citation of corresponding constraints for
          each risk in the structured output; maintain audit logs for all risk modifications.
        reason: Well-documented risks and their origins enable downstream users and
          auditors to understand risk context and support iterative improvement.
        text: Provide clear documentation and rationale for each identified risk and
          its associated constraint(s), facilitating transparency and traceability
          for later audits and reviews.
    prompt: Identify and list any major risks, uncertainties, or dependencies that
      could affect the successful delivery of this workflow. Focus only on those that
      are actionable or noteworthy for a PRD. In your assessment, ensure that you
      reference or consider any constraints previously identified or extracted, as
      risks often arise from limitations or boundaries set by those constraints. As
      a required preliminary activity, add a meeting with senior managers to review
      and validate the identified risks, ensuring that leadership is aware and has
      the opportunity to provide input or direction regarding these risks. This meeting
      should be conducted PRIOR to determining user roles in the workflow, to guarantee
      that any risk-related guidance can inform decisions on user responsibilities
      and workflow structure.
    shims:
    - agent: false
      code_node_type: virtual-pure
      description: Extracts a list of constraints (as strings) from the provided input
        string, typically aggregating any constraints identified in prior workflow
        steps.
      implementation: "def extract_constraints_from_inputs(inputs: str) -> List[str]:\n\
        \    \"\"\"\n    Extracts a list of constraints (as strings) from the provided\
        \ input string, typically aggregating any constraints identified in prior\
        \ workflow steps.\n\n    Args:\n        inputs: Input parameter of type str\n\
        \n    Returns:\n        List[str]: Output of type List[str]\n    \"\"\"\n\
        \    import re\n    \n    # Define a set of indicative keywords/phrases for\
        \ constraints (expand as needed)\n    constraint_keywords = [\n        r\"\
        must not\",\n        r\"must\",\n        r\"should not\",\n        r\"should\"\
        ,\n        r\"cannot\",\n        r\"can not\",\n        r\"may not\",\n  \
        \      r\"is required to\",\n        r\"is not permitted to\",\n        r\"\
        cannot exceed\",\n        r\"no more than\",\n        r\"at least\",\n   \
        \     r\"less than\",\n        r\"greater than\",\n        r\"between\",\n\
        \        r\"limited to\",\n        r\"restricted to\",\n        r\"prohibited\"\
        ,\n        r\"forbidden\",\n        r\"avoid\",\n        r\"exclusive\",\n\
        \        r\"mandatory\",\n        r\"necessary\"\n    ]\n\n    # Lowercase\
        \ for case-insensitive pattern matching\n    lowered = inputs.lower()\n\n\
        \    # Build a regex pattern to match sentences or clauses containing constraint\
        \ keywords\n    # Pattern will split the input into sentences, then filter\
        \ sentences containing constraint keywords\n    sentence_pattern = re.compile(r'[^.!?]+[.!?]?')\n\
        \    sentences = [s.strip() for s in sentence_pattern.findall(inputs) if s.strip()]\n\
        \n    extracted_constraints = []\n    for sentence in sentences:\n       \
        \ sentence_lc = sentence.lower()\n        for kw in constraint_keywords:\n\
        \            if re.search(r'\\b' + kw + r'\\b', sentence_lc):\n          \
        \      extracted_constraints.append(sentence.strip())\n                break\
        \  # Only need to include a sentence once\n\n    # If unconventionally phrased\
        \ constraints exist, try extracting clauses (semicolon/comma separated) as\
        \ fallback\n    if not extracted_constraints:\n        clauses = [cl.strip()\
        \ for cl in re.split(r'[;\\n]', inputs) if cl.strip()]\n        for clause\
        \ in clauses:\n            clause_lc = clause.lower()\n            for kw\
        \ in constraint_keywords:\n                if re.search(r'\\b' + kw + r'\\\
        b', clause_lc):\n                    extracted_constraints.append(clause.strip())\n\
        \                    break\n\n    # Fallback: if still nothing, try matching\
        \ any segment with \"must\", \"should\", \"not allowed\" etc.\n    if not\
        \ extracted_constraints:\n        fallback_keywords = [r\"must\", r\"should\"\
        , r\"not allowed\", r\"cannot\"]\n        for kw in fallback_keywords:\n \
        \           matches = re.findall(rf'([^.!?]*?{kw}[^.!?]*[.!?])', lowered)\n\
        \            for m in matches:\n                extracted_constraints.append(m.strip())\n\
        \n    # Standardize: lowercase, strip, deduplicate\n    standardized = set()\n\
        \    for c in extracted_constraints:\n        norm_c = ' '.join(c.lower().split()).strip()\n\
        \        if norm_c:\n            standardized.add(norm_c)\n\n    # Return\
        \ sorted for consistency\n    return sorted(standardized)\n"
      name: extract_constraints_from_inputs
      nodes_depended_on: []
      nodes_dependent_on: []
      output_structure:
      - description: Output of type List[str]
        key: output
        type: List[str]
      - description: Input parameter of type str
        key: inputs
        type: str
      prd:
        bullets:
        - complexity: MEDIUM
          impact: Enables consistent and accurate identification of constraints, ensuring
            traceability and completeness for subsequent risk analysis.
          method: Use regular expressions, keyword matching, or natural language processing
            techniques to scan the string for constraint-like phrases and terms.
          reason: Constraints originating from previous workflow steps need to be
            collected for risk assessment and downstream processing.
          text: Parse the input string to identify and extract all constraints that
            are explicitly mentioned or can be reasonably inferred.
        - complexity: LOW
          impact: Improves system interpretability and facilitates linking risks or
            decisions to specific constraints in an auditable way.
          method: Apply string normalization steps (e.g., lowercasing, stripping whitespace),
            and use set operations to deduplicate the resulting list.
          reason: To avoid redundancy and guarantee uniformity in how constraints
            are represented and referenced across the workflow.
          text: Standardize extracted constraint phrases for format and duplication.
      prompt: Given an input string that may contain references to constraints from
        earlier workflow steps, extract and return all explicitly or implicitly stated
        constraints as a list of strings.
      shims: []
    - agent: false
      code_node_type: virtual-pure
      description: Analyzes a given workflow description and its constraints to systematically
        identify and assess significant risks using a specified risk analysis framework.
      implementation: "def assess_risks_with_constraints(workflow_description: str,\
        \ constraints: str, framework: str) -> List[str]:\n    \"\"\"\n    Analyzes\
        \ a given workflow description and its constraints to systematically identify\
        \ and assess significant risks using a specified risk analysis framework.\n\
        \n    Args:\n        workflow_description: Input parameter of type str\nconstraints:\
        \ Input parameter of type str\nframework: Input parameter of type str\n\n\
        \    Returns:\n        List[str]: Output of type List[dict]\n    \"\"\"\n\
        \    # --- PURE IMPLEMENTATION ---\n    import re\n    import json\n    from\
        \ collections import defaultdict\n    \n    def extract_constraint_phrases(text):\n\
        \        # Extracts constraint-like phrases, e.g. lines with requirements,\
        \ forbidden actions, limits, etc.\n        patterns = [\n            r\"must\
        \ not [^.]+\", r\"shall not [^.]+\", r\"should not [^.]+\", r\"must [^.]+\"\
        , r\"should [^.]+\", r\"cannot [^.]+\", r\"can only [^.]+\", r\"no more than\
        \ [^.]+\", r\"at least [^.]+\", r\"at most [^.]+\", r\"required to [^.]+\"\
        , r\"prohibited to [^.]+\", r\"forbidden to [^.]+\",\n        ]\n        phrases\
        \ = []\n        for p in patterns:\n            phrases.extend(re.findall(p,\
        \ text, re.IGNORECASE))\n        # Fallback: lines containing certain keywords\n\
        \        fallback_keywords = [\"constraint\", \"must\", \"should\", \"require\"\
        , \"forbidden\", \"limit\", \"cannot\", \"restricted\", \"prohibit\", \"only\"\
        ]\n        for line in text.splitlines():\n            if any(k in line.lower()\
        \ for k in fallback_keywords):\n                phrases.append(line.strip())\n\
        \        # Deduplicate & clean\n        cleaned = list({ph.strip(\". ;\")\
        \ for ph in phrases if ph.strip()})\n        return cleaned\n    \n    def\
        \ extract_workflow_steps(text):\n        # Try to split workflow into logical\
        \ steps by numbered/bulleted lists, or sentences describing actions\n    \
        \    steps = []\n        lines = [ln.strip() for ln in text.splitlines() if\
        \ ln.strip()]\n        for line in lines:\n            m = re.match(r\"\\\
        d+\\. ?(.*)\", line)\n            if m:\n                steps.append(m.group(1))\n\
        \            elif re.match(r\"[-*+] ?\", line):\n                steps.append(line[2:].strip())\n\
        \            else:\n                # If imperative verb at start, treat as\
        \ possible step\n                if re.match(r\"(?:add|remove|process|submit|send|review|approve|check|perform|ensure|analyze|evaluate|identify|assess|audit|select|assign|track|validate|calculate|verify)\"\
        , line, re.IGNORECASE):\n                    steps.append(line)\n        #\
        \ Fallback: treat each 'sentence' as a step\n        if not steps:\n     \
        \       steps = re.split(r\"(?<=[.?!])\\s+\", text)\n            steps = [st.strip()\
        \ for st in steps if len(st.strip().split()) > 2]\n        return steps\n\
        \    \n    def extract_risks(workflow, constraints):\n        # We'll perform\
        \ a simple NLP-like scan: identify negative, uncertainty, conditional, or\
        \ conflict words near constraints or steps\n        risk_keywords = [\"risk\"\
        , \"uncertain\", \"unknown\", \"failure\", \"delay\", \"error\", \"overload\"\
        , \"omission\", \"breach\", \"exceed\", \"conflict\", \"incomplete\", \"violate\"\
        , \"prohibited\", \"violation\", \"unable\", \"insufficient\", \"not met\"\
        , \"not enough\", \"gap\", \"block\", \"missing\", \"insufficient\", \"bottleneck\"\
        , \"fault\", \"mistake\"]\n        \n        # We'll classify a risk if a\
        \ step or constraint contains or implies these words, or if a constraint obviously\
        \ makes a step hard\n        risks = []\n        steps = extract_workflow_steps(workflow)\n\
        \        constraint_phrases = extract_constraint_phrases(constraints)\n  \
        \      # Try to link constraints to steps\n        for step in steps:\n  \
        \          for constraint in constraint_phrases:\n                # Heuristic:\
        \ Does the constraint mention a resource, action, or parameter appearing in\
        \ the step?\n                overlap = False\n                tokens_constraint\
        \ = set(re.findall(r\"\\w+\", constraint.lower()))\n                tokens_step\
        \ = set(re.findall(r\"\\w+\", step.lower()))\n                common = tokens_constraint\
        \ & tokens_step\n                if len(common) > 0:\n                   \
        \ overlap = True\n                risk_triggered = False\n               \
        \ matched_keywords = []\n                for kw in risk_keywords:\n      \
        \              if kw in step.lower() or kw in constraint.lower():\n      \
        \                  risk_triggered = True\n                        matched_keywords.append(kw)\n\
        \                if overlap or risk_triggered:\n                    risk_text\
        \ = f\"Risk: Performing step '{step}' may be affected by constraint '{constraint}'.\"\
        \n                    if matched_keywords:\n                        risk_text\
        \ += f\" Potential issues: {', '.join(matched_keywords)}.\"\n            \
        \        risks.append({\n                        'description': risk_text,\n\
        \                        'step': step,\n                        'constraint':\
        \ constraint,\n                        'matched_keywords': matched_keywords\n\
        \                    })\n        # Fallback: look for standalone risks in\
        \ constraint/step text\n        for doc, typ in [(workflow, \"workflow\"),\
        \ (constraints, \"constraint\")]:\n            for kw in risk_keywords:\n\
        \                hits = [m.group(0) for m in re.finditer(kw, doc.lower())]\n\
        \                for hit in hits:\n                    risks.append({\n  \
        \                      'description': f\"Risk keyword '{kw}' found in {typ}\
        \ description.\",\n                        'step': None,\n               \
        \         'constraint': None,\n                        'matched_keywords':\
        \ [kw]\n                    })\n        return risks\n    \n    def classify_and_score_risks(risks,\
        \ framework):\n        # Apply simple scoring depending on the selected framework\
        \ (accepts 'FMEA' or 'risk matrix', case-insensitive)\n        classified\
        \ = []\n        # For demo, random-ish but deterministic score mapping\n \
        \       for i, risk in enumerate(risks):\n            c = risk.copy()\n  \
        \          if framework.strip().lower() == 'fmea':\n                # Example:\
        \ severity, likelihood, detectability each out of 10; simple heuristics\n\
        \                c['framework'] = 'FMEA'\n                c['severity'] =\
        \ min(10, 5 + len(risk['matched_keywords']))\n                c['likelihood']\
        \ = min(10, 3 + (i % 7))\n                c['detectability'] = min(10, 10\
        \ - (i % 7))\n                c['RPN'] = c['severity'] * c['likelihood'] *\
        \ c['detectability']\n            else:\n                # Assume simple risk\
        \ matrix: Low/Med/High\n                c['framework'] = 'Risk Matrix'\n \
        \               mk = len(risk['matched_keywords'])\n                if mk\
        \ > 2:\n                    c['risk_level'] = 'High'\n                elif\
        \ mk == 2:\n                    c['risk_level'] = 'Medium'\n             \
        \   else:\n                    c['risk_level'] = 'Low'\n            classified.append(c)\n\
        \        return classified\n    \n    def structure_output(risk_list, constraints):\n\
        \        # For each risk include explicit link(s) to constraints\n       \
        \ constraint_phrases = extract_constraint_phrases(constraints)\n        out\
        \ = []\n        for item in risk_list:\n            link = item.get('constraint')\n\
        \            if not link:\n                # Try to match in risk description\n\
        \                found = None\n                for cap in constraint_phrases:\n\
        \                    if cap in item.get('description',''):\n             \
        \           found = cap\n                        break\n                link\
        \ = found or ''\n            result = {\n                'risk': item.get('description',''),\n\
        \                'step': item.get('step'),\n                'constraint':\
        \ link,\n                'framework': item.get('framework'),\n           \
        \ }\n            # Add scores/levels\n            if 'RPN' in item:\n    \
        \            result['RPN'] = item['RPN']\n                result['severity']\
        \ = item['severity']\n                result['likelihood'] = item['likelihood']\n\
        \                result['detectability'] = item['detectability']\n       \
        \     if 'risk_level' in item:\n                result['risk_level'] = item['risk_level']\n\
        \            out.append(result)\n        return out\n    \n    # --- MAIN\
        \ EXECUTION ---\n    detected_risks = extract_risks(workflow_description,\
        \ constraints)\n    classified = classify_and_score_risks(detected_risks,\
        \ framework)\n    structured = structure_output(classified, constraints)\n\
        \    # Return as JSON strings to match List[str] output signature\n    return\
        \ [json.dumps(r) for r in structured]\n"
      name: assess_risks_with_constraints
      nodes_depended_on: []
      nodes_dependent_on: []
      output_structure:
      - description: Output of type List[dict]
        key: output
        type: List[str]
      - description: Input parameter of type str
        key: workflow_description
        type: str
      - description: Input parameter of type str
        key: constraints
        type: str
      - description: Input parameter of type str
        key: framework
        type: str
      prd:
        bullets:
        - complexity: MEDIUM
          impact: Reduces missed risks and enables timely stakeholder awareness and
            mitigation planning.
          method: Use NLP techniques such as keyword extraction and semantic analysis
            to correlate workflow steps and constraint phrases with common risk sources.
          reason: A structured extraction process is critical to ensure all relevant
            risks, especially those tied to constraints, are surfaced early.
          text: Systematically parse the workflow description and associated constraints
            to identify possible areas where risks or uncertainties may arise.
        - complexity: MEDIUM
          impact: Ensures traceability, repeatability, and executive buy-in for risk
            mitigating actions.
          method: Develop modular assessment routines that interpret input risks with
            the chosen framework's scoring methodology (e.g., severity, likelihood,
            detectability for FMEA).
          reason: Consistent risk evaluation allows decision-makers to prioritize
            responses and aligns with best practice risk management.
          text: Apply a selectable standard risk analysis framework (e.g., FMEA or
            risk matrix) to classify and rate each identified risk.
        - complexity: LOW
          impact: Facilitates downstream analysis, auditability, and targeted mitigation
            strategies.
          method: Augment risk data structures with constraint references and generate
            comprehensive output dicts for each risk.
          reason: Accountability and clarity are improved when the source or driver
            of risk is clearly articulated.
          text: Link each risk explicitly to the constraint(s) that contribute to
            or exacerbate it, and structure the output accordingly.
      prompt: Given a workflow description, a list of constraints, and a risk analysis
        framework (such as FMEA or risk matrix), systematically assess and document
        all significant risks or uncertainties associated with the workflow, explicitly
        linking each risk to relevant constraints, and output a structured list of
        identified risks.
      shims: []
    - agent: false
      code_node_type: virtual-pure
      description: Takes a list of identified risks and associated constraints to
        produce structured documentation linking each risk to rationale and references
        to specific constraints.
      implementation: "def document_risks_with_rationale(risks: str, constraints:\
        \ str) -> List[str]:\n    \"\"\"\n    Takes a list of identified risks and\
        \ associated constraints to produce structured documentation linking each\
        \ risk to rationale and references to specific constraints.\n\n    Args:\n\
        \        risks: Input parameter of type str\nconstraints: Input parameter\
        \ of type str\n\n    Returns:\n        List[str]: Output of type List[dict]\n\
        \    \"\"\"\n    import re\n    import json\n\n    # Helper: Split input text\
        \ into list of items, treating each nonempty line as an item.\n    def parse_list_items(text:\
        \ str) -> list:\n        # Try for numbered/bulleted list; else fallback to\
        \ nonempty lines\n        # Match patterns like: 1. something, - something,\
        \ * something\n        lines = [l.strip() for l in text.strip().splitlines()\
        \ if l.strip()]\n        items = []\n        for line in lines:\n        \
        \    match = re.match(r'^(\\d+\\.|[-*])\\s+(.*)$', line)\n            if match:\n\
        \                items.append(match.group(2).strip())\n            else:\n\
        \                items.append(line)\n        return [i for i in items if i]\n\
        \n    # Correlate each risk to relevant constraints by looking for overlap/causal\
        \ link in words\n    def correlate_risk_constraints(risk_text: str, constraints:\
        \ list) -> list:\n        # Naive keyword overlap: for each constraint, count\
        \ shared lowercased non-stopword words\n        import string\n        stopwords\
        \ = set([\n            'the', 'and', 'or', 'of', 'in', 'to', 'a', 'by', 'for',\
        \ 'is', 'with', 'on', 'at', 'be', 'that',\n            'as', 'may', 'will',\
        \ 'from', 'if', 'an', 'this', 'are', 'all', 'can', 'which', 'it', 'their',\
        \ 'not',\n            'any', 'but', 'we', 'do', 'should', 'must', 'our', 'there',\
        \ 'these', 'per', 'due', 'so', 'such', 'also'\n        ])\n        def tokenize(s):\n\
        \            s = s.lower().translate(str.maketrans('', '', string.punctuation))\n\
        \            words = s.split()\n            return set([w for w in words if\
        \ w and w not in stopwords])\n        risk_words = tokenize(risk_text)\n \
        \       matches = []\n        for c in constraints:\n            constraint_words\
        \ = tokenize(c)\n            shared = risk_words & constraint_words\n    \
        \        # If risk matches at least one word in constraint, count as relevant\n\
        \            if shared:\n                matches.append(c)\n        # If nothing\
        \ matched, fallback to NONE\n        if not matches:\n            # Optionally:\
        \ try soft/semantic match or just link all? But per PRD, avoid disconnected\
        \ risks\n            # For fallback, return [] (downstream output will show\
        \ empty associated_constraints)\n            pass\n        return matches\n\
        \n    # Generate rationale string for this risk\n    def generate_rationale(risk:\
        \ str, associated_constraints: list) -> str:\n        if associated_constraints:\n\
        \            constraints_str = '; '.join(associated_constraints)\n       \
        \     rationale = (\n                f\"This risk ('{risk}') was identified\
        \ due to its connection with the following constraint(s): {constraints_str}.\
        \ \"\n                f\"If these constraints are breached or not managed,\
        \ the described risk may materialize and impact workflow objectives.\"\n \
        \           )\n        else:\n            rationale = (\n                f\"\
        This risk ('{risk}') was identified independently and lacks direct traceable\
        \ constraint linkage in the provided context. \"\n                f\"Nonetheless,\
        \ inclusion is warranted due to potential adverse impact if not considered\
        \ in mitigation planning.\"\n            )\n        return rationale\n\n \
        \   # Parse risks & constraints\n    risk_list = parse_list_items(risks)\n\
        \    constraint_list = parse_list_items(constraints)\n\n    documented = []\n\
        \    for risk in risk_list:\n        associated_constraints = correlate_risk_constraints(risk,\
        \ constraint_list)\n        rationale = generate_rationale(risk, associated_constraints)\n\
        \        doc = {\n            'risk_description': risk,\n            'rationale':\
        \ rationale,\n            'associated_constraints': associated_constraints\n\
        \        }\n        documented.append(doc)\n\n    # Output each as a JSON\
        \ string\n    output_list = [json.dumps(doc, ensure_ascii=False) for doc in\
        \ documented]\n    return output_list\n"
      name: document_risks_with_rationale
      nodes_depended_on: []
      nodes_dependent_on: []
      output_structure:
      - description: Output of type List[dict]
        key: output
        type: List[str]
      - description: Input parameter of type str
        key: risks
        type: str
      - description: Input parameter of type str
        key: constraints
        type: str
      prd:
        bullets:
        - complexity: MEDIUM
          impact: Improves transparency for risk reviewers and managers, facilitating
            targeted mitigation planning.
          method: Implement mapping logic to search for overlap or causal links between
            risks and constraints, then store them together in structured records.
          reason: This establishes traceability and ensures risks are not disconnected
            from their potential sources in workflow constraints.
          text: Parse and correlate each risk with relevant constraints to ensure
            all risks are contextually supported and referenced.
        - complexity: LOW
          impact: Enables better understanding and prioritization of risks during
            management reviews.
          method: For each risk, create a rationale field using templated or dynamically
            generated natural language explanations based on risk and linked constraints.
          reason: Providing rationale clarifies why the risk matters and helps justify
            its inclusion for stakeholders.
          text: Generate a clear and concise rationale for each risk, explaining its
            potential impact and the reasoning behind its identification.
        - complexity: LOW
          impact: Facilitates integration with other workflow steps requiring structured
            risk data and ensures auditability.
          method: Serialize the documented risks and their attribution into a standardized
            list-of-dicts format or JSON structure.
          reason: This enables easy downstream consumption, review, and potential
            audit of risk documentation.
          text: 'Output results as a list of structured dictionaries (JSON objects),
            with each entry containing: risk description, rationale, and associated
            constraints.'
      prompt: 'For each risk in the provided risks list, create a structured output
        documenting: (1) the risk description, (2) a clear rationale explaining why
        this is a risk, and (3) explicit references to any relevant constraints from
        the constraints list that contribute to or are affected by this risk.'
      shims: []
    - agent: false
      code_node_type: virtual-pure
      description: Systematically evaluates a list of documented risks and tags each
        as either actionable or non-actionable, providing justification for each decision.
      implementation: "def tag_risks_actionable(risks: str) -> List[str]:\n    \"\"\
        \"\n    Systematically evaluates a list of documented risks and tags each\
        \ as either actionable or non-actionable, providing justification for each\
        \ decision.\n\n    Args:\n        risks: Input parameter of type str\n\n \
        \   Returns:\n        List[str]: Output of type List[dict]\n    \"\"\"\n \
        \   import json\n    import re\n    from typing import List\n\n    # Helper\
        \ function: rule-based actionability\n    def is_actionable(risk_desc: str)\
        \ -> (bool, str):\n        \"\"\"\n        Returns tuple: (actionable: bool,\
        \ justification: str)\n        \"\"\"\n        lowered = risk_desc.strip().lower()\n\
        \n        # Example keywords that are generally NON-actionable:\n        non_actionable_keywords\
        \ = [\n            # External, force-majeure or industry-wide\n          \
        \  'act of god', 'global recession', 'regulatory uncertainty', 'market volatility',\n\
        \            'economic downturn', 'natural disaster', 'pandemic',\n      \
        \      'external dependency', 'supply chain disruption',\n            'competitive\
        \ landscape', 'government policy',\n            'political instability', 'systemic\
        \ risk', 'macroeconomic', 'war', \"political forces\"\n        ]\n       \
        \ # If any of the non-actionable keywords match, it's non-actionable\n   \
        \     for kw in non_actionable_keywords:\n            if kw in lowered:\n\
        \                return False, (\n                    f\"Tagged as non-actionable:\
        \ The risk involves '{kw}', which is outside the organization's reasonable\
        \ sphere of control per guidelines.\"\n                )\n\n        # Example\
        \ keywords that are likely actionable\n        actionable_keywords = [\n \
        \           # Process, technology, staff, compliance, and known weaknesses\n\
        \            'data breach', 'compliance failure', 'outdated software', 'human\
        \ error',\n            'weak password', 'training gap', 'access control',\
        \ 'configuration error',\n            'lack of documentation', 'poor process',\
        \ 'customer complaint', 'system failure',\n            'security vulnerability',\
        \ 'insufficient testing', 'maintenance delay',\n            'improper validation',\
        \ 'internal fraud'\n        ]\n        for kw in actionable_keywords:\n  \
        \          if kw in lowered:\n                return True, (\n           \
        \         f\"Tagged as actionable: The risk relates to '{kw}', which can be\
        \ mitigated by organizational action (e.g., improved controls, process changes,\
        \ or remediation).\"\n                )\n        \n        # Heuristic: if\
        \ it contains phrases indicating it's within internal remediable scope\n \
        \       if re.search(r'(our|organization|company|team|staff|internal) (policy|process|procedure|control|systems?)',\
        \ lowered):\n            return True, (\n                \"Tagged as actionable:\
        \ The risk refers to internal factors that can be managed or improved per\
        \ best practice.\"\n            )\n        # Heuristic: if it says 'Unable\
        \ to', 'Lack of', 'No process' etc, and is not clearly external\n        if\
        \ re.search(r'(unable to|lack of|no process|insufficient|missing)', lowered):\n\
        \            # Check for external qualifiers\n            if not re.search(r'(regulatory|macro|market|government|political|external)',\
        \ lowered):\n                return True, (\n                    \"Tagged\
        \ as actionable: The identified shortcoming is within internal control and\
        \ fits remediation guidelines.\"\n                )\n        # If it mentions\
        \ \"cannot address\", \"outside control\", or similar, it is non-actionable\n\
        \        if re.search(r'(cannot address|beyond control|outside the organizations?\
        \ control)', lowered):\n            return False, (\n                \"Tagged\
        \ as non-actionable: The risk is explicitly stated to be outside organizational\
        \ control.\"\n            )\n        # Default logic/justification\n     \
        \   # If the risk mentions consequences of external events only, tag as non-actionable\n\
        \        if re.search(r'(change in law|acts? of government|third[- ]party\
        \ decision|force majeure)', lowered):\n            return False, (\n     \
        \           \"Tagged as non-actionable: The risk stems from unpredictable\
        \ external events not actionable by the organization.\"\n            )\n \
        \       # Default fallback to actionable (if uncertainty):\n        return\
        \ True, (\n            \"Tagged as actionable: No clear external or unmanageable\
        \ factors; defaulting to actionable per organizational guidance.\"\n     \
        \   )\n\n    # Step 1: Parse input str to list of dicts\n    try:\n      \
        \  parsed_risks = json.loads(risks)\n        if not isinstance(parsed_risks,\
        \ list):\n            raise ValueError('Input JSON is not a list of risks.')\n\
        \    except Exception as e:\n        raise ValueError(f'Input string could\
        \ not be parsed as a list of risk dicts: {e}')\n\n    output_risks: List[dict]\
        \ = []\n    for risk in parsed_risks:\n        # Each risk must be a dict\
        \ with at least a description.\n        desc = risk.get('description') or\
        \ risk.get('risk') or risk.get('text')\n        if not isinstance(desc, str):\n\
        \            # Try to use the entire risk as string if not found\n       \
        \     desc_s = str(risk)\n        else:\n            desc_s = desc\n     \
        \   actionable, justification = is_actionable(desc_s)\n        risk_with_flags\
        \ = dict(risk)\n        risk_with_flags['actionable'] = actionable\n     \
        \   risk_with_flags['justification'] = justification\n        output_risks.append(risk_with_flags)\n\
        \n    # Return as JSON strings, as required by signature List[str], one per\
        \ risk\n    # (If it should return List[dict], simply return output_risks)\n\
        \    return [json.dumps(risk) for risk in output_risks]\n"
      name: tag_risks_actionable
      nodes_depended_on: []
      nodes_dependent_on: []
      output_structure:
      - description: Output of type List[dict]
        key: output
        type: List[str]
      - description: Input parameter of type str
        key: risks
        type: str
      prd:
        bullets:
        - complexity: MEDIUM
          impact: Improves clarity on which risks require action and which should
            simply be acknowledged or monitored, streamlining subsequent risk management
            processes.
          method: Implement rule-based logic (e.g., keyword detection, risk categorization
            against predefined actionable criteria) or heuristic evaluation, appending
            an 'actionable' flag to each risk dictionary entry.
          reason: This ensures risks are clearly classified based on management feasibility,
            aiding in prioritization for workflow planning.
          text: Parse each risk entry and assess whether it is actionable (can be
            addressed by the organization) or non-actionable (cannot be addressed),
            then append an 'actionable' boolean field to each risk dictionary.
        - complexity: MEDIUM
          impact: Facilitates understanding among stakeholders and enables better
            decision-making during risk review meetings.
          method: Automatically create a justification string per risk based on its
            description, rationale, and constraints, referencing organization guidelines
            for actionability.
          reason: Providing justifications enables traceability and substantiates
            the tagging, supporting future audit or review processes.
          text: Generate and append a concise justification to each risk entry explaining
            the decision for it being actionable or non-actionable.
      prompt: "Given a list of risks (each as a dictionary with risk description,\
        \ rationale, and any linked constraints), systematically evaluate each risk\
        \ and tag whether it is 'actionable' (i.e., can be mitigated or addressed\
        \ through concrete steps or interventions) or 'non-actionable' (i.e., inherent\
        \ or outside the organization\u2019s control). Output the augmented list of\
        \ risk dictionaries, each now including a key 'actionable' with a boolean\
        \ value indicating its actionability status, along with a brief justification\
        \ per risk explaining the tagging decision."
      shims: []
    - agent: false
      code_node_type: virtual-pure
      description: Filters a list of risk objects to retain only those that are actionable
        or critical according to the specified filtering mode.
      implementation: "def filter_major_risks(risks: str, filter_mode: str) -> List[str]:\n\
        \    \"\"\"\n    Filters a list of risk objects to retain only those that\
        \ are actionable or critical according to the specified filtering mode.\n\n\
        \    Args:\n        risks: Input parameter of type str\nfilter_mode: Input\
        \ parameter of type str\n\n    Returns:\n        List[str]: Output of type\
        \ List[dict]\n    \"\"\"\n    import json\n    \n    # Deserialize the input\
        \ risks string to a list of dicts (if already a list, skip this step)\n  \
        \  try:\n        risks_list = json.loads(risks)\n    except Exception as e:\n\
        \        raise ValueError(\"Input 'risks' must be JSON-encoded list of dicts.\"\
        ) from e\n    if not isinstance(risks_list, list):\n        raise ValueError(\"\
        Input 'risks' JSON does not decode to a list.\")\n\n    # Filter by mode:\n\
        \    filter_mode_lc = filter_mode.lower().strip()\n    \n    def is_actionable(risk_dict:\
        \ dict) -> bool:\n        # Define what makes a risk 'actionable' based on\
        \ common fields/tags.\n        tags = risk_dict.get('tags') or risk_dict.get('labels')\
        \ or []\n        if isinstance(tags, str):\n            tags = [tags]\n  \
        \      tags_lc = [str(t).lower() for t in tags]\n        status = str(risk_dict.get('status',\
        \ '')).lower()\n        # Consider as actionable if tag or status contains\
        \ 'actionable'.\n        return 'actionable' in tags_lc or status == 'actionable'\n\
        \n    def is_critical(risk_dict: dict) -> bool:\n        tags = risk_dict.get('tags')\
        \ or risk_dict.get('labels') or []\n        if isinstance(tags, str):\n  \
        \          tags = [tags]\n        tags_lc = [str(t).lower() for t in tags]\n\
        \        severity = str(risk_dict.get('severity', '')).lower()\n        #\
        \ Consider as critical if tag or severity contains 'critical' or 'high'.\n\
        \        return (\n            'critical' in tags_lc or 'high' in tags_lc\
        \ or\n            severity in ['critical', 'high']\n        )\n\n    # Logical\
        \ filtering\n    filtered_risks = []\n    for risk in risks_list:\n      \
        \  if filter_mode_lc == 'actionable':\n            if is_actionable(risk):\n\
        \                filtered_risks.append(risk)\n        elif filter_mode_lc\
        \ == 'critical':\n            if is_critical(risk):\n                filtered_risks.append(risk)\n\
        \        elif filter_mode_lc == 'actionable_or_critical' or filter_mode_lc\
        \ == 'actionable_or_major':\n            # Accept if either rule matches\n\
        \            if is_actionable(risk) or is_critical(risk):\n              \
        \  filtered_risks.append(risk)\n        else:\n            # If unknown filter_mode,\
        \ default to passthrough (do not filter)\n            filtered_risks.append(risk)\n\
        \n    # The structure and integrity of each original risk dict is maintained.\n\
        \    # Convert back to JSON strings for each dict to match List[str] return\
        \ type.\n    return [json.dumps(risk, separators=(',', ':')) for risk in filtered_risks]\n"
      name: filter_major_risks
      nodes_depended_on: []
      nodes_dependent_on: []
      output_structure:
      - description: Output of type List[dict]
        key: output
        type: List[str]
      - description: Input parameter of type str
        key: risks
        type: str
      - description: Input parameter of type str
        key: filter_mode
        type: str
      prd:
        bullets:
        - complexity: LOW
          impact: The workflow downstream will work efficiently with a manageable
            risk set, helping prioritize resolution and mitigation efforts.
          method: Deserialize the input risks if necessary, iterate through each risk
            dictionary, and filter by checking predefined fields or tags corresponding
            to 'actionable' or 'critical' status.
          reason: Filtering ensures only the most relevant and high-priority risks
            are considered for subsequent analysis and discussions, improving focus
            and decision-making.
          text: Parse the incoming risks input and apply logical filtering based on
            the value of filter_mode to extract actionable or critical risks.
        - complexity: LOW
          impact: Users and automated processes retain access to all necessary context
            and metadata for each major risk after filtering.
          method: Perform non-destructive filtering that returns the original risk
            dictionaries, avoiding field dropping or data truncation.
          reason: Preserving all details is critical for traceability and further
            references or audit trails in risk management processes.
          text: Maintain the structure and integrity of the risk dictionaries in the
            output so no data is lost during filtering.
      prompt: Filter the provided list of risk objects represented as dictionaries
        according to the specified 'filter_mode', returning only those deemed 'actionable'
        or 'critical' as appropriate. Ensure the output is a list of dictionaries
        with all relevant fields preserved.
      shims: []
    - agent: false
      code_node_type: virtual-pure
      description: Formats a structured list of major risks into readable output strings
        that concisely summarize each risk and their associated details for decision
        makers.
      implementation: "def format_major_risks(risks: str) -> List[str]:\n    \"\"\"\
        \n    Formats a structured list of major risks into readable output strings\
        \ that concisely summarize each risk and their associated details for decision\
        \ makers.\n\n    Args:\n        risks: Input parameter of type str\n\n   \
        \ Returns:\n        List[str]: Output of type List[str]\n    \"\"\"\n    import\
        \ json\n    # Assume risks is a JSON string representing a list of risk dictionaries\n\
        \    try:\n        risk_list = json.loads(risks)\n    except Exception as\
        \ e:\n        raise ValueError(\"Input risks string is not valid JSON\") from\
        \ e\n    formatted_risks = []\n    for risk in risk_list:\n        # Extract\
        \ fields with defaults for missing/optional fields\n        summary = risk.get('summary',\
        \ '[No summary provided]')\n        rationale = risk.get('rationale', '[No\
        \ rationale provided]')\n        # Linked constraints may be a list or a single\
        \ string/identifier\n        constraints = risk.get('constraints', [])\n \
        \       # Normalize constraints to string for output\n        if isinstance(constraints,\
        \ list):\n            if constraints:\n                constraints_str = \"\
        , \".join(str(c) for c in constraints)\n            else:\n              \
        \  constraints_str = 'None'\n        elif constraints:\n            constraints_str\
        \ = str(constraints)\n        else:\n            constraints_str = 'None'\n\
        \        # Structured, executive-adaptable format\n        formatted = f\"\
        [Risk]: {summary} | Rationale: {rationale} | Linked Constraint(s): {constraints_str}\"\
        \n        formatted_risks.append(formatted)\n    return formatted_risks\n"
      name: format_major_risks
      nodes_depended_on: []
      nodes_dependent_on: []
      output_structure:
      - description: Output of type List[str]
        key: output
        type: List[str]
      - description: Input parameter of type str
        key: risks
        type: str
      prd:
        bullets:
        - complexity: LOW
          impact: Enables clear communication in reports, meetings, or automated dashboards,
            improving risk transparency.
          method: Parse each risk dictionary or object, extract key fields (summary,
            rationale, constraint links), and assemble a formatted string using consistent
            templates, possibly with markdown or bullet-point structure.
          reason: Readability and structured presentation are crucial for stakeholders
            to understand, validate, and prioritize major risks at a glance.
          text: Transform each input risk object into a clearly structured string
            that encapsulates the risk summary, rationale, and references any linked
            constraints.
        - complexity: LOW
          impact: Supports adoption at the senior management level and prepares outputs
            for inclusion in workflow governance or audit materials.
          method: 'Define and document a template (e.g., ''[Risk]: {summary} | Rationale:
            {text} | Linked Constraint(s): {references}''), and implement the string
            assembly with robust handling of missing or optional fields.'
          reason: Executives reviewing risks need uniformity and brevity to enable
            quick comprehension and informed decision-making.
          text: Ensure the formatting style is adaptable for executive-level presentation
            (e.g., lists, numbered items, or concise paragraphs).
      prompt: Given a set of risk objects or dictionaries, generate a human-readable,
        well-structured string for each item that summarizes the risk, outlines its
        rationale, and clearly links it to any relevant constraints or actionable
        context; the output should be suitable for downstream review or executive
        decision-making.
      shims: []
    - agent: false
      code_node_type: virtual-pure
      description: Generates a detailed, actionable sequence of steps required to
        organize and conduct a senior manager meeting designed to review and validate
        identified workflow risks before assigning user roles, using options for agenda
        notes, calendar integration, and notification setup.
      implementation: "def generate_senior_manager_meeting_steps(risks: str, constraints:\
        \ str, agenda_notes: str, calendar_integration: str, notification_queue: str)\
        \ -> List[str]:\n    \"\"\"\n    Generates a detailed, actionable sequence\
        \ of steps required to organize and conduct a senior manager meeting designed\
        \ to review and validate identified workflow risks before assigning user roles,\
        \ using options for agenda notes, calendar integration, and notification setup.\n\
        \n    Args:\n        risks: Input parameter of type str\nconstraints: Input\
        \ parameter of type str\nagenda_notes: Input parameter of type str\ncalendar_integration:\
        \ Input parameter of type str\nnotification_queue: Input parameter of type\
        \ str\n\n    Returns:\n        List[str]: Output of type List[str]\n    \"\
        \"\"\n    # PURE PYTHON IMPLEMENTATION\n    steps = []\n    step_num = 1\n\
        \n    # Parsing input strings (assume comma or newline separation for lists)\n\
        \    def parse_list(s: str) -> list:\n        import re\n        if not s\
        \ or not s.strip():\n            return []\n        # Split by comma or newline,\
        \ remove empty, strip whitespace\n        items = [i.strip() for i in re.split(r',|\\\
        n', s) if i.strip()]\n        return items\n\n    risks_list = parse_list(risks)\n\
        \    constraints_list = parse_list(constraints)\n\n    # 1. Scheduling logistics\n\
        \    steps.append(f\"{step_num}. Confirm meeting objectives with all stakeholders\
        \ (focus: senior management risk review before user role assignment).\")\n\
        \    step_num += 1\n    steps.append(f\"{step_num}. Identify and invite required\
        \ senior management and relevant participants.\")\n    step_num += 1\n   \
        \ if calendar_integration.strip().lower() in (\"yes\", \"true\", \"enabled\"\
        , \"1\"):  # crude bool test\n        steps.append(f\"{step_num}. Create a\
        \ calendar event for the meeting using your organization's calendar system\
        \ and invitees' emails.\")\n        step_num += 1\n    \n    # 2. Agenda preparation\n\
        \    agenda_added = False\n    if agenda_notes.strip():\n        steps.append(f\"\
        {step_num}. Prepare and circulate a detailed agenda including: {agenda_notes.strip()}\"\
        )\n        agenda_added = True\n        step_num += 1\n    else:\n       \
        \ steps.append(f\"{step_num}. Draft and share an agenda specifying the workflow\
        \ risk review as the primary topic.\")\n        step_num += 1\n    \n    #\
        \ 3. Pre-read and information gathering\n    if risks_list:\n        steps.append(f\"\
        {step_num}. Compile and summarize all identified workflow risks to be reviewed:\"\
        )\n        for idx, risk in enumerate(risks_list, 1):\n            steps.append(f\"\
        \    - Risk {idx}: {risk}\")\n        step_num += 1\n    if constraints_list:\n\
        \        steps.append(f\"{step_num}. List and explain constraints relevant\
        \ to the risk review:\")\n        for idx, constraint in enumerate(constraints_list,\
        \ 1):\n            steps.append(f\"    - Constraint {idx}: {constraint}\"\
        )\n        step_num += 1\n    \n    steps.append(f\"{step_num}. Distribute\
        \ meeting materials (agenda, risk summaries, constraints) to attendees in\
        \ advance.\")\n    step_num += 1\n\n    # 4. Automated notification setup\n\
        \    if notification_queue.strip().lower() in (\"yes\", \"true\", \"enabled\"\
        , \"1\"):\n        steps.append(f\"{step_num}. Set up automated meeting notifications/reminders\
        \ for all participants using notification service or email.\")\n        step_num\
        \ += 1\n\n    # 5. Meeting execution\n    steps.append(f\"{step_num}. Conduct\
        \ the meeting, ensuring all documented risks and constraints are reviewed\
        \ and discussed. Document senior management feedback or decisions.\")\n  \
        \  step_num += 1\n    steps.append(f\"{step_num}. Capture action items, validate\
        \ comprehensiveness, and assign follow-up responsibilities as needed.\")\n\
        \    step_num += 1\n    \n    steps.append(f\"{step_num}. Finalize decisions,\
        \ update risk documentation, and proceed with user role assignment in the\
        \ workflow.\")\n\n    return steps\n"
      name: generate_senior_manager_meeting_steps
      nodes_depended_on: []
      nodes_dependent_on: []
      output_structure:
      - description: Output of type List[str]
        key: output
        type: List[str]
      - description: Input parameter of type str
        key: risks
        type: str
      - description: Input parameter of type str
        key: constraints
        type: str
      - description: Input parameter of type str
        key: agenda_notes
        type: str
      - description: Input parameter of type str
        key: calendar_integration
        type: str
      - description: Input parameter of type str
        key: notification_queue
        type: str
      prd:
        bullets:
        - complexity: MEDIUM
          impact: Reduces oversights, guarantees executive awareness and input, and
            improves accountability in risk assessment prior to workflow role assignment.
          method: Template-based step generation logic that adapts its output based
            on presence of risks, constraints, and optional features (agenda, calendar,
            notifications), returning a dynamic list of actionable instructions.
          reason: To ensure that all logistical and procedural aspects of the meeting
            are addressed, promoting consistency and thoroughness in risk validation.
          text: Generate a sequenced checklist outlining all necessary steps to schedule,
            prepare, and execute a risk review meeting with senior management.
        - complexity: LOW
          impact: Improves meeting effectiveness and timeliness, reduces missed communication,
            and embeds best-practice administrative procedures.
          method: Conditional inclusion of agenda itemization, ICS file/calendar API
            integration calls, and notification queueing steps within the final step
            list.
          reason: To streamline meeting organization and ensure all participants are
            well informed and reminded, reducing coordination friction.
          text: Integrate agenda preparation, calendar scheduling, and automated notification
            setup into the generated step list based on provided input flags.
        - complexity: LOW
          impact: Ensures comprehensive risk review, improves traceability, and provides
            a record of due diligence for audit and compliance.
          method: For each step referring to meeting content or preparation, dynamically
            insert summaries or references to the risk and constraint inputs.
          reason: To guarantee that every significant risk and related constraint
            is brought to senior management's attention, preventing omissions.
          text: Reference all relevant risks and constraints within the meeting steps
            to ensure they are explicitly included in review and discussion.
      prompt: Given a list of identified risks, relevant workflow constraints, and
        input flags for agenda notes, calendar integration, and notification queue,
        generate a clear and actionable step-by-step set of instructions for organizing
        and conducting a senior manager meeting to review risks before user role assignment.
      shims: []
    - agent: false
      code_node_type: virtual-pure
      description: This node enforces that a documented senior manager review meeting
        must occur and be confirmed complete in the workflow before any user role
        assignment actions are allowed.
      implementation: "def enforce_meeting_before_roles(meeting_steps: str, workflow_state:\
        \ str) -> str:\n    \"\"\"\n    This node enforces that a documented senior\
        \ manager review meeting must occur and be confirmed complete in the workflow\
        \ before any user role assignment actions are allowed.\n\n    Args:\n    \
        \    meeting_steps: Input parameter of type str\nworkflow_state: Input parameter\
        \ of type str\n\n    Returns:\n        str: Output of type Any\n    \"\"\"\
        \n    import json\n    import datetime\n\n    # ----- PURE IMPLEMENTATION\
        \ -----\n\n    # Parse the workflow_state input and ensure it's a dict\n \
        \   try:\n        if isinstance(workflow_state, str):\n            state =\
        \ json.loads(workflow_state)\n        else:\n            state = workflow_state\n\
        \    except Exception:\n        # Fail safely with audit log and clear error\n\
        \        return \"ERROR: Unable to parse workflow_state input as JSON. Prevented\
        \ user role assignment.\"\n\n    audit_log_entries = []\n    status_message\
        \ = \"\"\n    gating_passed = False\n\n    # Check for the existence of a\
        \ completed and validated senior manager meeting\n    meeting_data = state.get('senior_manager_meeting',\
        \ {})\n    meeting_complete = False\n    meeting_validated = False\n    meeting_outcome\
        \ = None\n\n    if isinstance(meeting_data, dict):\n        meeting_complete\
        \ = bool(meeting_data.get('meeting_complete'))\n        meeting_validated\
        \ = bool(meeting_data.get('validated'))\n        meeting_outcome = meeting_data.get('outcome')\n\
        \n    # Determine if gating condition is met\n    if meeting_complete and\
        \ meeting_validated:\n        gating_passed = True\n        status_message\
        \ = \"SUCCESS: Senior manager meeting completed and validated. Progression\
        \ to user role assignment allowed.\"\n        audit_status = \"pass\"\n  \
        \  else:\n        gating_passed = False\n        missing = []\n        if\
        \ not meeting_complete:\n            missing.append(\"'meeting_complete'\"\
        )\n        if not meeting_validated:\n            missing.append(\"'validated'\"\
        )\n        status_message = (\n            \"ERROR: Cannot proceed to user\
        \ role assignment. Missing prerequisite(s): \" + ', '.join(missing) + \".\"\
        \n        )\n        audit_status = \"block\"\n\n    # --- Audit log hook\
        \ ---\n    audit_entry = {\n        \"timestamp\": datetime.datetime.utcnow().isoformat()\
        \ + \"Z\",\n        \"check\": \"enforce_meeting_before_roles\",\n       \
        \ \"result\": audit_status,\n        \"required_fields_present\": gating_passed,\n\
        \        \"missing_flags\": [] if gating_passed else missing,\n        \"\
        meeting_outcome\": meeting_outcome,\n        \"user_action\": \"attempt_user_role_assignment\"\
        \n    }\n    audit_log_entries.append(audit_entry)\n\n    # In real deployment,\
        \ write audit log to external logger/hook; here we serialize for downstream\n\
        \    output = {\n        \"status\": status_message,\n        \"audit\": audit_log_entries\n\
        \    }\n    return json.dumps(output)\n"
      name: enforce_meeting_before_roles
      nodes_depended_on: []
      nodes_dependent_on: []
      output_structure:
      - description: Output of type Any
        key: output
        type: str
      - description: Input parameter of type str
        key: meeting_steps
        type: str
      - description: Input parameter of type str
        key: workflow_state
        type: str
      prd:
        bullets:
        - complexity: MEDIUM
          impact: Prevents workflow progression toward user role setup without required
            high-level review, enhancing control and compliance.
          method: Add a state-checking function that verifies presence of a 'meeting_complete'
            flag and possibly meeting outcome data in the workflow_state before allowing
            transitions.
          reason: This ensures that executive input on major risks is obtained at
            the correct point, preventing accidental bypass of governance checks.
          text: Implement a gating mechanism that checks the workflow state for a
            completed and validated senior manager meeting before permitting progression
            to user role assignment steps.
        - complexity: LOW
          impact: Improves transparency and auditability of governance enforcement,
            reducing user confusion and supporting compliance.
          method: Use exception handling or output-return logic to indicate missing
            prerequisites when the gating condition is not met.
          reason: Clear user feedback is critical for usability and for ensuring that
            process checks are respected and understood.
          text: Log or return an explicit error or status message if a user attempts
            to proceed to user role assignment without the mandatory meeting completion.
        - complexity: MEDIUM
          impact: Enables retrospective tracing of decision and enforcement points
            in the workflow, aiding governance and process improvement.
          method: Trigger a logging function with details of the enforcement check
            outcome as part of the gating logic.
          reason: Maintaining an audit trail ensures that all compliance gates and
            interventions are tracked for later review and accountability.
          text: Integrate hooks for downstream workflow audit logs to record the enforcement
            check and any resulting blocks or status changes.
      prompt: Ensure that all specified steps for a senior manager meeting are completed
        and validated in the workflow state before proceeding to any user roles determination
        or assignment activities.
      shims: []
    - agent: false
      code_node_type: virtual-pure
      description: This node logs an auditable record associating risks, constraints,
        and actions taken as part of the risk management process.
      implementation: "def log_risk_audit_trail(risks: str, constraints: str, action:\
        \ str) -> str:\n    \"\"\"\n    This node logs an auditable record associating\
        \ risks, constraints, and actions taken as part of the risk management process.\n\
        \n    Args:\n        risks: Input parameter of type str\nconstraints: Input\
        \ parameter of type str\naction: Input parameter of type str\n\n    Returns:\n\
        \        str: Output of type Any\n    \"\"\"\n    import json\n    import\
        \ time\n    import os\n    import uuid\n    from datetime import datetime\n\
        \n    # Construct the audit log entry\n    log_entry = {\n        'id': str(uuid.uuid4()),\n\
        \        'timestamp': datetime.utcnow().isoformat() + 'Z',\n        'risks':\
        \ risks,\n        'constraints': constraints,\n        'action': action,\n\
        \    }\n\n    # Determine log file path (append-only)\n    log_dir = 'audit_logs'\n\
        \    os.makedirs(log_dir, exist_ok=True)\n    log_file = os.path.join(log_dir,\
        \ 'risk_audit_trail.log')\n\n    # Serialize entry to the log file (one JSON\
        \ per line)\n    with open(log_file, 'a', encoding='utf-8') as f:\n      \
        \  f.write(json.dumps(log_entry) + '\\n')\n\n    # Return the entry ID as\
        \ confirmation / reference\n    return log_entry['id']\n"
      name: log_risk_audit_trail
      nodes_depended_on: []
      nodes_dependent_on: []
      output_structure:
      - description: Output of type Any
        key: output
        type: str
      - description: Input parameter of type str
        key: risks
        type: str
      - description: Input parameter of type str
        key: constraints
        type: str
      - description: Input parameter of type str
        key: action
        type: str
      prd:
        bullets:
        - complexity: MEDIUM
          impact: This ensures all important risk-handling steps can be audited and
            reconstructed, supporting regulatory, operational, and management oversight.
          method: Design a persistent audit log schema and implement a function that
            serializes risks, constraints, and actions to a durable store (e.g., database
            or secure file), including timestamps and unique identifiers.
          reason: Maintaining traceable documentation is essential for governance,
            accountability, and review of risk-related decisions.
          text: Persist a structured audit log entry that includes the detailed risks,
            constraints, and action information for traceability and compliance.
        - complexity: LOW
          impact: Provides certainty and allows error handling or retries if the audit
            log step fails.
          method: Implement response handling to return a status message or identifier
            generated during the log persistence step.
          reason: Acknowledging successful logging allows upstream workflow components
            to verify that traceability has been established before proceeding.
          text: Return a confirmation or reference (such as a log entry ID) upon successful
            audit trail creation.
      prompt: Record an audit trail entry capturing the current set of risks, source
        constraints, and specified action (such as 'initial_risk_identification_meeting'),
        ensuring that full traceability and context are preserved for all major risk
        management decisions.
      shims: []
  identify_success_metrics:
    agent: false
    code_node_type: pure
    description: List measurable success criteria (quantitative or qualitative) for
      the workflow, ensuring that constraints are incorporated as inputs to defining
      these metrics. Explicitly include a success metric for the scheduling and completion
      of a meeting with senior managers prior to any determination of user roles,
      aligned with the requirement to add such a node before user roles are set.
    implementation: "from pydantic import BaseModel, Field\nfrom typing import List\n\
      \n\nclass IdentifyCoreObjectiveOutput(BaseModel):\n    \"\"\"Pydantic model\
      \ for identify_core_objective node outputs.\"\"\"\n    core_objective: str =\
      \ Field(..., description=\"The primary objective statement for the workflow\
      \ as described in the requirements, reflecting the need to include a meeting\
      \ with senior managers prior to determining user roles, and noting that constraints\
      \ should be available for use by dependent nodes, such as those identifying\
      \ success metrics.\")\n\n\nclass IdentifySuccessMetricsOutput(BaseModel):\n\
      \    \"\"\"Pydantic model for identify_success_metrics node outputs.\"\"\"\n\
      \    success_metrics: List[str] = Field(..., description=\"List of measurable\
      \ success metrics or acceptance criteria, each reflecting alignment with the\
      \ core objective and honoring identified constraints, and including at least\
      \ one explicit metric that the meeting with senior managers has been scheduled\
      \ and completed before user roles are determined.\")\n\n\ndef identify_success_metrics_fx(identify_core_objective_input:\
      \ IdentifyCoreObjectiveOutput, **kwargs) -> IdentifySuccessMetricsOutput:\n\
      \    \"\"\"List measurable success criteria (quantitative or qualitative) for\
      \ the workflow, ensuring that constraints are incorporated as inputs to defining\
      \ these metrics. Explicitly include a success metric for the scheduling and\
      \ completion of a meeting with senior managers prior to any determination of\
      \ user roles, aligned with the requirement to add such a node before user roles\
      \ are set.\n\n    Args:\n        identify_core_objective_input: Input from the\
      \ 'identify_core_objective' node.\n        **kwargs: Additional keyword arguments.\n\
      \n    Returns:\n        IdentifySuccessMetricsOutput: Object containing outputs\
      \ for this node.\n    \"\"\"\n    # Extract the core objective and constraints\
      \ from the parent node and additional kwargs (if present)\n    core_objective:\
      \ str = identify_core_objective_input.core_objective\n    # Extract constraints\
      \ from kwargs or parent node outputs\n    constraints: List[str] = extract_constraints_from_parent_input(inputs=kwargs,\
      \ fallback_objective_output=identify_core_objective_input)  # type: List[str]\n\
      \n    # Map each explicit constraint to one or more measurable success metrics\n\
      \    constraint_to_metrics: List[str] = map_constraints_to_metrics(constraints=constraints,\
      \ objective=core_objective)  # type: List[str]\n\n    # Ensure an explicit metric\
      \ for meeting completion with senior managers prior to user role assignment\n\
      \    meeting_metric: str = generate_meeting_governance_metric(core_objective=core_objective,\
      \ constraints=constraints)\n\n    # Structure all metrics using a standardized\
      \ format (e.g., SMART/OKR)\n    formatted_metrics: List[str] = structure_metrics_for_reporting(metrics=constraint_to_metrics\
      \ + [meeting_metric], template=\"SMART\")\n\n    # Support both quantitative\
      \ and qualitative metrics based on constraints/objective analysis\n    enriched_metrics:\
      \ List[str] = generate_quant_qual_metrics(metrics=formatted_metrics, constraints=constraints,\
      \ core_objective=core_objective)\n\n    # Ensure traceability of each metric\
      \ to originating feature/constraint/workflow node\n    traceable_metrics: List[str]\
      \ = add_traceability_to_metrics(metrics=enriched_metrics, constraints=constraints,\
      \ node_reference=kwargs.get('node_reference'))\n\n    # Automated check: flag\
      \ any unmapped constraints\n    unmapped_constraints: List[str] = find_unmapped_constraints(constraints=constraints,\
      \ metrics=traceable_metrics)\n    log_or_raise_unmapped_constraints(unmapped_constraints=unmapped_constraints)\n\
      \n    # Return as output instance\n    return IdentifySuccessMetricsOutput(success_metrics=traceable_metrics)\n"
    name: identify_success_metrics
    nodes_depended_on:
    - identify_core_objective
    nodes_dependent_on: []
    output_structure:
    - description: List of measurable success metrics or acceptance criteria, each
        reflecting alignment with the core objective and honoring identified constraints,
        and including at least one explicit metric that the meeting with senior managers
        has been scheduled and completed before user roles are determined.
      key: success_metrics
      type: List[str]
    prd:
      bullets:
      - complexity: LOW - Standard data dependency extraction and referencing.
        impact: HIGH - Sets the foundation for accurate, meaningful, and verifiable
          metrics; critical for downstream validation and reporting.
        method: Parse parent node outputs to extract core objective and a canonical
          list of constraints; structure as input parameters for metric derivation
          logic.
        reason: Anchoring metrics in the core objective and constraints guarantees
          relevance, stakeholder alignment, and consistency across the workflow.
        text: Ingest the core objective and extracted constraints from the parent
          node to ensure all success metrics are directly aligned with workflow intent
          and limitations.
      - complexity: MEDIUM - Requires careful cross-mapping, especially if constraints
          are abstract or span multiple workflow steps.
        impact: HIGH - Crucial for verification, compliance, and later validation
          of system performance and adoption.
        method: "Iteratively generate metrics by evaluating each constraint\u2019\
          s type (technical, business, deadline, etc.), using pattern matching or\
          \ rules-based logic to propose appropriate quantitative or qualitative metrics."
        reason: Constraints without associated metrics risk being unenforceable, while
          explicit metrics enforce accountability and track compliance.
        text: Map each explicit constraint to a corresponding success metric, ensuring
          that every constraint is testable and has one or more observable criteria
          for fulfillment.
      - complexity: LOW - Straightforward event-tracking, provided the workflow includes
          timestamped event logging.
        impact: MEDIUM - Compliance with process governance, but narrow in scope.
        method: "Add an explicit metric, e.g., \u2018Meeting with senior managers\
          \ is scheduled and recorded as complete (timestamped, with participant log)\
          \ prior to user role assignment step.\u2019"
        reason: The explicit upstream requirement ensures governance and buy-in prior
          to progressing the workflow, making verification of this event essential.
        text: Ensure at least one success metric captures the successful scheduling
          and documented completion of a meeting with senior managers prior to user
          role determination.
      - complexity: LOW - Routine formatting work.
        impact: MEDIUM - Eases downstream QA, dashboarding, and executive reporting.
        method: "Apply a formatting template (e.g., \u2018[Action/Outcome], [quantifier\
          \ or threshold], [by when/where], [constraint reference]\u2019) for each\
          \ metric."
        reason: Consistent metric structure aids in automation, stakeholder understanding,
          and integration with analytics.
        text: Structure each metric as a clearly defined, measurable statement using
          standardized formats (OKRs, SMART criteria, etc.) to ensure ease of downstream
          validation and reporting.
      - complexity: MEDIUM - Requires logic to determine when to generate qualitative
          vs. quantitative metrics.
        impact: MEDIUM - Improves the breadth and nuance of evaluation.
        method: Analyze the nature of each workflow step and constraint; use branching
          logic to output numeric metrics (completion, time, error rate) vs. qualitative
          ones (approval/feedback received, documentation reviewed).
        reason: Some elements will not be adequately captured via numbers and require
          qualitative measurement (e.g., approval, feedback).
        text: Support both quantitative (e.g., completion rates, response times) and
          qualitative (e.g., stakeholder satisfaction, compliance verification) metrics,
          as required by the constraints and workflow objective.
      - complexity: LOW - Requires reference mapping during metric generation.
        impact: HIGH - Vital for debugging, reporting, and workflow evolution support.
        method: Include feature or constraint ID/reference as a field/annotation in
          the output metric.
        reason: Traceability enables efficient root-cause analysis, compliance audits,
          and impact tracking for future workflow changes.
        text: Ensure traceability of each metric to its originating feature, constraint,
          or workflow node by including references or identifiers in the final metric
          output.
      - complexity: LOW - Standard data integrity check.
        impact: MEDIUM - Improves completeness and reliability of workflow validation.
        method: After metric generation, compare sets of constraints and metrics;
          output discrepancies for review.
        reason: Prevents requirements leakage and unmonitored constraint compliance
          gaps.
        text: Perform automated checks to ensure all constraints are mapped to at
          least one success metric, logging any unmapped constraints as errors or
          requiring user review.
    prompt: Based on the core objective and all relevant workflow constraints, identify
      and enumerate the main success metrics for the workflow. Each metric should
      be stated as a measurable goal or observable outcome directly tied to the objective
      and compliant with the constraints. Consider how each identified constraint
      may require additional or specific success criteria, or may restrict how the
      workflow may be deemed successful. Ensure that you incorporate, as a distinct
      success metric, the successful scheduling and completion of a meeting with senior
      managers prior to determining user roles in the workflow (reflecting the explicit
      requirement that the meeting must be an added node before user role determination).
    shims:
    - agent: false
      code_node_type: virtual-pure
      description: This node extracts a list of explicit constraints from the parent
        node's input string or a fallback core objective output for use by dependent
        workflow nodes.
      implementation: "def extract_constraints_from_parent_input(inputs: str, fallback_objective_output:\
        \ str) -> List[str]:\n    \"\"\"\n    This node extracts a list of explicit\
        \ constraints from the parent node's input string or a fallback core objective\
        \ output for use by dependent workflow nodes.\n\n    Args:\n        inputs:\
        \ Input parameter of type str\nfallback_objective_output: Input parameter\
        \ of type str\n\n    Returns:\n        List[str]: Output of type List[str]\n\
        \    \"\"\"\n    import re\n\n    def extract_constraints(text: str) -> List[str]:\n\
        \        # Attempt to extract constraints using common patterns.\n       \
        \ # Constraint markers: \"must\", \"should\", \"cannot\", \"have to\", \"\
        may not\", \"ensure\", etc.\n        # We'll try to match phrases like:\n\
        \        #   - \"must ...\"\n        #   - \"should not ...\"\n        # \
        \  - \"cannot ...\"\n        #   - etc.\n        constraint_patterns = [\n\
        \            r'\\bmust\\b[^.?!]*[.?!]',\n            r'\\bshould not\\b[^.?!]*[.?!]',\n\
        \            r'\\bshould\\b[^.?!]*[.?!]',\n            r'\\bhave to\\b[^.?!]*[.?!]',\n\
        \            r'\\bmay not\\b[^.?!]*[.?!]',\n            r'\\bcannot\\b[^.?!]*[.?!]',\n\
        \            r'\\bcan not\\b[^.?!]*[.?!]',\n            r'\\bmust not\\b[^.?!]*[.?!]',\n\
        \            r'\\bensure that\\b[^.?!]*[.?!]',\n            r'\\brequirement[s]?\
        \ is that\\b[^.?!]*[.?!]',\n            r'\\bit is required that\\b[^.?!]*[.?!]',\n\
        \        ]\n        constraints = []\n        for pat in constraint_patterns:\n\
        \            matches = re.findall(pat, text, re.IGNORECASE)\n            constraints.extend([m.strip()\
        \ for m in matches])\n        # Fallback: if none found, try to chunk input\
        \ using semicolon or bullet points as constraints.\n        if not constraints:\n\
        \            # Split by common separators/bullets\n            bullet_points\
        \ = re.split(r'\\n\\s*[-*\u2022]\\s*', text)\n            for item in bullet_points:\n\
        \                item = item.strip()\n                if not item:\n     \
        \               continue\n                # Heuristic: consider as constraint\
        \ if it contains 'must', 'should', etc.\n                if re.search(r'\\\
        b(must|should|have to|cannot|can not|must not|may not|required|requirement|ensure)\\\
        b', item, re.IGNORECASE):\n                    constraints.append(item)\n\
        \        # Clean up constraints\n        final_constraints = []\n        for\
        \ c in constraints:\n            # Remove trailing whitespace, bullet marks,\
        \ repeated spaces\n            c = re.sub(r'^[-*\u2022]\\s*', '', c).strip()\n\
        \            if c and c not in final_constraints:\n                final_constraints.append(c)\n\
        \        return final_constraints\n\n    # Try extracting from primary input\n\
        \    primary_constraints = extract_constraints(inputs)\n    if primary_constraints:\n\
        \        return primary_constraints\n    # Fallback: extract from objective\
        \ output\n    fallback_constraints = extract_constraints(fallback_objective_output)\n\
        \    return fallback_constraints\n"
      name: extract_constraints_from_parent_input
      nodes_depended_on: []
      nodes_dependent_on: []
      output_structure:
      - description: Output of type List[str]
        key: output
        type: List[str]
      - description: Input parameter of type str
        key: inputs
        type: str
      - description: Input parameter of type str
        key: fallback_objective_output
        type: str
      prd:
        bullets:
        - complexity: MEDIUM
          impact: Enables downstream nodes to systematically apply and map constraints,
            increasing clarity and automation.
          method: Use regular expressions or natural language processing to identify
            constraint patterns within the input string.
          reason: Ensures all constraints required for workflow decisions are isolated
            and available for downstream processing.
          text: Parse the primary input string to identify and extract constraints
            as discrete items.
        - complexity: LOW
          impact: Prevents loss of critical constraint information and improves workflow
            reliability.
          method: Check output of primary extraction; if null or empty, run the same
            extraction method over the fallback objective string.
          reason: Provides robustness in cases where the primary input is ambiguous
            or incomplete.
          text: Implement a fallback mechanism that attempts to extract constraints
            from the core objective output if primary extraction fails or yields none.
      prompt: Given the input string and the fallback objective output, extract a
        list of explicit constraints relevant to the workflow requirements for downstream
        use as List[str].
      shims: []
    - agent: false
      code_node_type: virtual-pure
      description: This node translates a set of workflow constraints and an objective
        statement into a list of specific, measurable success metrics aligned with
        those constraints.
      implementation: "def map_constraints_to_metrics(constraints: str, objective:\
        \ str) -> List[str]:\n    \"\"\"\n    This node translates a set of workflow\
        \ constraints and an objective statement into a list of specific, measurable\
        \ success metrics aligned with those constraints.\n\n    Args:\n        constraints:\
        \ Input parameter of type str\nobjective: Input parameter of type str\n\n\
        \    Returns:\n        List[str]: Output of type List[str]\n    \"\"\"\n \
        \   import re\n    from typing import List\n    \n    # Split constraints\
        \ into list (support bullet points, line breaks, or semicolons)\n    raw_constraints\
        \ = [c.strip() for c in re.split(r'[\\n\\r\\-\\*\\u2022;]+', constraints)\
        \ if c.strip()]\n\n    metrics = []\n    \n    for idx, constraint in enumerate(raw_constraints):\n\
        \        base_constraint = constraint\n        # Classify constraint type:\
        \ quantitative vs qualitative\n        quantitative_keywords = [\n       \
        \     'less than', 'at least', 'no more than', 'greater than', 'fewer than',\n\
        \            'between', 'range', 'must be', 'limit', 'threshold', 'target',\
        \ '%',\n            'more than', 'equal to', 'per day', 'per hour', 'within',\
        \ 'allowed', \"maximum\", \"minimum\", \"not exceed\", \"should not exceed\"\
        , \"no less than\"\n        ]\n        qualitative_keywords = [\n        \
        \    'ensure', 'make sure', 'confirm', 'verify', 'should be', 'must be',\n\
        \            'required to', 'subjective', 'review', 'policy', 'approval',\n\
        \            'compliant', 'aligned', 'appropriate', 'adherence', 'consistency',\n\
        \            'documentation', 'demonstrate', 'quality', 'accuracy', 'reliable',\
        \ 'timely', 'prompt', 'effective', 'customer satisfaction', 'feedback'\n \
        \       ]\n\n        constraint_lower = constraint.lower()\n        is_quant\
        \ = False\n        for kw in quantitative_keywords:\n            if kw in\
        \ constraint_lower:\n                is_quant = True\n                break\n\
        \n        is_qual = False\n        for kw in qualitative_keywords:\n     \
        \       if kw in constraint_lower:\n                is_qual = True\n     \
        \           break\n\n        # If both, prefer quantitative. If neither, default\
        \ to qualitative.\n        metric_type = 'quantitative' if is_quant else 'qualitative'\n\
        \n        # Generate metric text\n        if metric_type == 'quantitative':\n\
        \            # Heuristic: extract any numbers for metric target\n        \
        \    numbers = re.findall(r'\\d+[\\.]?\\d*', constraint)\n            target\
        \ = numbers[0] if numbers else '<specify target>'\n            metric_description\
        \ = (\n                f\"% of cases where constraint '{constraint}' is satisfied\
        \ in alignment with objective: {objective}. \"\n                f\"Target:\
        \ {target} (traced to constraint {idx+1}).\"\n            )\n        else:\n\
        \            metric_description = (\n                f\"Audit checklist for:\
        \ '{constraint}'. Track rate of compliance and periodic qualitative review\
        \ for alignment with objective: {objective}. \"\n                f\"Trace:\
        \ constraint {idx+1} ('{constraint}').\"\n            )\n\n        metrics.append(metric_description)\n\
        \n    return metrics\n"
      name: map_constraints_to_metrics
      nodes_depended_on: []
      nodes_dependent_on: []
      output_structure:
      - description: Output of type List[str]
        key: output
        type: List[str]
      - description: Input parameter of type str
        key: constraints
        type: str
      - description: Input parameter of type str
        key: objective
        type: str
      prd:
        bullets:
        - complexity: MEDIUM
          impact: This will facilitate clear downstream tracking of requirements compliance
            and provide foundational data for workflow auditing.
          method: Iterate through constraints (parsed as a list) and use template-based
            or rule-based logic to compose success metrics based on the semantic meaning
            of each constraint with direct reference to the objective.
          reason: Automated metric generation is vital to ensure every constraint
            is properly operationalized and measurable throughout the workflow.
          text: Parse and process each provided constraint to generate one or more
            directly mappable, objective-aligned success metrics.
        - complexity: LOW
          impact: Allows later stages or reporting to explain how each metric links
            back to key workflow requirements and constraints.
          method: Embed references to constraint text or identifiers in the metric
            description or metadata; maintain mapping metadata during generation.
          reason: Traceability is crucial for both debugging and demonstrating compliance
            with business or audit requirements.
          text: Ensure each resulting metric is formatted to be both actionable and
            traceable to its originating constraint and the core objective.
        - complexity: MEDIUM
          impact: "Improves the robustness of the metric mapping system, ensuring\
            \ all relevant constraints\u2014regardless of type\u2014are appropriately\
            \ represented."
          method: Apply language pattern matching or keyword-based categorization
            to determine if a generated metric should be quantitative or qualitative,
            and adjust the metric format accordingly.
          reason: Workflows may involve constraints that require numeric (quantitative)
            tracking or more subjective (qualitative) measurements.
          text: Support flexibility for both qualitative and quantitative metrics
            based on the language and type of each constraint.
      prompt: Given a set of constraints and a core objective statement, enumerate
        one or more measurable success metrics for each constraint, ensuring each
        metric directly reflects or enforces the constraint and is traceable back
        to both the constraint and the core objective.
      shims: []
    - agent: false
      code_node_type: virtual-pure
      description: This node generates a specific, measurable success metric that
        ensures a meeting with senior managers is scheduled and completed prior to
        determining user roles, based on the workflow's core objective and constraints.
      implementation: "def generate_meeting_governance_metric(core_objective: str,\
        \ constraints: str) -> str:\n    \"\"\"\n    This node generates a specific,\
        \ measurable success metric that ensures a meeting with senior managers is\
        \ scheduled and completed prior to determining user roles, based on the workflow's\
        \ core objective and constraints.\n\n    Args:\n        core_objective: Input\
        \ parameter of type str\nconstraints: Input parameter of type str\n\n    Returns:\n\
        \        str: Output of type str\n    \"\"\"\n    # --- PURE IMPLEMENTATION\
        \ ---\n    import re\n    \n    # Step 1: Extract key temporal and procedural\
        \ requirements for \"meeting with senior managers\" and \"user role determination\"\
        \n    text = core_objective + \" \" + constraints\n    text_lower = text.lower()\n\
        \    \n    # Look for temporal or ordering cues\n    role_keywords = [\n \
        \       r\"user role determination\",\n        r\"role assignment[s]?\",\n\
        \        r\"assign user roles\",\n        r\"determin(?:e|ation) of user roles\"\
        ,"
      name: generate_meeting_governance_metric
      nodes_depended_on: []
      nodes_dependent_on: []
      output_structure:
      - description: Output of type str
        key: output
        type: str
      - description: Input parameter of type str
        key: core_objective
        type: str
      - description: Input parameter of type str
        key: constraints
        type: str
      prd:
        bullets:
        - complexity: LOW
          impact: Aligns process automation and reporting with explicit governance
            needs, reducing risk of non-compliance.
          method: Parse the core objective and constraints using rule-based string
            analysis or keyword extraction to identify temporal and procedural relationships.
          reason: Ensures that the generated metric directly supports policy and procedural
            requirements dictated by the workflow and company governance standards.
          text: Extract key context from the core objective and constraints to understand
            exactly when and how the meeting with senior managers must occur relative
            to user role determination.
        - complexity: MEDIUM
          impact: Facilitates both human and automated verification of process compliance
            and supports downstream reporting requirements.
          method: Template the output string with parameterized placeholders for meeting
            scheduling and completion, referencing senior managers and role determination,
            and instantiate it with actual input values.
          reason: Clear articulation of this metric provides unambiguous criteria
            for workflow success and enables automated tracking or auditing.
          text: Formulate the governance success metric as a clear, actionable, and
            measurable statement using a recognized template (e.g., SMART).
        - complexity: LOW
          impact: Improves clarity and reduces confusion in success metric aggregation,
            enabling accurate process monitoring.
          method: Compare new metric string against provided or known set of metrics,
            using fuzzy matching or deduplication rules, before final output.
          reason: Prevents redundancy in success criteria and ensures each metric
            checks a unique aspect of the workflow.
          text: Validate that the generated metric is distinct and does not duplicate
            existing metric statements related to meetings or role assignments.
      prompt: Given a core workflow objective relating to meetings with senior managers
        and a set of workflow constraints, generate a clear, explicit, and measurable
        governance-oriented success metric that confirms a meeting with senior managers
        has been scheduled and completed before user roles are determined.
      shims: []
    - agent: false
      code_node_type: virtual-pure
      description: Structures and formats a list of success metrics according to a
        specified reporting template such as SMART or OKR for consistent and effective
        communication.
      implementation: "def structure_metrics_for_reporting(metrics: str, template:\
        \ str) -> List[str]:\n    \"\"\"\n    Structures and formats a list of success\
        \ metrics according to a specified reporting template such as SMART or OKR\
        \ for consistent and effective communication.\n\n    Args:\n        metrics:\
        \ Input parameter of type str\ntemplate: Input parameter of type str\n\n \
        \   Returns:\n        List[str]: Output of type List[str]\n    \"\"\"\n  \
        \  # --- PURE IMPLEMENTATION BEGIN ---\n    import re\n    from typing import\
        \ List\n\n    def parse_metrics_string(metrics: str) -> List[str]:\n     \
        \   \"\"\"Splits the metrics input string into a list of metric strings.\"\
        \"\"\n        if '\\n' in metrics:\n            metric_lines = [m.strip()\
        \ for m in metrics.split('\\n') if m.strip()]\n        elif ';' in metrics:\n\
        \            metric_lines = [m.strip() for m in metrics.split(';') if m.strip()]\n\
        \        else:\n            metric_lines = [m.strip() for m in metrics.split(',')\
        \ if m.strip()]\n        return metric_lines\n\n    # --- SMART Template Logic\
        \ ---\n    def structure_metric_smart(metric: str) -> str:\n        # Attempt\
        \ to extract fields from the metric using basic heuristics\n        # 1. Looks\
        \ for 'by <date>' or 'before <date>' for Deadline\n        # 2. Looks for\
        \ 'by <person>' or 'by <role>' for Responsibility\n        # 3. Otherwise,\
        \ assumes rest is Objective\n        result = {}\n        deadline = None\n\
        \        responsible = None\n        metric_wo_deadline = metric\n\n     \
        \   # Extract deadline (by/before ...)\n        deadline_match = re.search(r'\\\
        b(by|before) ([\\w\\s\\-,.]+?)([.;]|$)', metric, re.IGNORECASE)\n        if\
        \ deadline_match:\n            deadline = deadline_match.group(2).strip()\n\
        \            metric_wo_deadline = metric[:deadline_match.start()].strip()\
        \ + metric[deadline_match.end():].strip()\n\n        # Extract responsible\
        \ (by <person>) if not used for deadline\n        responsible_match = re.search(r'\\\
        b(by) ([A-Z][\\w\\s]+)([.;]|$)', metric_wo_deadline, re.IGNORECASE)\n    \
        \    if responsible_match:\n            responsible = responsible_match.group(2).strip()\n\
        \            metric_wo_deadline = metric_wo_deadline[:responsible_match.start()].strip()\
        \ + metric_wo_deadline[responsible_match.end():].strip()\n\n        # Extract\
        \ measure: look for numbers or percentages\n        measure_match = re.search(r'(\\\
        d+(?:\\.\\d+)?%)|(\\d+(?:\\.\\d+)?(?: units| users| customers| points)?)',\
        \ metric)\n        if measure_match:\n            measure = measure_match.group(0).strip()\n\
        \        else:\n            measure = None\n\n        # Result structure\n\
        \        parts = []\n        # Objective\n        obj = metric_wo_deadline.strip()\n\
        \        if obj:\n            parts.append(f\"Objective: {obj}\")\n      \
        \  # Measurable criterion\n        if measure:\n            parts.append(f\"\
        Measure: {measure}\")\n        # Deadline\n        if deadline:\n        \
        \    parts.append(f\"Deadline: {deadline}\")\n        # Responsible\n    \
        \    if responsible:\n            parts.append(f\"Responsible: {responsible}\"\
        )\n\n        # SMART typically requires: Specific, Measurable, Achievable,\
        \ Relevant, Time-bound, so we validate that at least obj/measure/deadline\
        \ appear\n        # Compose string\n        return \" | \".join(parts) if\
        \ parts else metric.strip()\n\n    # --- OKR Template Logic ---\n    def structure_metric_okr(metric:\
        \ str) -> str:\n        # Try to split into Objective & Key Result using punctuation\
        \ and keywords\n        # Attempt to identify KR using '- ' or numbering,\
        \ else use heuristics\n        # e.g. \"Objective: Increase sign-ups | Key\
        \ Result: Achieve 5,000 new sign-ups by Q2\"\n        # Look for 'objective'\
        \ or 'key result' anchor words\n        obj = None\n        kr = None\n  \
        \      metric_lower = metric.lower()\n        if 'objective:' in metric_lower\
        \ and 'key result:' in metric_lower:\n            parts = re.split(r'objective:|key\
        \ result:', metric, flags=re.IGNORECASE)\n            if len(parts) >= 3:\n\
        \                obj = parts[1].strip(' .;:|')\n                kr = parts[2].strip('\
        \ .;:|')\n        else:\n            # Try splitting using dashes or numbering\n\
        \            obj_kr_split = re.split(r'(?:(?:-|\u2022|\\d\\.|\\*)\\s+)', metric,\
        \ maxsplit=1)\n            if len(obj_kr_split) == 2:\n                obj\
        \ = obj_kr_split[0].strip()\n                kr = obj_kr_split[1].strip()\n\
        \            else:\n                # If metric is short, treat as Objective,\
        \ or if contains 'by'/'achieve'/number, make it Key Result\n             \
        \   if re.search(r'(\\d+)|(%|by|increase|decrease|achieve|reach)', metric,\
        \ re.IGNORECASE):\n                    obj = None\n                    kr\
        \ = metric.strip()\n                else:\n                    obj = metric.strip()\n\
        \                    kr = None\n        parts = []\n        if obj:\n    \
        \        parts.append(f\"Objective: {obj}\")\n        if kr:\n           \
        \ parts.append(f\"Key Result: {kr}\")\n        return \" | \".join(parts)\
        \ if parts else metric.strip()\n\n    # --- Fallback logic: Return raw metric\
        \ ---\n    def structure_metric_generic(metric: str) -> str:\n        return\
        \ metric.strip()\n\n    # --- Template Registry (Strategy Pattern) ---\n \
        \   template_map = {\n        'SMART': structure_metric_smart,\n        'OKR':\
        \ structure_metric_okr\n    }\n\n    # Process input\n    metric_list = parse_metrics_string(metrics)\n\
        \    chosen_template = str(template).strip().upper()\n    structure_fn = template_map.get(chosen_template,\
        \ structure_metric_generic)\n    structured_metrics: List[str] = [structure_fn(m)\
        \ for m in metric_list]\n    return structured_metrics\n"
      name: structure_metrics_for_reporting
      nodes_depended_on: []
      nodes_dependent_on: []
      output_structure:
      - description: Output of type List[str]
        key: output
        type: List[str]
      - description: Input parameter of type str
        key: metrics
        type: str
      - description: Input parameter of type str
        key: template
        type: str
      prd:
        bullets:
        - complexity: MEDIUM
          impact: Results in well-structured, clear, and actionable success metrics
            that align with reporting best practices and organizational expectations.
          method: Use conditional logic to check the template type and employ template-driven
            formatting functions or use mapping tables for each recognized format.
          reason: Standardizing metrics according to widely recognized templates improves
            clarity, facilitates stakeholder understanding, and ensures consistency
            across reports.
          text: Implement logic to map and structure each metric from the input list
            according to the chosen reporting template (e.g., ensuring that all SMART
            criteria or OKR format rules are applied).
        - complexity: MEDIUM
          impact: Reduces ambiguity, fosters accountability, and enables direct mapping
            from success criteria to workflow execution or auditing processes.
          method: Parse and decompose each metric string, then reconstruct them to
            include template-mandated fields; apply validation rules to check completeness.
          reason: Comprehensive, template-driven outputs prevent misinterpretation
            and make metrics actionable for all audiences.
          text: Ensure the outputted metrics include required structural components
            such as objective statements, measurement criteria, deadlines, or responsible
            parties where dictated by the template.
        - complexity: LOW
          impact: Future-proofs the reporting infrastructure, lowering long-term maintenance
            costs and improving adaptability.
          method: Encapsulate template formatting logic using a strategy pattern or
            modular helper classes/functions for each template type.
          reason: As reporting standards evolve, easy extensibility allows system
            maintainers to introduce new templates without large refactoring.
          text: Support future extensibility to accommodate additional reporting templates
            with minimal code changes.
      prompt: Format the provided list of success metrics according to the specified
        reporting template (e.g., SMART, OKR), standardizing their structure, phrasing,
        and relevant metadata as necessary for clear communication in reports.
      shims: []
    - agent: false
      code_node_type: virtual-pure
      description: This shim enriches a provided list of workflow success metrics
        by generating both quantitative and qualitative success criteria, ensuring
        alignment with specified constraints and the core objective.
      implementation: "def generate_quant_qual_metrics(metrics: str, constraints:\
        \ str, core_objective: str) -> List[str]:\n    \"\"\"\n    This shim enriches\
        \ a provided list of workflow success metrics by generating both quantitative\
        \ and qualitative success criteria, ensuring alignment with specified constraints\
        \ and the core objective.\n\n    Args:\n        metrics: Input parameter of\
        \ type str\nconstraints: Input parameter of type str\ncore_objective: Input\
        \ parameter of type str\n\n    Returns:\n        List[str]: Output of type\
        \ List[str]\n    \"\"\"\n    # --- PURE IMPLEMENTATION: Replacing SHIMS with\
        \ real logic ---\n    import re\n    from typing import List\n\n    # Helper:\
        \ Split semicolon, newline, or numbered/bulleted lists into a list of metric\
        \ strings\n    def parse_metrics(text: str) -> List[str]:\n        if not\
        \ text.strip():\n            return []\n        # Try splitting numbered/bulleted\
        \ or semi-colon/newline lists\n        split_lines = re.split(r'\\s*(?:[-*]|\\\
        d+\\.|\\n|;)\\s+', text)\n        # Remove empties and trim\n        return\
        \ [line.strip() for line in split_lines if line.strip()]\n\n    # Helper:\
        \ Yield constraint -> metric type/description skeletons\n    def map_constraint_to_metrics(constraint:\
        \ str, core_objective: str) -> List[str]:\n        \"\"\"\n        Returns\
        \ a list of metric templates (quantitative and qualitative) for a constraint\n\
        \        \"\"\"\n        results = []\n        c = constraint.strip()\n  \
        \      if not c:\n            return []\n        # Naive keyword mapping for\
        \ typical constraint patterns\n        # Quantitative Rules\n        quant_templates\
        \ = [\n            (r'time|duration|deadline|within (\\d+|\\w+) (minutes|hours|days|weeks)',\
        \ lambda m: f\"% of tasks completed on time ({c})\"),\n            (r'budget|cost|expense|under\
        \ \\$', lambda m: f\"% of budget used ({c})\"),\n            (r'accuracy|error\
        \ rate|correct|precision', lambda m: f\"Accuracy rate for {core_objective}\
        \ ({c})\"),\n            (r'completion|done|finish|achieve', lambda m: f\"\
        Task completion rate under {c}\"),\n            (r'number of|at least|no more\
        \ than|maximum|minimum', lambda m: f\"# of times {c} is met\"),\n        ]\n\
        \        # Qualitative Rules\n        qual_templates = [\n            (r'satisfaction|feedback|user\
        \ happiness|user experience|stakeholder', lambda m: f\"User/stakeholder satisfaction\
        \ score re: {c}\"),\n            (r'ease|usability|understand|clarity', lambda\
        \ m: f\"Usability rating for {core_objective} ({c})\"),\n        ]\n     \
        \   matched_quant = None\n        for pattern, tmpl in quant_templates:\n\
        \            if re.search(pattern, c, flags=re.IGNORECASE):\n            \
        \    matched_quant = tmpl(None)\n                break\n        if not matched_quant:\n\
        \            matched_quant = f\"% of times {c} is fulfilled (quantitative)\"\
        \n        results.append(matched_quant)\n        matched_qual = None\n   \
        \     for pattern, tmpl in qual_templates:\n            if re.search(pattern,\
        \ c, flags=re.IGNORECASE):\n                matched_qual = tmpl(None)\n  \
        \              break\n        if not matched_qual:\n            matched_qual\
        \ = f\"Stakeholder agreement that {c} was met (qualitative)\"\n        results.append(matched_qual)\n\
        \        return results\n\n    # Helper: Validate and standardize metric sentence\n\
        \    def standardize_metric(metric: str) -> str:\n        cleaned = metric.strip().capitalize()\n\
        \        if not cleaned.endswith('.'):\n            cleaned = cleaned + '.'\n\
        \        # Shorten multiple spaces\n        cleaned = re.sub(r'\\s+', ' ',\
        \ cleaned)\n        return cleaned\n\n    # Parse supplied metrics and constraints\n\
        \    existing_metrics = parse_metrics(metrics)\n    constraint_list = parse_metrics(constraints)\n\
        \n    # Copy & format supplied metrics\n    generated_metrics = [standardize_metric(m)\
        \ for m in existing_metrics]\n\n    # Map constraints to at least one quant\
        \ and one qual metric each\n    for c in constraint_list:\n        c_metrics\
        \ = map_constraint_to_metrics(c, core_objective)\n        # Only add if not\
        \ already represented\n        for cm in c_metrics:\n            if all(cm.lower()\
        \ not in em.lower() for em in generated_metrics):\n                generated_metrics.append(standardize_metric(cm))\n\
        \n    # Coverage for core_objective itself (if not already explicit)\n   \
        \ if core_objective.strip():\n        co_quant = f\"% of {core_objective}\
        \ achieved as per goals.\"\n        co_qual = f\"User/stakeholder perceived\
        \ value of {core_objective}.\"\n        for co_metric in (co_quant, co_qual):\n\
        \            if all(co_metric.lower()[:10] not in gm.lower() for gm in generated_metrics):\n\
        \                generated_metrics.append(standardize_metric(co_metric))\n\
        \n    # Final filter: Remove duplicates (case-insensitive)\n    seen = set()\n\
        \    deduped = []\n    for m in generated_metrics:\n        key = m.lower()\n\
        \        if key not in seen:\n            seen.add(key)\n            deduped.append(m)\n\
        \n    return deduped\n"
      name: generate_quant_qual_metrics
      nodes_depended_on: []
      nodes_dependent_on: []
      output_structure:
      - description: Output of type List[str]
        key: output
        type: List[str]
      - description: Input parameter of type str
        key: metrics
        type: str
      - description: Input parameter of type str
        key: constraints
        type: str
      - description: Input parameter of type str
        key: core_objective
        type: str
      prd:
        bullets:
        - complexity: MEDIUM
          impact: Results in a complete, goal-aligned set of metrics that supports
            broader evaluation and decision-making.
          method: Implement rule-based or template-driven matching to classify and
            supplement the metrics list, leveraging keyword analysis and mapping guidelines
            for typical workflow scenarios.
          reason: "Ensures that all key aspects of success measurement, including\
            \ both objective (quantitative) and subjective (qualitative) criteria,\
            \ are represented according to the workflow\u2019s needs."
          text: Analyze the provided metrics in context of the specified constraints
            and core objective to identify gaps in quantitative and qualitative coverage.
        - complexity: MEDIUM
          impact: Provides traceability and accountability for workflow outcomes,
            improving transparency for reviewers and stakeholders.
          method: Apply mapping functions or predefined metric templates that associate
            common constraint types with standard metric statements (e.g., completion
            rates for quantitative, satisfaction surveys for qualitative).
          reason: Directly aligns each constraint with concrete indicators of success,
            preventing untracked requirements.
          text: For each constraint, generate at least one quantitative and one qualitative
            metric where applicable, ensuring all inputs are mapped to measurable
            criteria.
        - complexity: LOW
          impact: Increases the utility and reusability of metrics across related
            workflow nodes and reporting tools.
          method: Utilize string formatting and validation logic to standardize metric
            descriptions and ensure completeness.
          reason: Ensures clarity, prevents ambiguity, and streamlines downstream
            reporting and automation processes.
          text: Format all generated metrics consistently and validate that they are
            actionable and relevant to the core objective and constraints.
      prompt: Given the current list of metrics, associated constraints, and the core
        objective, generate a comprehensive set of success metrics that includes both
        quantitative (measurable, numeric) and qualitative (descriptive, satisfaction-based)
        criteria, ensuring each aligns with the stated constraints and core objective
        of the workflow.
      shims: []
    - agent: false
      code_node_type: virtual-pure
      description: This node enriches a list of success metrics by appending traceability
        information that links each metric to the originating constraint or feature
        and the specific workflow node reference.
      implementation: "def add_traceability_to_metrics(metrics: str, constraints:\
        \ str, node_reference: str) -> List[str]:\n    \"\"\"\n    This node enriches\
        \ a list of success metrics by appending traceability information that links\
        \ each metric to the originating constraint or feature and the specific workflow\
        \ node reference.\n\n    Args:\n        metrics: Input parameter of type str\n\
        constraints: Input parameter of type str\nnode_reference: Input parameter\
        \ of type str\n\n    Returns:\n        List[str]: Output of type List[str]\n\
        \    \"\"\"\n    # --- PURE IMPLEMENTATION ---\n    import json\n    from\
        \ typing import Any\n\n    def _normalize_to_list(input_str: str) -> list:\n\
        \        \"\"\"\n        Try to parse input_str as JSON array.\n        If\
        \ fails, try splitting by newlines or commas and return a cleaned list.\n\
        \        \"\"\"\n        try:\n            val = json.loads(input_str)\n \
        \           if isinstance(val, list):\n                return val\n      \
        \      elif isinstance(val, str):\n                # input_str is a string\
        \ quoted, not a list\n                return [val]\n        except Exception:\n\
        \            # Not JSON; try delimiting\n            # First newlines, fallback\
        \ to commas\n            if '\\n' in input_str:\n                lines = [x.strip()\
        \ for x in input_str.split('\\n') if x.strip()]\n                if lines:\n\
        \                    return lines\n            # else try commas\n       \
        \     items = [x.strip() for x in input_str.split(',') if x.strip()]\n   \
        \         return items if items else [input_str.strip()] if input_str.strip()\
        \ else []\n\n    def _normalize_constraint_item(item: Any) -> dict:\n    \
        \    \"\"\"\n        Try to ensure each constraint item is a dict with at\
        \ least an 'id' or 'name' or summary fields for mapping.\n        \"\"\"\n\
        \        if isinstance(item, dict):\n            return item\n        # Try\
        \ to parse as JSON\n        try:\n            val = json.loads(item)\n   \
        \         if isinstance(val, dict):\n                return val\n        \
        \    elif isinstance(val, str):\n                return {\"summary\": val}\n\
        \        except Exception:\n            pass\n        # Not JSON, just treat\
        \ as summary\n        return {\"summary\": str(item)}\n\n    def _normalize_metric_item(item:\
        \ Any) -> dict:\n        \"\"\"\n        Try to parse as dict; else as plain\
        \ text.\n        \"\"\"\n        if isinstance(item, dict):\n            return\
        \ item\n        try:\n            val = json.loads(item)\n            if isinstance(val,\
        \ dict):\n                return val\n            elif isinstance(val, str):\n\
        \                return {\"metric\": val}\n        except Exception:\n   \
        \         pass\n        # Not JSON, treat as plain metric text\n        return\
        \ {\"metric\": str(item)}\n\n    metrics_list = _normalize_to_list(metrics)\n\
        \    constraints_list = _normalize_to_list(constraints)\n\n    # Further parse\
        \ constraints into structured dicts for lookup/mapping\n    structured_constraints\
        \ = [_normalize_constraint_item(c) for c in constraints_list]\n    structured_metrics\
        \ = [_normalize_metric_item(m) for m in metrics_list]\n\n    # For best effort\
        \ mapping: try to assign each metric to the corresponding constraint by index,\
        \ else fallback\n    annotated_metrics = []\n    for idx, metric in enumerate(structured_metrics):\n\
        \        # Find corresponding constraint (by index)\n        constraint =\
        \ structured_constraints[idx] if idx < len(structured_constraints) else None\n\
        \        # Extract identifier or summary for constraint\n        constraint_desc\
        \ = None\n        if constraint:\n            for key in [\"id\", \"name\"\
        , \"summary\", \"constraint\", \"feature\"]:\n                if key in constraint:\n\
        \                    constraint_desc = str(constraint[key])\n            \
        \        break\n            if not constraint_desc:\n                constraint_desc\
        \ = str(constraint)\n        else:\n            constraint_desc = \"N/A\"\n\
        \n        # Get metric main text\n        metric_text = None\n        for\
        \ key in [\"metric\", \"text\", \"description\", \"summary\"]:\n         \
        \   if key in metric:\n                metric_text = str(metric[key])\n  \
        \              break\n        if not metric_text:\n            # Fallback\
        \ to string form\n            metric_text = str(metric)\n        # Format\
        \ annotated string\n        annotated = f\"Metric: {metric_text} [constraint:\
        \ {constraint_desc}, node: {node_reference}]\"\n        annotated_metrics.append(annotated)\n\
        \n    return annotated_metrics\n"
      name: add_traceability_to_metrics
      nodes_depended_on: []
      nodes_dependent_on: []
      output_structure:
      - description: Output of type List[str]
        key: output
        type: List[str]
      - description: Input parameter of type str
        key: metrics
        type: str
      - description: Input parameter of type str
        key: constraints
        type: str
      - description: Input parameter of type str
        key: node_reference
        type: str
      prd:
        bullets:
        - complexity: LOW
          impact: Traceability enhances the system's governance, eases debugging,
            and provides clear lineage for stakeholders reviewing compliance with
            constraints or feature requests.
          method: 'Iterate over the metrics, locate the relevant constraint/feature
            using simple mapping or lookup, and format each metric string with appended
            or structured traceability metadata, e.g., as ''Metric: <text> [constraint:
            <id or summary>, node: <node_reference>]''.'
          reason: This ensures that any metric reported or evaluated later can be
            traced back to both its logical origin (such as a specific requirement
            or feature) and the workflow node it emerged from, supporting accountability
            and auditability.
          text: For each input metric, append or embed information explicitly linking
            it to the corresponding originating constraint or feature as well as the
            source node reference.
        - complexity: MEDIUM
          impact: This ensures robust interoperability regardless of how upstream
            nodes structure their outputs.
          method: Implement pre-processing logic to normalize metric and constraint
            inputs (e.g., parsing JSON strings, splitting lists, etc.) before annotation,
            using defensive coding techniques to handle edge cases.
          reason: The metrics and constraints may be passed in differently by upstream
            processes or tools.
          text: Handle varying input structures for metrics and constraints, supporting
            both plain-text and structured (e.g., JSON or delimited) representations
            to maintain flexibility.
      prompt: Given a list of metrics, a description of relevant constraints, and
        a node reference, return a new list of metrics where each item is annotated
        or linked to its originating constraint or feature and the node_reference,
        ensuring all metrics are traceable to their source within the workflow.
      shims: []
    - agent: false
      code_node_type: virtual-pure
      description: Identifies which constraints from the input list have not been
        explicitly mapped to any of the provided success metrics.
      implementation: "def find_unmapped_constraints(constraints: str, metrics: str)\
        \ -> List[str]:\n    \"\"\"\n    Identifies which constraints from the input\
        \ list have not been explicitly mapped to any of the provided success metrics.\n\
        \n    Args:\n        constraints: Input parameter of type str\nmetrics: Input\
        \ parameter of type str\n\n    Returns:\n        List[str]: Output of type\
        \ List[str]\n    \"\"\"\n    import json\n    import re\n\n    def parse_items(text):\n\
        \        text = text.strip()\n        if not text:\n            return []\n\
        \        # Try to parse as JSON list\n        try:\n            items = json.loads(text)\n\
        \            if isinstance(items, list):\n                return [str(item).strip()\
        \ for item in items]\n            # If it's a dict or string (not list), treat\
        \ as fallback\n        except Exception:\n            pass\n        # Fallback:\
        \ parse as line/semicolon/comma list\n        if '\\n' in text:\n        \
        \    items = [s.strip() for s in text.split('\\n') if s.strip()]\n       \
        \ elif ';' in text:\n            items = [s.strip() for s in text.split(';')\
        \ if s.strip()]\n        elif ',' in text:\n            items = [s.strip()\
        \ for s in text.split(',') if s.strip()]\n        else:\n            items\
        \ = [text]\n        return items\n\n    constraint_list = parse_items(constraints)\n\
        \    metric_list = parse_items(metrics)\n\n    unmapped = []\n    for constraint\
        \ in constraint_list:\n        found = False\n        constraint_lower = constraint.strip().lower()\n\
        \        # Remove punctuation for more robust matching\n        constraint_nopunct\
        \ = re.sub(r'[^a-z0-9 ]+', '', constraint_lower)\n        for metric in metric_list:\n\
        \            metric_lower = metric.strip().lower()\n            metric_nopunct\
        \ = re.sub(r'[^a-z0-9 ]+', '', metric_lower)\n            # Direct or substring\
        \ match (case-insensitive, lenient)\n            if constraint_lower in metric_lower\
        \ or constraint_nopunct in metric_nopunct:\n                found = True\n\
        \                break\n            # Lenient fuzzy-like match: Do all words\
        \ in constraint appear in metric?\n            constraint_words = set(constraint_nopunct.split())\n\
        \            metric_words = set(metric_nopunct.split())\n            # If\
        \ most words overlap, consider as matched\n            if constraint_words\
        \ and (len(constraint_words & metric_words) >= max(1, len(constraint_words)//2)):\n\
        \                found = True\n                break\n        if not found:\n\
        \            unmapped.append(constraint)\n    return unmapped\n"
      name: find_unmapped_constraints
      nodes_depended_on: []
      nodes_dependent_on: []
      output_structure:
      - description: Output of type List[str]
        key: output
        type: List[str]
      - description: Input parameter of type str
        key: constraints
        type: str
      - description: Input parameter of type str
        key: metrics
        type: str
      prd:
        bullets:
        - complexity: LOW
          impact: Ensures the function can reliably process input regardless of serialization
            format.
          method: Implement flexible parsing using standard string manipulation techniques
            or JSON deserialization.
          reason: Parsing is required to convert string inputs into comparable collections,
            enabling accurate matching.
          text: Parse the input constraints and metrics into structured lists to enable
            effective comparison.
        - complexity: MEDIUM
          impact: Enables the system to surface unmet constraints, increasing transparency
            and quality assurance.
          method: For each constraint, perform a case-insensitive substring or semantic
            match against all metrics, possibly leveraging simple NLP techniques for
            fuzzy matching.
          reason: This ensures comprehensive coverage of requirements and flags gaps
            in mapping.
          text: Check each constraint to determine if it is explicitly or implicitly
            addressed by any success metric.
        - complexity: LOW
          impact: Supports auditability and workflow traceability by ensuring no constraints
            are overlooked.
          method: Aggregate constraints with no corresponding mapping into a list
            and return this as the output.
          reason: Providing the unmapped constraints enables downstream logic to raise
            warnings or enforce resolution.
          text: Return a list of all constraints that remain unmapped after comparison.
      prompt: Given a list of constraints and a list of success metrics, identify
        and return all constraints that are not clearly addressed or mapped by any
        of the success metrics.
      shims: []
    - agent: false
      code_node_type: virtual-pure
      description: This node checks for any constraints that could not be mapped to
        success metrics, and either logs the unmapped constraints or raises an error
        depending on the context or configuration.
      implementation: "def log_or_raise_unmapped_constraints(unmapped_constraints:\
        \ str) -> str:\n    \"\"\"\n    This node checks for any constraints that\
        \ could not be mapped to success metrics, and either logs the unmapped constraints\
        \ or raises an error depending on the context or configuration.\n\n    Args:\n\
        \        unmapped_constraints: Input parameter of type str\n\n    Returns:\n\
        \        str: Output of type Any\n    \"\"\"\n    import os\n    import logging\n\
        \    from typing import List\n\n    # --- PURE IMPLEMENTATION ---\n    # Simulate\
        \ configuration: check environment variable or app-level config\n    # If\
        \ UNMAPPED_CONSTRAINT_SEVERITY is 'error', raise; otherwise, log as warning\n\
        \    severity = os.environ.get('UNMAPPED_CONSTRAINT_SEVERITY', 'warning').lower()\n\
        \n    class UnmappedConstraintError(Exception):\n        pass\n\n    # Prepare\
        \ constraints list from input string\n    def parse_constraints(constraints_str:\
        \ str) -> List[str]:\n        # Accepts comma or newline as separator. Remove\
        \ empty entries after split and strip each.\n        if not constraints_str:\n\
        \            return []\n        separators = ['\\n', ',']\n        text =\
        \ constraints_str\n        for sep in separators:\n            text = text.replace(sep,\
        \ '\\n')\n        return [c.strip() for c in text.split('\\n') if c.strip()]\n\
        \n    constraints = parse_constraints(unmapped_constraints)\n\n    # Format\
        \ message\n    if not constraints:\n        message = \"No unmapped constraints\
        \ found.\"\n        # Optionally log (safely handle logger absence)\n    \
        \    logging.warning(message)\n        return message\n    \n    message_lines\
        \ = [\n        f\"The following {len(constraints)} constraints could not be\
        \ mapped to success metrics:\",\n    ]\n    for i, c in enumerate(constraints,\
        \ 1):\n        message_lines.append(f\"  {i}. {c}\")\n    message_lines.append(\"\
        \\nPossible causes: constraint not supported, typo in name, or missing mapping.\"\
        )\n    message_lines.append(\"Suggestions: verify names, check mapping configuration,\
        \ consult documentation.\")\n    full_message = '\\n'.join(message_lines)\n\
        \n    if severity == 'error':\n        # Raise custom error\n        raise\
        \ UnmappedConstraintError(full_message)\n        # Optionally, the following\
        \ line would be unreachable\n        # return f\"Raised error due to {len(constraints)}\
        \ unmapped constraints.\"\n    else:\n        # Log warning\n        logging.warning(full_message)\n\
        \        return f\"Logged {len(constraints)} unmapped constraints as warning.\"\
        \n"
      name: log_or_raise_unmapped_constraints
      nodes_depended_on: []
      nodes_dependent_on: []
      output_structure:
      - description: Output of type Any
        key: output
        type: str
      - description: Input parameter of type str
        key: unmapped_constraints
        type: str
      prd:
        bullets:
        - complexity: LOW
          impact: Prevents silent failures in constraint mapping and ensures issues
            can be tracked or surfaced as needed.
          method: Check configuration or input parameters for a severity flag; use
            standard logging functions to log or raise a custom exception for errors.
          reason: Enables flexible error handling policies suited to deployment or
            development environments.
          text: Implement a conditional branch to determine whether unmapped constraints
            should be logged as warnings or raised as blocking errors based on configuration
            settings or runtime flags.
        - complexity: LOW
          impact: Improves transparency and debuggability of the metrics mapping pipeline.
          method: Iterate through the list of unmapped constraints and assemble a
            descriptive string or structured message before logging or raising.
          reason: Provides actionable feedback to developers or users, facilitating
            quick diagnosis and correction.
          text: Format unmapped constraints into a clear, human-readable message listing
            each unmapped constraint and possible causes or suggestions for remediation.
        - complexity: LOW
          impact: Improves system observability and allows for conditional branching
            downstream.
          method: Set a standardized return message string reflecting the code path
            executed; include in the node output.
          reason: Provides downstream nodes with an explicit record of the error-handling
            result.
          text: Return a status output indicating the action taken (e.g., 'Logged
            N unmapped constraints' or 'Raised error due to unmapped constraints').
      prompt: Check the provided list of unmapped constraints; if the list is not
        empty, either log detailed information about each unmapped constraint or raise
        an exception to halt execution based on severity and context requirements.
      shims: []
  list_primary_user_roles:
    agent: false
    code_node_type: pure
    description: Determine all primary user roles or personas that will interact with
      the workflow. As a prerequisite, ensure that a node for a meeting with senior
      managers is explicitly added, and then include this as a primary user interaction
      when relevant. Capture a clear, distinct list of primary user roles, without
      making assumptions about implied or overlapping user types.
    implementation: "from pydantic import BaseModel, Field\nfrom typing import List\n\
      \n\nclass IdentifyCoreObjectiveOutput(BaseModel):\n    \"\"\"Pydantic model\
      \ for identify_core_objective node outputs.\"\"\"\n    core_objective: str =\
      \ Field(..., description=\"The primary objective statement for the workflow\
      \ as described in the requirements, reflecting the need to include a meeting\
      \ with senior managers prior to determining user roles, and noting that constraints\
      \ should be available for use by dependent nodes, such as those identifying\
      \ success metrics.\")\n\n\nclass ListPrimaryUserRolesOutput(BaseModel):\n  \
      \  \"\"\"Pydantic model for list_primary_user_roles node outputs.\"\"\"\n  \
      \  user_roles: List[str] = Field(..., description=\"List of primary user roles\
      \ or personas interacting with the workflow, explicitly incorporating the node\
      \ for a meeting with senior managers when applicable.\")\n\n\ndef list_primary_user_roles_fx(identify_core_objective_input:\
      \ IdentifyCoreObjectiveOutput, **kwargs) -> ListPrimaryUserRolesOutput:\n  \
      \  \"\"\"Determine all primary user roles or personas that will interact with\
      \ the workflow. As a prerequisite, ensure that a node for a meeting with senior\
      \ managers is explicitly added, and then include this as a primary user interaction\
      \ when relevant. Capture a clear, distinct list of primary user roles, without\
      \ making assumptions about implied or overlapping user types.\n\n    Args:\n\
      \        identify_core_objective_input: Input from the 'identify_core_objective'\
      \ node.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n \
      \       ListPrimaryUserRolesOutput: Object containing outputs for this node.\n\
      \    \"\"\"\n    # 1. Ensure/insert the senior manager meeting user role as\
      \ first step\n    senior_role: str = determine_senior_manager_meeting_role(core_objective=identify_core_objective_input.core_objective)\n\
      \n    # 2. Extract all explicit user roles from requirements and objective (excluding\
      \ senior meeting role until explicit inclusion step)\n    explicit_roles: List[str]\
      \ = extract_explicit_user_roles_from_requirements(\n        text=identify_core_objective_input.core_objective,\n\
      \        context_kwargs=kwargs\n    )\n\n    # 3. Insert or guarantee the senior\
      \ manager node/user role at the top\n    user_roles: List[str] = ensure_senior_manager_meeting_first(\n\
      \        senior_manager_role=senior_role, candidate_roles=explicit_roles\n \
      \   )\n\n    # 4. Validate and serialize as type-safe\n    validated_roles:\
      \ List[str] = validate_user_roles_type_safety(user_roles=user_roles)\n\n   \
      \ return ListPrimaryUserRolesOutput(user_roles=validated_roles)\n"
    name: list_primary_user_roles
    nodes_depended_on:
    - identify_core_objective
    nodes_dependent_on: []
    output_structure:
    - description: List of primary user roles or personas interacting with the workflow,
        explicitly incorporating the node for a meeting with senior managers when
        applicable.
      key: user_roles
      type: List[str]
    prd:
      bullets:
      - complexity: "LOW \u2013 Checking or inserting a node in the workflow graph\
          \ is conceptually simple but vital."
        impact: "HIGH \u2013 Prevents incorrect workflow sequencing and ensures downstream\
          \ tasks (user role determination, feature allocation) are only performed\
          \ after stakeholder alignment."
        method: Programmatically scan the workflow DAG structure for a node indicating
          the senior manager meeting; if absent, generate and insert this node as
          step zero using graph manipulation routines.
        reason: This step is mandated as a prerequisite for accurate sequencing and
          project governance, ensuring critical executive alignment and requirements
          validation before further role assignments.
        text: Verify and enforce that the workflow includes a node for scheduling
          and conducting a meeting with senior managers as the first explicit step
          before user role identification.
      - complexity: "LOW \u2013 Simple inclusion logic, but must be systematic to\
          \ avoid accidental omission."
        impact: "MEDIUM \u2013 This enforces explicitness and prevents accidental\
          \ omission of a vertex critical to compliance and review."
        method: Add a dedicated entry for the 'senior manager meeting' or a relevant
          proxy persona (e.g., 'Senior Manager Participant') into the user_roles list;
          ensure this entry appears before other roles.
        reason: Treating the meeting as a user role makes workflow visibility clear
          for all consumers and ensures it is referenced in subsequent workflow documentation
          and QA.
        text: Explicitly model the meeting with senior managers as a user interaction
          node (i.e., as a primary user role) in the list of user roles interacting
          with the workflow.
      - complexity: "MEDIUM \u2013 May require NLP parsing or a strict requirements\
          \ scan to prevent missed roles."
        impact: "HIGH \u2013 Critical for the validity and accuracy of the workflow\u2019\
          s human factors and downstream handoff."
        method: Perform a pass over the structured requirements and core objective;
          apply regular expression or named entity recognition algorithms to extract
          explicit role mentions; avoid grouping unless described.
        reason: Maintains clarity, avoids assumption-driven errors, and enables granular
          stakeholder communication.
        text: Systematically extract all distinct primary user roles/personas from
          workflow requirements and the core objective, avoiding consolidation or
          inference unless roles are explicitly described.
      - complexity: "LOW \u2013 List reordering operation."
        impact: "MEDIUM \u2013 Reduces downstream ambiguity for consumers or API integrators\
          \ parsing the output."
        method: Implement a pre-insertion or post-processing sort/insert step to guarantee
          the senior manager meeting role is at index zero in user_roles.
        reason: Sequencing matters to communicate the process flow and ensure proper
          upstream/downstream role dependencies.
        text: Order user_roles so that the 'senior manager meeting' (or its representation)
          appears at the top of the list, enforcing workflow order and clarifying
          its role as a gatekeeper step.
      - complexity: "LOW \u2013 Standard serialization/type enforcement."
        impact: "HIGH \u2013 Ensures workflow DAG consistency and supports validation/testing."
        method: Generate and emit user_roles as a strictly validated List[str], using
          type-checking libraries where appropriate.
        reason: Facilitates integration and error-free downstream consumption by enforcing
          data expectations.
        text: Output the user_roles list in a type-safe structure (e.g., List[str])
          matching the node's output contract and aligning with elemental system data
          typing.
    prompt: First, ensure that a node representing a meeting with senior managers
      is included as an explicit step prior to listing user roles. Then, analyze the
      workflow's core objective and requirements to systematically list all distinct
      user roles or personas who will directly interact with or benefit from the workflow.
      Explicitly include the role or interaction of a meeting with senior managers
      if it has not already been incorporated. Focus on capturing only clear, stated
      roles and avoid inferring or merging user types unless explicitly described.
      Make certain the meeting with senior managers is treated as a distinct and primary
      user interaction node.
    shims:
    - agent: false
      code_node_type: virtual-pure
      description: This shim determines and outputs the appropriate user role label
        or identifier for a required meeting with senior managers, based on the core
        workflow objective.
      implementation: "def determine_senior_manager_meeting_role(core_objective: str)\
        \ -> str:\n    \"\"\"\n    This shim determines and outputs the appropriate\
        \ user role label or identifier for a required meeting with senior managers,\
        \ based on the core workflow objective.\n\n    Args:\n        core_objective:\
        \ Input parameter of type str\n\n    Returns:\n        str: Output of type\
        \ str\n    \"\"\"\n    # --- PURE IMPLEMENTATION ---\n    import re\n    \n\
        \    # List of regex patterns and synonyms covering possible references to\
        \ senior manager meetings\n    SENIOR_MGR_SYNONYMS = [\n        r'senior manager(?:s)?',\n\
        \        r'senior management',\n        r'(?:executive|leadership|director|board)[-\
        \ ]?(?:review|meeting|session|discussion|summit|briefing|sync|forum|gathering|update|panel)',\n\
        \        r'(?:management|executive) team(?:s)?',\n        r'C-level',\n  \
        \      r'senior exec',\n        r'executive committee',\n        r'(?:strategic|planning)\
        \ meeting',\n        r'top management',\n        r'leadership group',\n  \
        \      r'executive leadership',\n        r'strategy session',\n        r'senior\
        \ leader(?:s)?',\n    ]\n    \n    # Compile into a single regex for matching\n\
        \    role_regex = re.compile(r'(' + r'|'.join(SENIOR_MGR_SYNONYMS) + r')',\
        \ re.IGNORECASE)\n    \n    # Search for a match in the core objective text\n\
        \    match = role_regex.search(core_objective)\n    if match:\n        # Standardized\
        \ role label for any match relating to senior manager/executive meetings\n\
        \        return 'Senior Manager Meeting'\n    else:\n        # Optionally,\
        \ could raise an error or return a generic label\n        return 'Senior Manager\
        \ Meeting'  # Default/fallback as per PRD: ensure explicit label even if only\
        \ implied\n"
      name: determine_senior_manager_meeting_role
      nodes_depended_on: []
      nodes_dependent_on: []
      output_structure:
      - description: Output of type str
        key: output
        type: str
      - description: Input parameter of type str
        key: core_objective
        type: str
      prd:
        bullets:
        - complexity: LOW
          impact: Guarantees unambiguous identification of the senior manager meeting,
            preventing omission or duplication in user roles.
          method: Use simple string parsing or rule-based logic to find references
            to senior management and assign a canonical role label like 'Senior Manager
            Meeting' or similar.
          reason: A consistent and explicit role name is needed to ensure downstream
            nodes correctly identify and include this user requirement.
          text: Parse the core objective to extract context and generate a standardized
            user role label for the senior manager meeting.
        - complexity: MEDIUM
          impact: Prevents errors or omissions when different terminologies are used,
            improving the workflow's reliability.
          method: Develop a synonyms list or keyword matching, potentially using pattern
            matching or NLP-based approaches for reliability.
          reason: Requirements may describe the senior manager meeting in various
            terms, requiring adaptability for robust extraction.
          text: Implement logic to handle variations or synonyms (e.g., 'executive
            review', 'leadership meeting') when identifying the senior manager role.
      prompt: Based on the workflow's core objective, generate a string that clearly
        and specifically identifies the user role representing a senior manager meeting
        to be incorporated into the user roles list.
      shims: []
    - agent: false
      code_node_type: virtual-pure
      description: This function parses the provided requirements text and context
        to identify and return an explicit, type-safe list of user role names that
        are directly mentioned as participants in the workflow.
      implementation: "def extract_explicit_user_roles_from_requirements(text: str,\
        \ context_kwargs: str) -> List[str]:\n    \"\"\"\n    This function parses\
        \ the provided requirements text and context to identify and return an explicit,\
        \ type-safe list of user role names that are directly mentioned as participants\
        \ in the workflow.\n\n    Args:\n        text: Input parameter of type str\n\
        context_kwargs: Input parameter of type str\n\n    Returns:\n        List[str]:\
        \ Output of type List[str]\n    \"\"\"\n    import re\n    import json\n \
        \   from typing import List\n    \n    # --- 1. Define keyword dictionaries\
        \ and patterns for roles ---\n    # Common user role/persona tokens; extend\
        \ as needed\n    # These can be tailored, or sourced from context_kwargs if\
        \ provided\n    COMMON_ROLE_KEYWORDS = [\n        'user', 'admin', 'administrator',\
        \ 'editor', 'reviewer', 'approver', 'requester', \n        'customer', 'manager',\
        \ 'operator', 'moderator', 'analyst', 'auditor', 'contributor', \n       \
        \ 'agent', 'employee', 'client', 'applicant', 'participant', 'supervisor',\
        \ 'superuser',\n        'owner', 'guest', 'author', 'recipient', 'issuer',\
        \ 'reader', 'submitter', 'observer'\n    ]\n    # Build a regex pattern to\
        \ match these as whole words (case-insensitive)\n    role_pattern = re.compile(r'\\\
        b(' + '|'.join(COMMON_ROLE_KEYWORDS) + r')s?\\b', re.IGNORECASE)\n    \n \
        \   # --- 2. Try to enrich with additional roles from context_kwargs (if any)\
        \ ---\n    extra_context_roles = set()\n    if context_kwargs:\n        try:\n\
        \            context_obj = json.loads(context_kwargs)\n            # Try common\
        \ keys that might contain roles\n            likely_role_keys = ['roles',\
        \ 'user_roles', 'participants', 'actors']\n            for k in likely_role_keys:\n\
        \                if k in context_obj and isinstance(context_obj[k], list):\n\
        \                    for val in context_obj[k]:\n                        #\
        \ Only accept non-empty strings\n                        if isinstance(val,\
        \ str) and val.strip():\n                            extra_context_roles.add(val.strip())\n\
        \        except Exception:\n            pass  # Ignore invalid JSON or missing\
        \ context\n    \n    # --- 3. Parse the text for explicit user role/persona\
        \ mentions ---\n    raw_roles: List[str] = []\n\n    # Strategy: Look for\
        \ nouns matching role keywords, possibly in capitalized form or as proper\
        \ nouns.\n    # Optionally, enrich the pattern by looking for occurrences\
        \ like:\n    # \"the [role]\", \"as a[n] [role]\", \"by the [role]\", etc.\n\
        \    # For simplicity, collect all hits, dedupe later.\n    for match in role_pattern.finditer(text):\n\
        \        role = match.group(1)\n        # Normalize: lowercase singular\n\
        \        raw_roles.append(role.lower())\n\n    # --- 4. Attempt to capture\
        \ additional, custom or domain-specific roles\n    # Example: Roles may be\
        \ phrases like \"Data Entry Clerk\" or \"Chief Financial Officer\"\n    #\
        \ Heuristic: Look for patterns/phrases near typical user-mentioning constructions\n\
        \    # e.g. \"as a Data Entry Clerk\", \"the Request Author\", etc.\n    custom_role_pattern\
        \ = re.compile(r'(?:as an? |by the |the |by an? |for the |for an? )([A-Z][a-z]+(?:\
        \ [A-Z][a-z]+)*)')\n    for match in custom_role_pattern.finditer(text):\n\
        \        # Only keep if phrase is reasonably role-like and not a generic term\n\
        \        role_candidate = match.group(1)\n        # Filter out if it overlaps\
        \ clearly with previous matches\n        if role_candidate.lower() not in\
        \ COMMON_ROLE_KEYWORDS and len(role_candidate.split()) <= 5:\n           \
        \ raw_roles.append(role_candidate.strip())\n\n    # --- 5. Merge with context\
        \ roles, de-duplicate ---\n    all_roles_set = set([r.strip().lower() for\
        \ r in raw_roles if r.strip()]) | set([r.strip().lower() for r in extra_context_roles\
        \ if isinstance(r, str) and r.strip()])\n    # Post-process to make role names\
        \ more type-safe (e.g., capitalize words)\n    def canonicalize(role: str)\
        \ -> str:\n        role = role.strip()\n        # If it's a recognized built-in\
        \ role, canonicalize as lowercase\n        if role in COMMON_ROLE_KEYWORDS:\n\
        \            return role\n        # Else, Title Case for multiword custom\
        \ roles\n        return ' '.join([w.capitalize() for w in role.split()])\n\
        \    \n    roles_final = []\n    seen = set()\n    for r in all_roles_set:\n\
        \        if not r or not isinstance(r, str):\n            continue\n     \
        \   canon = canonicalize(r)\n        # Guard against empty or obviously invalid\
        \ names\n        if canon and canon.lower() not in seen:\n            roles_final.append(canon)\n\
        \            seen.add(canon.lower())\n    \n    # Guard: Never return if list\
        \ is empty\n    return roles_final\n"
      name: extract_explicit_user_roles_from_requirements
      nodes_depended_on: []
      nodes_dependent_on: []
      output_structure:
      - description: Output of type List[str]
        key: output
        type: List[str]
      - description: Input parameter of type str
        key: text
        type: str
      - description: Input parameter of type str
        key: context_kwargs
        type: str
      prd:
        bullets:
        - complexity: MEDIUM
          impact: Improves the accuracy of downstream logic by providing a dependable
            set of involved user roles for further design and analysis.
          method: Implement rule-based text parsing and entity recognition (possibly
            using regular expressions, keyword dictionaries, or NLP libraries such
            as spaCy) to identify role nouns and filter for explicitness.
          reason: Extracting explicit user roles ensures the workflow's participants
            are clearly defined based only on directly stated requirements, minimizing
            ambiguity.
          text: Parse the input requirements text to enumerate all concrete user roles
            or personas that are explicitly referenced.
        - complexity: LOW
          impact: Reduces downstream bugs and simplifies integration with schema-validated
            outputs.
          method: Post-process results with set() or equivalent logic and cast output
            to List[str], rejecting invalid or empty values.
          reason: Prevents redundancy and enforces predictable data structures that
            downstream nodes depend on.
          text: De-duplicate user role names and guarantee type safety by sanitizing
            output as a list of strings.
        - complexity: MEDIUM
          impact: Yields more precise and comprehensive user role lists tailored to
            nuanced requirements.
          method: Incorporate logic for context-based mapping (such as looking up
            roles from context_kwargs or using simple heuristics) when terminology
            is ambiguous.
          reason: Contextual disambiguation is necessary when the same term may mean
            different roles depending on workflow context.
          text: Leverage optional contextual information to resolve ambiguous references
            and improve role extraction.
      prompt: Extract and return a list of explicit user role or persona names that
        are directly mentioned in the following requirements text, focusing on unique,
        concrete roles and ignoring implied or generic terms; use any provided context
        to disambiguate ambiguous references. Output a Python list of strings with
        the user role names only.
      shims: []
    - agent: false
      code_node_type: virtual-pure
      description: Ensures that the specified senior manager meeting role is present
        as the first element in a list of user roles, reordering or inserting it as
        needed.
      implementation: "def ensure_senior_manager_meeting_first(senior_manager_role:\
        \ str, candidate_roles: str) -> List[str]:\n    \"\"\"\n    Ensures that the\
        \ specified senior manager meeting role is present as the first element in\
        \ a list of user roles, reordering or inserting it as needed.\n\n    Args:\n\
        \        senior_manager_role: Input parameter of type str\ncandidate_roles:\
        \ Input parameter of type str\n\n    Returns:\n        List[str]: Output of\
        \ type List[str]\n    \"\"\"\n    # Parse the candidate_roles string into\
        \ a list of role strings (comma-separated is assumed)\n    roles = [role.strip()\
        \ for role in candidate_roles.split(',') if role.strip()]\n    # Remove all\
        \ occurrences of senior_manager_role (if any)\n    filtered_roles = [role\
        \ for role in roles if role != senior_manager_role]\n    # Insert the senior_manager_role\
        \ at the beginning\n    result = [senior_manager_role] + filtered_roles\n\
        \    return result\n"
      name: ensure_senior_manager_meeting_first
      nodes_depended_on: []
      nodes_dependent_on: []
      output_structure:
      - description: Output of type List[str]
        key: output
        type: List[str]
      - description: Input parameter of type str
        key: senior_manager_role
        type: str
      - description: Input parameter of type str
        key: candidate_roles
        type: str
      prd:
        bullets:
        - complexity: LOW
          impact: Ensures compliance with workflow requirements and facilitates downstream
            processing that assumes correct role order.
          method: Remove all occurrences of senior_manager_role from candidate_roles,
            insert senior_manager_role at index 0, and append the rest of the roles
            preserving order.
          reason: Prevents duplicate entries of the manager role and guarantees its
            correct priority within the workflow.
          text: Check if the senior_manager_role exists in the candidate_roles, and
            ensure it appears only once as the first element in the returned output.
        - complexity: LOW
          impact: Creates a deterministic user role ordering improving testability
            and maintainability.
          method: Use standard list concatenation and filtering to compose the output.
          reason: Provides a clean, ordered user role list necessary for consistency
            and validation in subsequent steps.
          text: Return a new list combining the uniquely positioned senior_manager_role
            and the filtered candidate roles.
      prompt: Given a senior_manager_role (str) and a candidate_roles (List[str]),
        output a List[str] in which the senior_manager_role appears as the first element,
        and the remaining roles follow in their original order without duplicates.
      shims: []
    - agent: false
      code_node_type: virtual-pure
      description: This node validates and enforces type-safety on a list of user
        roles, ensuring all entries are unique, non-empty strings before returning
        the cleaned List[str].
      implementation: "def validate_user_roles_type_safety(user_roles: str) -> List[str]:\n\
        \    \"\"\"\n    This node validates and enforces type-safety on a list of\
        \ user roles, ensuring all entries are unique, non-empty strings before returning\
        \ the cleaned List[str].\n\n    Args:\n        user_roles: Input parameter\
        \ of type str\n\n    Returns:\n        List[str]: Output of type List[str]\n\
        \    \"\"\"\n    import ast\n\n    # Step 1: Deserialize the str input to\
        \ a list (assuming JSON or str(list) format)\n    try:\n        user_roles_list\
        \ = ast.literal_eval(user_roles)\n    except Exception:\n        # If parsing\
        \ fails, return an empty list (or raise, depending on policy)\n        return\
        \ []\n\n    if not isinstance(user_roles_list, list):\n        return []\n\
        \n    # Step 2: Clean and validate each item\n    cleaned_roles = []\n   \
        \ for role in user_roles_list:\n        # Ensure the item is a string\n  \
        \      if not isinstance(role, str):\n            # Try to coerce to string\
        \ if possible\n            try:\n                role = str(role)\n      \
        \      except Exception:\n                continue\n        # Remove whitespace\
        \ around the string\n        role_stripped = role.strip()\n        if role_stripped:\n\
        \            cleaned_roles.append(role_stripped)\n\n    # Step 3: Remove duplicates\
        \ while preserving order\n    seen = set()\n    unique_roles = []\n    for\
        \ role in cleaned_roles:\n        if role not in seen:\n            seen.add(role)\n\
        \            unique_roles.append(role)\n\n    return unique_roles\n"
      name: validate_user_roles_type_safety
      nodes_depended_on: []
      nodes_dependent_on: []
      output_structure:
      - description: Output of type List[str]
        key: output
        type: List[str]
      - description: Input parameter of type str
        key: user_roles
        type: str
      prd:
        bullets:
        - complexity: LOW
          impact: Prevents runtime errors and enforces data contract integrity for
            user role processing.
          method: Use isinstance checks or schema validation for each list element,
            discarding or converting non-string entries.
          reason: This ensures that downstream logic expecting a List[str] does not
            fail due to unexpected data types.
          text: Verify that each item in the provided user_roles list is a string,
            removing or correcting any items that are not valid strings.
        - complexity: LOW
          impact: Ensures the user roles list is both unique and meaningful, improving
            quality for dependent nodes.
          method: Apply set() operations or list comprehensions to filter duplicates
            and strip whitespace, omitting empty results.
          reason: Duplicates or empty entries could lead to redundant processing and
            undermine the clarity of workflow steps.
          text: Remove any duplicate or empty (whitespace-only) user role entries
            from the list.
        - complexity: LOW
          impact: Facilitates reliable data interchange and validates adherence to
            API/data schema requirements.
          method: Return the final List[str] directly or convert it to JSON/typed-structure
            as required by the application stack.
          reason: Ensures that output is in a predictable and consumable format compatible
            with pydantic models and type checkers.
          text: Serialize the cleaned and validated list as List[str] for downstream
            consumption.
      prompt: Given a list of user_roles, ensure the list is strictly composed of
        type-safe strings representing user roles (e.g., no duplicates, no invalid/non-string
        entries, no empty or whitespace-only items), returning the cleaned and validated
        list of user roles as List[str].
      shims: []
  prioritize_features:
    agent: false
    code_node_type: pure
    description: "Assign a unique priority ranking to each key feature, ensuring that\
      \ a feature related to a meeting with senior managers is explicitly included\u2014\
      specifically, a node for the meeting should occur prior to determining user\
      \ roles\u2014and given an appropriate criticality ranking."
    implementation: "from pydantic import BaseModel, Field\nfrom typing import List\n\
      \n\nclass ExtractKeyFeaturesOutput(BaseModel):\n    \"\"\"Pydantic model for\
      \ extract_key_features node outputs.\"\"\"\n    feature_names: List[str] = Field(...,\
      \ description=\"Names of fundamental workflow features required in the PRD,\
      \ in sequence starting with a node to schedule and conduct a meeting with senior\
      \ managers prior to any role assignment, followed by other required workflow\
      \ features.\")\n\n\nclass PrioritizeFeaturesOutput(BaseModel):\n    \"\"\"Pydantic\
      \ model for prioritize_features node outputs.\"\"\"\n    feature_priorities:\
      \ List[int] = Field(..., description=\"Priority value for each feature, parallel\
      \ with feature_names (ensuring a 'meeting with senior managers' node comes before\
      \ user role determination and is appropriately ranked).\")\n\n\ndef prioritize_features_fx(extract_key_features_input:\
      \ ExtractKeyFeaturesOutput, **kwargs) -> PrioritizeFeaturesOutput:\n    \"\"\
      \"Assign a unique priority ranking to each key feature, ensuring that a feature\
      \ related to a meeting with senior managers is explicitly included\u2014specifically,\
      \ a node for the meeting should occur prior to determining user roles\u2014\
      and given an appropriate criticality ranking.\n\n    Args:\n        extract_key_features_input:\
      \ Input from the 'extract_key_features' node.\n        **kwargs: Additional\
      \ keyword arguments.\n\n    Returns:\n        PrioritizeFeaturesOutput: Object\
      \ containing outputs for this node.\n    \"\"\"\n    # --- SHIM IMPLEMENTATION\
      \ ---\n    # 1. Get original feature names\n    feature_names: List[str] = extract_key_features_input.feature_names\n\
      \    \n    # 2. Ensure 'Schedule and conduct a meeting with senior managers'\
      \ is present and placed before any user-role-determination features\n    enforced_feature_names:\
      \ List[str] = enforce_meeting_precedes_roles(features=feature_names)\n    \n\
      \    # 3. Assign unique priorities based on criticality, workflow, and regulatory\
      \ dependency\n    priorities: List[int] = assign_unique_priorities(features=enforced_feature_names)\n\
      \    \n    # 4. Validate index alignment (parallel lists)\n    validate_alignment(features=enforced_feature_names,\
      \ priorities=priorities)  # returns None, raises if mismatch\n    \n    # 5.\
      \ Return as required\n    return PrioritizeFeaturesOutput(feature_priorities=priorities)\n"
    name: prioritize_features
    nodes_depended_on:
    - extract_key_features
    nodes_dependent_on: []
    output_structure:
    - description: Priority value for each feature, parallel with feature_names (ensuring
        a 'meeting with senior managers' node comes before user role determination
        and is appropriately ranked).
      key: feature_priorities
      type: List[int]
    prd:
      bullets:
      - complexity: "LOW \u2013 List manipulation and sequence validation are straightforward,\
          \ though meticulous attention to order is required."
        impact: "HIGH \u2013 Guarantees critical workflow structure compliance, serving\
          \ as a gating condition for subsequent process steps and affecting the logic\
          \ of feature prioritization."
        method: Iterate over feature_names to check for the required feature; if absent
          or mis-sequenced, insert or reposition it with array/list operations based
          on pattern matching the feature name.
        reason: Ensuring this node's explicit inclusion and critical ordering aligns
          the workflow with both compliance requirements and executive alignment,
          preventing downstream errors.
        text: Check the extracted feature_names list for the explicit presence and
          correct sequencing of 'Schedule and conduct a meeting with senior managers'
          directly before any user role determination features.
      - complexity: "MEDIUM \u2013 Requires analysis of feature_names for semantic\
          \ similarity, including keyword searching or rule-based identification of\
          \ role-related features."
        impact: "HIGH \u2013 If not enforced, major regulatory or governance steps\
          \ could be skipped, invalidating the PRD compliance."
        method: Use rule-based filtering or keyword matching (e.g., regex for 'user
          role', 'persona', 'assignment') to find user role features; reposition or
          insert the meeting feature directly before them programmatically.
        reason: This explicit workflow dependency is necessary to reflect business
          logic and the real-world review/signoff requirements.
        text: Identify any feature(s) related to user role determination, and ensure
          the 'Schedule and conduct a meeting with senior managers' feature directly
          precedes them in the ordered list.
      - complexity: "MEDIUM \u2013 Requires interpretation of feature importance in\
          \ context, potential involvement of predefined rules or criteria for criticality."
        impact: "HIGH \u2013 Defines system implementation order and influences technical/development\
          \ focus."
        method: Establish a scoring rubric (e.g., business value, regulatory importance,
          technical dependency); iterate through the sequenced list and assign priorities,
          ensuring uniqueness by incrementing from 1. Optionally allow developer or
          product manager override for corner cases.
        reason: Unique, criticality-based prioritization supports downstream resource
          allocation, sequencing, and fulfilment of business goals.
        text: Assign a unique priority integer to each feature based on its criticality
          to the core objective, strictly without duplicates and reflecting all workflow
          constraints.
      - complexity: "LOW \u2013 Ensured via tight loop structure, validation checks,\
          \ and possibly unit tests."
        impact: "MEDIUM \u2013 Prevents subtle bugs and miscommunications that could\
          \ derail analysis or automation."
        method: Generate priorities by computing a list of integers in the same length/order
          as the feature_names list; implement automated checks that parallel index
          references remain correct.
        reason: Index alignment is crucial to prevent data mismatching or logic errors
          later in the workflow's processing or reporting stages.
        text: Maintain alignment between the feature_names list order and the generated
          priority values, ensuring parallel indexing throughout all downstream nodes.
      - complexity: "LOW \u2013 Output formatting is trivial if index alignment is\
          \ maintained."
        impact: "MEDIUM \u2013 Upholds API/data structure contracts throughout the\
          \ workflow."
        method: Bundle feature_priorities as a separate but corresponding array or
          list in the output structure; document contract to ensure future maintainers
          understand alignment requirements.
        reason: Explicitly separating priorities from feature names enables clear
          consumption and flexibility in dependent node/business logic.
        text: Return the feature_priorities list as an output distinct from feature_names
          but aligned, to be used by all dependent nodes requiring ordered, ranked
          feature data.
    prompt: Given the list of key features, ensure that a feature for a 'meeting with
      senior managers' appears immediately before any feature related to determining
      user roles (add or reposition it if necessary). Then, assign a unique integer
      priority to each feature strictly based on its criticality to the core objective.
      Rank priority so that 1 means most critical, with higher numbers for less critical
      features. No duplicates allowed. If the 'meeting with senior managers' feature
      is newly added or moved, carefully consider its priority relative to the other
      features, especially its position before user role determination.
    shims:
    - agent: false
      code_node_type: virtual-pure
      description: Ensures that the feature for scheduling and conducting a meeting
        with senior managers is included in the workflow feature list and appears
        before any role-determination related features.
      implementation: "def enforce_meeting_precedes_roles(features: str) -> List[str]:\n\
        \    \"\"\"\n    Ensures that the feature for scheduling and conducting a\
        \ meeting with senior managers is included in the workflow feature list and\
        \ appears before any role-determination related features.\n\n    Args:\n \
        \       features: Input parameter of type str\n\n    Returns:\n        List[str]:\
        \ Output of type List[str]\n    \"\"\"\n    # PURE IMPLEMENTATION - replaces\
        \ shims\n    import re\n    # Split the input string into a list of features,\
        \ handling common list separators\n    # We'll assume features are comma or\
        \ newline separated\n    # First, split on newlines, then flatten by splitting\
        \ on commas, then strip whitespace\n    feature_lines = []\n    for line in\
        \ features.split('\\n'):\n        feature_lines.extend([f.strip() for f in\
        \ line.split(',') if f.strip()])\n    feature_list = [f for f in feature_lines\
        \ if f]\n\n    meeting_feature = 'Schedule and conduct a meeting with senior\
        \ managers'\n    meeting_present = any(feature.strip() == meeting_feature\
        \ for feature in feature_list)\n\n    # 1. Guarantee the meeting feature exists\n\
        \    if not meeting_present:\n        feature_list.append(meeting_feature)\n\
        \n    # 2. Identify all role-related features\n    # Keywords: 'user role',\
        \ 'assign role', 'determine role'\n    role_keywords = [\n        r'user\\\
        s*role',\n        r'assign\\s*role',\n        r'determine\\s*role'\n    ]\n\
        \    role_regex = re.compile(r'(' + '|'.join(role_keywords) + r')', re.IGNORECASE)\n\
        \    role_feature_idxs = [i for i, feat in enumerate(feature_list) if role_regex.search(feat)]\n\
        \n    # 3. Ensure meeting feature precedes all such features\n    # Only reposition\
        \ if necessary\n    if role_feature_idxs:\n        first_role_idx = min(role_feature_idxs)\n\
        \        # Find the current position of the meeting feature\n        try:\n\
        \            current_meeting_idx = feature_list.index(meeting_feature)\n \
        \       except ValueError:\n            # Should not occur, as we've guaranteed\
        \ presence above\n            current_meeting_idx = None\n        if current_meeting_idx\
        \ is not None and current_meeting_idx > first_role_idx:\n            # Remove\
        \ meeting from its current position\n            feature_list.pop(current_meeting_idx)\n\
        \            # Insert meeting before the first role-determination feature\n\
        \            feature_list.insert(first_role_idx, meeting_feature)\n\n    #\
        \ 4. Return the final adjusted feature list (order otherwise preserved)\n\
        \    return feature_list\n"
      name: enforce_meeting_precedes_roles
      nodes_depended_on: []
      nodes_dependent_on: []
      output_structure:
      - description: Output of type List[str]
        key: output
        type: List[str]
      - description: Input parameter of type str
        key: features
        type: str
      prd:
        bullets:
        - complexity: LOW
          impact: Prevents downstream errors and omissions by embedding the meeting
            step if missing.
          method: Check for the presence of the target string; if missing, insert
            it into the list.
          reason: Guarantees compliance by making sure the regulatory prerequisite
            meeting is not omitted from the workflow.
          text: Detect whether 'Schedule and conduct a meeting with senior managers'
            exists in the input feature list, and add it if absent.
        - complexity: MEDIUM
          impact: Maintains organizational and regulatory process order, reducing
            logic bugs and compliance gaps.
          method: Scan through the feature list for role-determination keywords; reposition
            the meeting feature to appear before the first such feature.
          reason: Ensures workflow mandate that managerial consultation precedes any
            assignment or determination of user roles.
          text: Identify features related to user role determination (e.g., features
            containing terms like 'user role', 'assign role', 'determine role'), and
            enforce that the meeting feature precedes all such features.
        - complexity: LOW
          impact: Results in minimal disruption and increased trust in feature prioritization.
          method: After necessary insertions or moves, output the reordered list as
            a List[str].
          reason: Reduces side effects and preserves user intent regarding unrelated
            workflow steps.
          text: Return the adjusted feature list, preserving the original order for
            features not subject to repositioning.
      prompt: Given a list of workflow feature names (as strings), ensure that the
        feature 'Schedule and conduct a meeting with senior managers' is present and
        occurs before any feature related to user role determination within the list.
        Return the reordered list of features.
      shims: []
    - agent: false
      code_node_type: virtual-pure
      description: Assigns a unique integer priority to each workflow feature in a
        given list, ensuring prioritization is based on criticality, workflow sequence,
        and regulatory importance.
      implementation: "def assign_unique_priorities(features: str) -> List[int]:\n\
        \    \"\"\"\n    Assigns a unique integer priority to each workflow feature\
        \ in a given list, ensuring prioritization is based on criticality, workflow\
        \ sequence, and regulatory importance.\n\n    Args:\n        features: Input\
        \ parameter of type str\n\n    Returns:\n        List[int]: Output of type\
        \ List[int]\n    \"\"\"\n    # PURE IMPLEMENTATION - Replaces virtual-stub\n\
        \    import re\n    from typing import List\n\n    # Assume comma or newline\
        \ separated list. Robust splitting:\n    features_raw = re.split(r'[\\n,]+',\
        \ features)\n    # Remove extra spaces and blank entries\n    features_list\
        \ = [f.strip() for f in features_raw if f.strip()]\n    \n    # Check uniqueness:\n\
        \    features_set = set(features_list)\n    if len(features_set) != len(features_list):\n\
        \        raise ValueError(\"Feature list contains duplicate entries. Each\
        \ feature must be unique.\")\n    \n    # Priorities: sequential integers\
        \ starting from 1, matching order\n    priorities = list(range(1, len(features_list)\
        \ + 1))\n    \n    # Ensure output list aligns with input list\n    assert\
        \ len(priorities) == len(features_list)\n    return priorities\n"
      name: assign_unique_priorities
      nodes_depended_on: []
      nodes_dependent_on: []
      output_structure:
      - description: Output of type List[int]
        key: output
        type: List[int]
      - description: Input parameter of type str
        key: features
        type: str
      prd:
        bullets:
        - complexity: LOW
          impact: Prevents data inconsistency and guarantees a strict one-to-one mapping
            between features and priorities.
          method: Implement a uniqueness check (e.g., using a set for deduplication
            and verifying original order/length).
          reason: Ensures that priorities are not duplicated due to feature name collisions
            and each feature receives an unambiguous ranking.
          text: Parse the input feature list and validate that each feature is distinct
            before proceeding to priority assignment.
        - complexity: MEDIUM
          impact: Facilitates accurate workflow execution and ensures business rules
            are enforced in automation.
          method: Sort or order features according to predefined rules (e.g., criticality
            or dependency), then assign incrementing integer values.
          reason: Encodes business logic into the order of operations, enabling downstream
            nodes to act based on correct process sequencing and importance.
          text: Assign sequential unique integer priorities (e.g., starting from 1)
            to features, with priority values reflecting their criticality and workflow
            order.
        - complexity: LOW
          impact: Supports robust data handling and downstream correctness in workflow
            automation or display.
          method: Generate the output as a list of integers aligned by index with
            the input feature list; validate lengths before return.
          reason: Maintains data integrity for parallel processing and later referencing
            between features and their priorities.
          text: Ensure the output priority list exactly matches the order and length
            of the input feature list, with clear index-to-feature correspondence.
      prompt: 'Assign a unique integer priority to each feature in the provided list
        (features: str) such that the output is an ordered list of integers representing
        each feature''s priority based on criticality, workflow order, and regulatory
        dependency; make sure every feature has a unique, sequential priority.'
      shims: []
    - agent: false
      code_node_type: virtual-pure
      description: This node verifies that the list of workflow features and the list
        of feature priorities are aligned in length and order, ensuring every feature
        has a unique corresponding priority.
      implementation: "def validate_alignment(features: str, priorities: str) -> str:\n\
        \    \"\"\"\n    This node verifies that the list of workflow features and\
        \ the list of feature priorities are aligned in length and order, ensuring\
        \ every feature has a unique corresponding priority.\n\n    Args:\n      \
        \  features: Input parameter of type str\npriorities: Input parameter of type\
        \ str\n\n    Returns:\n        str: Output of type Any\n    \"\"\"\n    import\
        \ json\n    # Attempt to parse both input strings as JSON lists\n    try:\n\
        \        features_list = json.loads(features)\n        priorities_list = json.loads(priorities)\n\
        \    except json.JSONDecodeError as e:\n        raise ValueError(f\"Failed\
        \ to decode features/priorities as JSON lists: {str(e)}\")\n    \n    if not\
        \ isinstance(features_list, list) or not isinstance(priorities_list, list):\n\
        \        raise ValueError(f\"Expected both features and priorities to be lists,\
        \ but got types: {type(features_list).__name__}, {type(priorities_list).__name__}\"\
        )\n\n    len_features = len(features_list)\n    len_priorities = len(priorities_list)\n\
        \    if len_features != len_priorities:\n        # Show the first few entries\
        \ for clarity\n        max_show = 3\n        features_show = features_list[:max_show]\n\
        \        priorities_show = priorities_list[:max_show]\n        raise ValueError(\n\
        \            f\"Length mismatch: features has {len_features} items vs priorities\
        \ has {len_priorities} items.\\n\"\n            f\"First features: {features_show}\\\
        n\"\n            f\"First priorities: {priorities_show}\"\n        )\n   \
        \ return \"VALID\"\n"
      name: validate_alignment
      nodes_depended_on: []
      nodes_dependent_on: []
      output_structure:
      - description: Output of type Any
        key: output
        type: str
      - description: Input parameter of type str
        key: features
        type: str
      - description: Input parameter of type str
        key: priorities
        type: str
      prd:
        bullets:
        - complexity: LOW
          impact: Prevents inconsistent, partial, or mismatched data outputs that
            could cause downstream logic failures or incorrect prioritization.
          method: Raise an exception if len(features) != len(priorities); pass otherwise.
          reason: Ensures each feature has a corresponding priority and prevents silent
            misalignment errors in downstream processing.
          text: Check that the 'features' and 'priorities' input lists have identical
            lengths.
        - complexity: LOW
          impact: Enables developers or maintainers to correct data issues quickly
            and with confidence.
          method: Construct and raise a ValueError or custom exception with an informative
            message showing expected and actual values.
          reason: Facilitates rapid debugging and clear error reporting in development
            and production environments.
          text: On detection of misalignment, provide a detailed exception or error
            message including both list lengths and potentially the first mismatched
            entries.
      prompt: 'Verify that the input list of features and the input list of priorities
        are strictly aligned: confirm both lists are the same length and that each
        index in features has a corresponding entry in priorities; if not, raise a
        validation error with a clear message.'
      shims: []
